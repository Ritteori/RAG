5. Нормы и расстояния 
41. Что такое L1-норма (манхэттенская)? 
L1-норма — это способ измерить длину вектора, где мы считаем, сколько шагов 
нужно сделать по осям, чтобы дойти до точки. 
Представь, что ты идёшь по городу с прямыми улицами (как в Манхэттене): 
• нельзя идти по диагонали, 
• можно только по горизонтали и вертикали. 
Чтобы попасть из точки A в точку B, ты идёшь сначала вправо, потом вверх (или 
наоборот), и складываешь все шаги. 
L1-норма — это сумма всех таких шагов. 
Пример: 
Если точка имеет координаты (3, -2), то: 
L1-норма=∣3∣+∣−2∣=5  
То есть нужно пройти 3 шага вправо и 2 шага вниз — всего 5 шагов. 
42.Что такое L2-норма (евклидова)? 
L2-норма — это длина вектора, измеренная как прямая линия от начала 
координат до точки. 
Это обычное расстояние «по линейке» — как по теореме Пифагора. 
Формула для вектора x = (x1 ,x2 ,...,xn ): 
∥x∥2 = sqrt(x12 +x22 +⋯+xn2 )   
Геометрически — это длина гипотенузы. 
Пример: 
Если точка (3, 4), то L2-норма: 
∥x∥2 = sqrt(32+42) = 9+16 = 5 
43. В чём разница между L1 и L2-нормой? 
Интуитивно: 
• L1-норма считает расстояние по прямым углам, как будто ты идёшь по 
городу по улицам: сначала вправо, потом вверх. 
• L2-норма — это расстояние напрямую, по диагонали — как будто ты идёшь 
по прямой через дома (если бы мог). 
�
� Геометрия: 
• В L1 расстояние между (0,0) и (3,4) = 3 + 4 = 7 
• В L2 расстояние между (0,0) и (3,4) = 9+16 =5 
➡ То есть L2 всегда ≤ L1, если координаты не нули. 
�
� Применение: 
• L1-норма: 
o проще вычисляется (без корня); 
o используется, если важно снизить влияние выбросов (устойчива к 
большим значениям); 
o активно применяется в Lasso-регрессии — помогает занулять 
ненужные признаки. 
• L2-норма: 
o чувствительнее к большим значениям (выбросы сильно влияют); 
o часто используется в Ridge-регрессии и при градиентном спуске, 
потому что хорошо сглаживает; 
o геометрически ближе к реальному расстоянию в пространстве. 
Вывод: 
• L1 — проще, грубее, устойчивее к шуму. 
• L2 — ближе к «настоящему расстоянию», но требует больше вычислений. 
44. Что такое косинусная схожесть? 
Косинусная схожесть (cosine similarity) — это метрика сходства между двумя 
векторами, которая измеряет угол между ними, а не расстояние. 
�
� Геометрический смысл: 
• Если угол между векторами 0°, то они совпадают по направлению → 
схожесть = 1. 
• Если угол 90° → векторы ортогональны → схожесть = 0. 
• Если угол 180° → векторы противоположны по направлению → схожесть = 
−1. 
➡ То есть косинусная схожесть показывает, насколько вектора смотрят в одну 
сторону, игнорируя их длину. 
�
� Почему это полезно? 
• В задачах обработки текста (NLP), где документы — это векторы частот 
слов, важно не длина (размер документа), а направление (содержание). 
• В рекомендательных системах: товары/пользователи сравниваются по 
направлению предпочтений. 
45. Почему L2-норма часто используется в машинном обучении? 
L2-норма — это корень из суммы квадратов элементов вектора, то есть евклидово 
расстояние между точками в пространстве. Её широко применяют в ML по 
нескольким причинам: 
• Геометрически L2 измеряет «прямое» расстояние между точками, что 
естественно отражает степень отличия объектов. 
• Она является гладкой и дифференцируемой функцией, что критично для 
алгоритмов, основанных на градиентном спуске, поскольку позволяет 
эффективно вычислять и использовать производные. 
• L2-норма «наказывает» большие ошибки сильнее (из-за квадрата), что 
помогает модели уделять больше внимания большим отклонениям и 
улучшать точность. 
• При регуляризации (например, Ridge regression) L2 способствует снижению 
переобучения, заставляя веса модели оставаться малыми и равномерно 
распределёнными. 
• В итоге, благодаря этим свойствам, L2-норма обеспечивает стабильность и 
эффективность обучения, делая её стандартным выбором в ML. 
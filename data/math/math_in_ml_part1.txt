�7. Применение к ML 
51. Как скалярное произведение используется в Attention механизмах? — 
подробнее 
В механизме Attention в трансформерах для каждого элемента 
последовательности вычисляются три вектора: Query (запрос), Key (ключ) и Value 
(значение). 
• Query — задаёт, на что мы “смотрим” сейчас. 
• Key — содержит информацию, с чем мы сравниваем. 
• Value — это сама информация, которую хотим получить или взвесить. 
Чтобы понять, насколько важен один элемент по отношению к другому, мы 
вычисляем скалярное произведение между Query текущего элемента и Key 
другого элемента. Формула для двух векторов q и k: 
Это число показывает, насколько похожи эти два вектора (элементы), то есть 
насколько “внимание” должно быть направлено на соответствующий элемент. 
Затем полученные скалярные произведения проходят через функцию softmax, 
которая преобразует эти значения в вероятности (веса), суммирующиеся в 1. 
Эти веса используются для взвешивания Value-векторов, и итоговый результат — 
это сумма Value-векторов, умноженных на соответствующие веса внимания. 
Важные моменты: 
• Чем выше скалярное произведение, тем сильнее связь между элементами. 
• Чтобы избежать слишком больших значений, которые мешают обучению, 
скалярное произведение часто делят на sqrt(dk ), где dk— размерность Key
векторов (это называется масштабированное скалярное произведение). 
В итоге скалярное произведение — ключевой шаг, позволяющий вычислить 
важность каждого элемента входа относительно других, что и есть суть механизма 
Attention. 
52.Как работает матричное умножение в слое нейросети? 
В нейросетевом слое (например, полносвязном/линейном слое) матрица весов 
умножается на входной вектор (или матрицу), чтобы вычислить выходные 
значения. 
Пусть: 
• x — входной вектор размерности n 
• W — матрица весов размерности m×n 
• b — вектор смещений (bias) размерности m 
Тогда выход вычисляется по формуле: 
y=W⋅x+b  
Здесь: 
• Каждая строка матрицы W — это набор весов, с которыми входные 
значения комбинируются для одного выхода. 
• Умножение W⋅x по сути создаёт линейную комбинацию признаков — 
каждое новое значение получается как взвешенная сумма входов. 
• При обучении сети веса в W и смещения b подстраиваются так, чтобы 
минимизировать функцию потерь. 
Таким образом, матричное умножение в нейросети трансформирует вход в выход 
через линейную операцию, задаваемую весами. 
53. Как вычисляется градиент функции потерь? 
Градиент функции потерь вычисляется при помощи метода обратного 
распространения ошибки (backpropagation), который основан на цепном 
правиле дифференцирования. 
Если у нас есть: 
• модель с параметрами (весами) w  b, 
• выход модели: y^ =w⋅x+b, 
• и функция потерь: L=(y^ −y)2, 
то чтобы узнать, как менять веса, мы вычисляем производные по каждому 
параметру: 
Это и есть цепное правило. Оно позволяет посчитать влияние изменения каждого 
параметра на итоговую ошибку. 
54. Как работает градиентный спуск с точки зрения математики? 
Градиентный спуск — это итерационный алгоритм оптимизации, суть которого 
— минимизировать функцию потерь путём движения вдоль антиградиента 
(против направления наибольшего роста функции). 
�
� Математическая формулировка: 
Пусть у нас есть функция потерь L(w), зависящая от параметров модели w ∈ Rn. 
Тогда на каждом шаге мы обновляем параметры по формуле: 
wnew =wold − η⋅∇L(w)  
где: 
• ∇L(w) — градиент функции потерь (вектор частных производных по всем 
параметрам), 
• η — скорость обучения (learning rate), 
• знак минус — потому что идём в сторону уменьшения функции. 
�
� Что происходит на практике: 
1. Мы выбираем начальные значения параметров w. 
2. Находим градиент:  
3. Обновляем каждый параметр: 
wi := wi  − η ⋅ ∂L / ∂wi 
4. Повторяем шаги, пока не достигнем минимума (или достаточно маленького 
градиента). 
�
� Ключевая идея: 
Мы шагаем по поверхности функции потерь вниз, всегда в сторону, где она 
убывает сильнее всего, пока не окажемся в локальном или глобальном 
минимуме. 
55. Почему производные важны для backpropagation? 
�
� Расширенное объяснение: 
Backpropagation (обратное распространение ошибки) — это метод, который: 
1. Рассчитывает ошибку на выходе сети. 
2. Использует производные (градиенты) для того, чтобы распространить эту 
ошибку назад через все слои модели. 
3. Позволяет понять вклад каждого веса в общую ошибку. 
�
� Почему именно производные? 
Производная — это мера того, насколько функция меняется, если мы изменим 
вход (вес, например): 
∂L / ∂w  
показывает, как изменится функция потерь L, если вес w изменится на единицу. 
�
� Как это используется: 
В процессе обучения: 
• Мы вычисляем градиенты всех весов с помощью цепного правила. 
• После этого обновляем веса с учётом этих градиентов: 
w := w − η ⋅ ∂L / ∂w   
�
� Без производных: 
Без производных невозможно понять, какие параметры нужно менять и в какую 
сторону. То есть невозможно обучить модель с помощью градиентного спуска 
или его модификаций. 
56. Как матрицы весов трансформируют данные в линейной модели? 
�
� Полный ответ: 
Матрицы весов в линейной модели выполняют линейное преобразование 
входных данных. Это значит, что они: 
• меняют масштаб (растягивают/сжимают); 
• поворачивают данные; 
• смещают их (если добавляется bias — свободный член). 
�
� Формула: 
Для модели: 
y^ =Wx+b  
где: 
• x — входной вектор (размерность n×1); 
• W — матрица весов (размерность m×n); 
• b — вектор смещений (bias); 
• y^  — выходной вектор (размерность m×1). 
�
� Что делает матрица W? 
• Каждая строка в W — это набор весов, с которыми входной вектор x 
складывается (скалярно умножается). 
• Вся матрица W как бы "переводит" входной вектор x в новое пространство 
признаков (feature space). 
�
� Вывод: 
Матрица весов W определяет, как входные данные изменятся, а значит, как 
будет вести себя модель. Это ключ к обучению: мы изменяем W, чтобы получить 
нужный выход. 
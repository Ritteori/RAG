Раздел 3: Математика и Статистика 
Линейная алгебра:  
1.  Что такое собственные вектора и собственные числа? 
Геометрическая интерпретация.  
Формальное определение: Для квадратной матрицы A ненулевой вектор v называется 
собственным вектором, а число λ — собственным числом, если выполняется 
уравнение: 
A * v = λ * v 
Геометрическая интерпретация (самая важная часть): 
• Обычное умножение матрицы на вектор может делать с вектором всё что 
угодно: поворачивать, отражать, сжимать, растягивать и сдвигать. 
• Умножение на собственный вектор — это особый случай. Матрица A 
действует на вектор v и ведет себя как простое число λ. 
• Геометрически: Собственный вектор v не меняет своего направления в 
пространстве под действием матрицы A. Он только растягивается или 
сжимается в λ раз. 
o Если |λ| > 1 — растяжение. 
o Если |λ| < 1 — сжатие. 
o Если λ отрицательное — вектор еще и меняет направление на 
противоположное. 
Аналогия: Представьте, что матрица A — это сила ветра. 
• Большинство предметов (обычные векторы) эта сила будет крутить и сносить в 
сторону. 
• Но есть особые направления (собственные векторы), вдоль которых предметы 
будут просто сдвигаться вперед или назад, не меняя ориентации. Сила ветра 
в этом направлении (насколько сильно сдвигает) — это и есть собственное 
число λ. 
Зачем это нужно в Machine Learning? 
• Собственные векторы задают "главные направления" данных. 
Классический пример — Метод главных компонент (PCA). Первая главная 
компонента — это собственный вектор, соответствующий наибольшему 
собственному числу ковариационной матрицы данных. Он показывает 
направление наибольшей дисперсии (разброса) данных. 
• Собственные числа показывают "важность" этих направлений. В PCA 
собственное число говорит, какая доля дисперсии объясняется данной 
компонентой. 
Пример: Допустим, матрица A описывает преобразование на плоскости. 
• У нее может быть собственный вектор v₁ = [1, 0] с λ₁ = 2. Это значит, что все 
векторы, направленные вдоль оси X, это преобразование удваивает. 
• И собственный вектор v₂ = [0, 1] с λ₂ = 0.5. Все векторы вдоль оси Y оно 
сжимает в два раза. 
Таким образом, собственные вектора и числа раскладывают сложное 
преобразование матрицы на простые, независимые растяжения по главным осям. 
2. Что такое сингулярное разложение (SVD)? Где применяется?  
Что это? SVD — это способ разложения любой матрицы (не обязательно 
квадратной!) A размера m x n в произведение трех матриц: 
A = U * Σ * V^T 
Где: 
• U (левая сингулярная матрица): Размер m x m. Ее столбцы — это собственные 
векторы матрицы A * A^T. Они образуют ортонормированный базис в 
пространстве строк. 
• Σ (сингулярная матрица): Размер m x n. Это диагональная матрица, на 
диагонали которой лежат сингулярные числа (σ₁, σ₂, ...). Они всегда 
неотрицательны и упорядочены по убыванию: σ₁ ≥ σ₂ ≥ ... ≥ 0. Они показывают 
"важность" каждого компонента. 
• V^T (правая сингулярная матрица, транспонированная): Размер n x n. Ее строки 
— это собственные векторы матрицы A^T * A. Они образуют 
ортонормированный базис в пространстве столбцов. 
Геометрическая интерпретация: Любое линейное преобразование, заданное 
матрицей A, можно представить как последовательность трех операций: 
1. Поворот/Отражение (V^T). 
2. Растяжение/Сжатие вдоль координатных осей (Σ). Именно сингулярные числа 
на диагонали Σ показывают, во сколько раз происходит растяжение по каждому 
из главных направлений. 
3. Еще один поворот/Отражение (U). 
Как именно используется для снижения размерности? 
1. Мы вычисляем SVD для нашей матрицы данных A. 
2. Смотрим на сингулярные числа в Σ. Первые k чисел — самые большие, а 
остальные близки к нулю. Это значит, что информация в данных в основном 
содержится в первых k компонентах. 
3. Мы аппроксимируем исходную матрицу, отбросив все сингулярные числа, 
начиная с k+1-го. Формально, мы берем только первые k столбцов U, первую k 
x k часть Σ и первые k строк V^T. 
A ≈ U[:, :k] * Σ[:k, :k] * V^T[:k, :] 
Эта операция уменьшает размерность представления наших данных с n до k, 
отфильтровывая "шум" (маловажные компоненты). 
Где применяется в Machine Learning? 
1. Снижение размерности (Аналог PCA): SVD — это вычислительно 
устойчивый способ нахождения главных компонент. Именно так внутри 
работает PCA из sklearn. 
2. Рекомендательные системы: Матрица пользователь-товар огромна и 
разрежена. SVD используется для коллаборативной фильтрации, позволяя 
найти скрытые (латентные) факторы, которые объясняют предпочтения 
пользователей. 
3. Сжатие изображений: Матрицу изображения можно разложить через SVD. 
Оставив только 10% самых больших сингулярных чисел, мы получим 
изображение, которое визуально почти неотличимо от оригинала, но занимает 
гораздо меньше места. (Это та "практически без потери качества", о которой ты 
сказал). 
4. Вычисление псевдообратной матрицы (Moore-Penrose inverse): Что 
критически важно для решения задач линейной регрессии методом наименьших 
квадратов, особенно когда матрица признаков вырождена. 
5. LSA (Latent Semantic Analysis): Извлечение скрытых тем из текстовых 
документов. 
3. Объясните, что такое норма вектора (L1, L2). Где это используется в 
ML? 
Теория вероятностей:  
4. Объясните формулу Байеса. Приведите пример использования.  
Формула: P(A|B) = [ P(B|A) * P(A) ] / P(B) 
Смысл компонентов: 
• P(A|B) (Апостериорная вероятность): Это то, что мы хотим найти. Какова 
вероятность гипотезы A при условии, что событие B произошло. 
• P(A) (Априорная вероятность): Наша исходная, "доопытная" уверенность в 
гипотезе A. 
• P(B|A) (Правдоподобие): Насколько вероятно увидеть доказательство B, если 
наша гипотеза A верна. 
• P(B) (Полная вероятность доказательства): Общая вероятность наступления 
события B при всех возможных гипотезах. 
Пример использования (Медицинская диагностика): 
Предположим, мы тестируем болезнь: 
• Болезнь есть у 1% населения. P(Болезнь) = 0.01 
• Точность теста: 90% (если человек болен, тест положительный с вероятностью 
90%). P(Положительный|Болезнь) = 0.9 
• Ложноположительный результат: 5% (если человек здоров, тест все равно 
может ошибочно быть положительным с вероятностью 5%). 
P(Положительный|Здоров) = 0.05 
Вопрос: Человек получил положительный результат теста. Какова вероятность, что он 
действительно болен? P(Болезнь|Положительный) = ? 
Решение по формуле Байеса: 
1. P(Болезнь) = 0.01 (Наша априорная уверенность) 
2. P(Положительный|Болезнь) = 0.9 (Правдоподобие) 
3. P(Положительный) (Полная вероятность положительного теста) = 
a. (Вероятность быть больным и получить "+") + (Вероятность быть 
здоровым и получить "+") = 
b. P(Болезнь) * P(Положительный|Болезнь) + P(Здоров) * 
P(Положительный|Здоров) 
c. = (0.01 * 0.9) + (0.99 * 0.05) = 0.009 + 0.0495 = 0.0585 
4. Подставляем в формулу: 
a. P(Болезнь|Положительный) = (0.9 * 0.01) / 0.0585 ≈ 0.1538 
Вывод: Вероятность того, что человек действительно болен при положительном 
результате теста, составляет всего около 15.4%, несмотря на кажущуюся высокую 
точность теста (90%). Этот контринтуитивный результат возникает из-за низкой 
априорной вероятности болезни (всего 1%). Формула Байеса позволяет нам корректно 
объединить все эти данные. 
Применение в ML: 
• Наивный байесовский классификатор: Основан на формуле Байеса и 
используется для классификации текстов (спам/не спам), несмотря на его 
"наивное" предположение о независимости признаков. 
• Байесовская оптимизация: Используется для подбора гиперпараметров 
моделей. 
• Генеративные модели: Многие модели, такие как Latent Dirichlet Allocation 
(LDA), основаны на байесовском подходе. 
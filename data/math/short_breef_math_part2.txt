5. В чем разница между условной и безусловной вероятностью?  
1. Безусловная вероятность (Unconditional / Marginal Probability) 
• Что это? Вероятность события в "вакууме", без учета какой-либо другой 
информации или произошедших событий. Это наша исходная, базовая оценка. 
• Обозначение: P(A). 
• Пример: P(Решка) = 0.5. Это наша исходная вера в выпадение решки при 
подбрасывании честной монеты, без каких-либо дополнительных условий. 
2. Условная вероятность (Conditional Probability) 
• Что это? Вероятность того, что событие A произойдет, при условии, что 
событие B уже произошло. 
• Обозначение: P(A|B). 
• Формула: P(A|B) = P(A ∩ B) / P(B), при условии, что P(B) > 0. 
• Пример: P(Решка со 2-й попытки | Решка с 1-й попытки). Условная вероятность 
говорит: "Мы уже знаем, что первый раз выпала решка. Как это знание меняет 
нашу веру во второй исход?" Для честной монеты она останется 0.5, потому что 
события независимы. 
Ключевые различия 
Критерий 
Контекст 
Безусловная 
Вероятность 
Без каких-либо 
условий 
Условная Вероятность 
При наличии условия (произошло событие B) 
Обозначение P(A) 
`P(A 
Суть 
Исходная, 
априорная оценка 
Обновленная, апостериорная оценка на 
основе новых данных 
Важное уточнение: 
• Безусловная вероятность есть у любого события. 
• Понятие "независимые события" — это частный случай, когда наступление 
одного события НЕ меняет вероятность другого: P(A|B) = P(A). В твоем 
примере с кубиками это работает. 
Применение в ML: 
• Наивный Байес: Краеугольный камень этого классификатора — 
предположение о независимости признаков при условии класса, то есть работа с 
условными вероятностями P(Признак_i | Класс). 
• Языковые модели: Вероятность следующего слова в последовательности 
почти всегда является условной вероятностью, зависящей от предыдущих слов: 
P("столе" | "Кот сидит на"). 
6. Что такое распределение вероятностей? Назовите примеры 
(нормальное, Бернулли, Пуассона).  
Что это такое? Распределение вероятностей — это закон, который описывает, как 
вероятности распределены между всеми возможными значениями случайной 
величины. Проще говоря, оно показывает, какие значения случайная величина может 
принимать и насколько часто или вероятно каждое из них. 
Примеры распределений 
1. Нормальное распределение (Гаусса) 
• Что описывает: Непрерывные величины, на которые влияет множество 
случайных факторов (рост, вес, ошибки измерений). 
• Форма: Знаменитая "колоколообразная" симметричная кривая. 
• Параметры: 
o μ (мю) — среднее значение, определяет центр распределения. 
o σ (сигма) — стандартное отклонение, определяет "разброс" или ширину 
колокола. 
• Правило "3-х сигм": 
o ~68% данных лежат в интервале μ ± σ 
o ~95% данных лежат в интервале μ ± 2σ (то, о чем ты говорил: от -2 до 2 в 
стандартном распределении) 
o ~99.7% данных лежат в интервале μ ± 3σ 
2. Распределение Бернулли 
• Что описывает: Результат одного эксперимента с двумя исходами 
(Успех/Неудача, Орёл/Решка, Спам/Не спам). 
• Параметр: 
o p — вероятность успеха (например, P(Успех) = p). 
• Вероятности: 
o P(X=1) = p (Успех) 
o P(X=0) = 1 - p (Неудача) 
3. Распределение Пуассона 
• Что описывает: Число событий, происходящих в фиксированный промежуток 
времени или на фиксированном участке пространства, при условии, что эти 
события происходят независимо друг от друга с постоянной средней 
интенсивностью. 
• Примеры: 
o Количество посетителей магазина в час. 
o Количество вызовов в колл-центр за минуту. 
o Количество опечаток на странице текста. 
• Параметр: 
o λ (лямбда) — среднее количество событий за указанный интервал. 
Почему это важно в Machine Learning? 
• Предположения моделей: Многие алгоритмы делают предположения о 
распределении данных. Например, линейная регрессия часто предполагает, что 
ошибки распределены нормально. 
• Оценка параметров: Метод максимального правдоподобия (MLE), 
используемый для обучения моделей, напрямую опирается на предположение о 
виде распределения. 
• Байесовские методы: В байесовском подходе все параметры модели 
рассматриваются как случайные величины со своими априорными 
распределениями. 
• Генеративные модели: Модели вроде наивного Байеса явно моделируют 
распределения признаков для каждого класса. 
Связь с предыдущим вопросом: Формула Байеса, которую мы разбирали, является 
основным инструментом для работы с распределениями в байесовской статистике, 
позволяя обновлять наши знания о параметрах распределений по мере поступления 
данных. 
7. Что такое Maximum Likelihood Estimation (MLE)? Объясните 
интуицию. 
Maximum Likelihood Estimation (MLE) — это метод оценки параметров 
статистической модели, который находит такие значения параметров, при 
которых функция правдоподобия достигает максимума. 
Интуиция: 
MLE отвечает на вопрос: "Какие параметры модели с наибольшей вероятностью 
породили бы те данные, которые я вижу?" Мы предполагаем вид распределения 
данных (например, нормальное) и затем "подкручиваем" параметры (например, 
среднее и дисперсию), чтобы сделать наблюдаемые данные максимально вероятными. 
Как это работает:  
1. Задается функция правдоподобия L(θ | X) — вероятность наблюдать данные 
X при параметрах θ. 
2. Максимизируется эта функция. На практике почти всегда работают с 
логарифмическим правдоподобием, так как оно преобразует произведение в 
сумму, что упрощает вычисление производной. 
3. Решением является набор параметров θ, доставляющий максимум 
правдоподобия. 
Связь с функциями потерь в ML: 
Минимизация многих функций потерь эквивалентна максимизации правдоподобия: 
• MSE получается из MLE, если предположить нормальное распределение 
ошибок. 
• Binary Cross-Entropy получается из MLE, если предположить распределение 
Бернулли. 
• Cross-Entropy для многоклассовой классификации получается из MLE для 
категориального распределения. 
Таким образом, MLE — это не просто "оценка качества", а фундаментальный 
принцип, который связывает вероятностные предположения о данных с 
практическими алгоритмами их обучения.
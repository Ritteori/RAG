57. Что происходит с нормой градиента при переобучении? 
Градиент — это вектор, который показывает направление и скорость изменения 
функции потерь относительно параметров модели (например, весов). Норма 
градиента — это длина этого вектора, которая отражает, насколько сильно нужно 
изменить веса, чтобы уменьшить ошибку. 
Почему градиент уменьшается в процессе обучения? 
• В начале обучения веса модели случайны или инициализированы не очень 
удачно, поэтому ошибка (функция потерь) обычно большая. 
• Когда ошибка большая, градиент показывает сильное направление 
изменения — норма градиента большая, и веса обновляются 
значительными шагами. 
• По мере того, как модель улучшает свои веса и ошибка уменьшается, 
становится всё сложнее значительно снизить ошибку — модель 
приближается к минимуму функции потерь. 
• В точках, близких к минимуму, функция потерь почти не меняется при 
небольших изменениях параметров, поэтому градиенты становятся 
маленькими — норма градиента уменьшается. 
• Таким образом, уменьшение нормы градиента — это естественный сигнал, 
что модель "учится" и приближается к оптимальному решению. 
Что происходит при переобучении? 
• Если обучение продолжается слишком долго, модель начинает 
подстраиваться под шум и случайные детали тренировочных данных. 
• В этот момент модель "застревает" в локальных минимумах или плато 
функции потерь, где изменения весов мало влияют на ошибку. 
• Градиенты становятся очень малыми, потому что модель уже почти 
идеально описывает тренировочные данные. 
• При этом способность модели обобщать на новые данные падает — это и 
есть переобучение. 
Итог: 
• Уменьшение нормы градиента — нормальный и ожидаемый процесс в 
обучении модели. 
• Очень маленькая норма градиента и отсутствие улучшения качества на 
валидации могут быть сигналом переобучения. 
58. Почему плохо, если градиент слишком маленький (vanishing gradient)? 
Если градиент слишком маленький, это значит, что при обучении нейросети веса 
в первых (и иногда промежуточных) слоях практически не изменяются. Почему так 
происходит? При обратном распространении ошибки градиенты на каждом слое 
вычисляются как произведение производных функций активации и весов 
предыдущих слоев. Если функции активации — например, сигмоида — дают 
маленькие производные (а они всегда меньше или равны 1, часто близки к 0 на 
больших по модулю значениях), то при перемножении много таких маленьких 
чисел итоговый градиент становится очень близким к нулю. 
В результате первые слои нейросети учатся очень медленно или вообще 
перестают обновляться, из-за чего модель не может эффективно обучаться и 
улучшать свои предсказания. Это ухудшает качество обучения, замедляет 
сходимость и может привести к плохой обобщающей способности модели. 
Пример: если в нескольких слоях подряд используется сигмоида, то даже если на 
последнем слое градиент относительно большой (например, 0.25), при 
прохождении обратно через слои с малыми производными (например, 0.01) 
итоговый градиент может стать порядка 0.25 × 0.01 × 0.01 = 0.000025 — слишком 
маленький, чтобы существенно менять веса. 
Таким образом, затухающие градиенты затрудняют обучение глубоких 
нейросетей, особенно с такими активациями, как сигмоида. Поэтому для глубоких 
моделей часто используют функции активации, которые не вызывают сильного 
затухания градиентов (например, ReLU). 
59. Как линейная регрессия решается через матрицы? 
�
� Ответ: 
Линейная регрессия — это задача нахождения таких весов (коэффициентов), при 
которых линейная модель наилучшим образом приближает зависимости между 
признаками и целевой переменной. 
Чтобы решить её через матрицы, используется аналитическое (матричное) 
решение — формула нормального уравнения: 
где: 
• X — матрица признаков (размером n×d, где n — количество объектов, d — 
признаков), 
• y — вектор целевых значений (размером n×1), 
• w — вектор весов (размером d×1), 
• (XTX)−1 — обратная матрица (если существует), 
• XT — транспонированная матрица X. 
�
� Почему это работает: 
Формула получается из условия минимума функции потерь (обычно MSE — 
среднеквадратичная ошибка). Мы ищем такие веса, чтобы сумма квадратов 
ошибок ∥Xw−y∥2 была минимальной. Берем производную по w, приравниваем к 
нулю и решаем. 
Ответ: 
Полученные веса: w0 =1 (bias), w1 =1 (угол наклона линии). 
Значит, модель линейной регрессии: 
y^ =1+1⋅x  
Модель идеально аппроксимирует данные y=x+1, что видно по исходной таблице. 
�
� Итого: 
• Линейную регрессию можно решить вручную через матричную формулу. 
• Решение основано на минимизации функции потерь. 
• Метод точен, но плохо масштабируется при большом числе признаков или 
коллинеарности — тогда используют градиентный спуск или 
регуляризацию. 
Почему нельзя логистическую регрессию решать так 
�
� 1. Нелинейная функция активации 
В логистической регрессии выходной слой использует сигмоиду: 
Это нелинейная функция. А аналитическое решение (нормальное уравнение) 
работает только с линейной зависимостью между входами и выходом. 
То есть: 
В линейной регрессии: 
y^ =Xw(линейно)  
В логистической регрессии: 
y^ =σ(Xw)(нелинейно)  
Из-за наличия сигмоиды нельзя просто взять производную, приравнять к нулю и 
выразить веса — уравнение становится нелинейным и не имеет аналитического 
решения. 
�
� 2. Функция потерь тоже нелинейная 
Для логистической регрессии применяется log-loss (или binary cross-entropy): 
Эта функция также нелинейна, и её нельзя свести к квадратичной форме, как в 
MSE. 
�
� 3. Нет замкнутого аналитического выражения для минимума 
Аналитическое решение возможно, если можно приравнять градиент к нулю и 
решить это уравнение вручную. Но с логистической регрессией: 
• градиент сложнее, 
• функция нелинейна, 
• аналитического минимума не существует. 
Поэтому мы используем итеративные методы, например: 
• градиентный спуск 
• Newton-Raphson 
• stochastic gradient descent (SGD) 
�
� Пример 
Допустим, у тебя есть логистическая модель: 
Найти такие веса w1 =2,w2 =3 через матричную формулу — невозможно. Нужно: 
1. Вычислить логлосс 
2. Найти градиенты по w1  и w2  
3. Обновить веса итеративно 
�
� 60. Как устроена формула аналитического решения линейной регрессии? 
Аналитическое решение для весов линейной регрессии (при наличии признаков X 
и целевой переменной y) выглядит так: 
�
� Что делает эта формула? 
Она находит такие веса w, которые минимизируют среднеквадратичную ошибку 
(MSE) между предсказанными значениями Xw и реальными y: 
MSE(w)=1/n ∥Xw−y∥2
82. Что такое контроль версий для данных? Слышал ли ты о DVC? 
Контроль версий для данных — это практика управления изменениями в наборах 
данных аналогично тому, как Git управляет изменениями кода. Позволяет: 
1. Отслеживать разные версии датасетов (например, dataset_v1.csv, 
dataset_v2.csv). 
2. Воспроизводить эксперименты, четко привязывая результат модели к 
конкретной версии данных, на которых она обучалась. 
3. Совместно работать над проектом, не передавая гигабайты данных по почте, а 
синхронизируя их через удаленное хранилище (S3, GCS, SSH). 
Проблема, которую решает: Git не предназначен для больших и бинарных файлов 
(модели, данные). Хранить их в репозитории — плохая практика. 
DVC (Data Version Control) — это самый популярный инструмент для этого. Работает 
поверх Git. 
• Как работает: 
o Вы указываете DVC, какие файлы данных нужно версионировать (dvc 
add dataset.csv). 
o DVC не кладет сами данные в Git, а создает небольшой текстовый файл
указатель (.dvc-файл), который похож на симлинк и содержит хэш 
данных. 
o Этот .dvc-файл вы коммитите в Git. Сами данные DVC отправляет в 
удаленное хранилище (например, ваше облако S3). 
• Итог: В Git вы храните только легковесный код и .dvc-файлы. Чтобы получить 
нужную версию данных, вы делаете git checkout на нужный коммит, а затем 
команду dvc pull — DVC по .dvc-файлу понимает, какую версию данных из 
облака скачать. 
• Аналогия: .dvc-файл — это чек на получение багажа (данных). Git
репозиторий — это ваш паспорт с кучей чеков. Удаленное хранилище — это 
камера хранения с самим багажом. Чтобы получить багаж, вы предъявляете 
паспорт с правильным чеком. 
Вывод: DVC — это "Git для данных", необходимый инструмент для 
воспроизводимости ML-экспериментов. 
Библиотеки:  
83. В чем разница между NumPy массивами и Python списками?  
Разница фундаментальна и лежит в архитектуре, производительности и 
предназначении. 
1. Тип данных и производительность: 
a. Python list: Это гетерогенная коллекция указателей на произвольные 
объекты в памяти (числа, строки, другие списки). Это гибко, но очень 
медленно для численных операций, так как каждый элемент требует 
отдельного разыменования и проверки типа. 
b. NumPy ndarray: Это гомогенный, плотный массив элементов одного 
типа (int32, float64 и т.д.), расположенный в непрерывном блоке памяти. 
Это позволяет выполнять векторизованные операции, написанные на 
быстром C/Fortran, обходя интерпретатор Python. Это дает прирост 
скорости в 10-100 раз. 
2. Многомерность и синтаксис: 
a. List: Многомерная структура (например, 2D) — это "список списков". 
Индексирование: list[i][j]. Размеры могут быть "рваными". 
b. Ndarray: Это истинный n-мерный массив с единым синтаксисом array[i, 
j]. Имеет четкую форму (shape) и поддерживает продвинутую 
индексацию (срезы, булевы маски). 
3. Операции: 
a. List: Операции + и * означают конкатенацию и повторение. 
b. Ndarray: Эти же операции выполняются поэлементно (element-wise), 
что является основой линейной алгебры. Также поддерживает всю 
математику (np.sin(arr), arr.sum(), матричное умножение @). 
Итог: List — универсальный контейнер для любых объектов. NumPy array — 
специализированная, высокопроизводительная структура для численных данных, 
которая является основой всего научного стека Python (pandas, scikit-learn, PyTorch). 
84. Что такое pandas DataFrame? Как сделать группировку или применить 
функцию к колонке? 
pandas.DataFrame — это двумерная, помеченная табличная структура данных с 
изменяемым размером. Столбцы могут быть разных типов (dtypes). Это аналог 
таблицы в БД или листа Excel в памяти, построенный для эффективной обработки и 
анализа. 
1. Группировка (GroupBy): 
Метод .groupby() не возвращает готовый DataFrame. Он создает объект "GroupBy", 
описывающий, как данные должны быть сгруппированы. 
Чтобы получить результат, нужно применить к этому объекту агрегирующую 
функцию: 
python 
# Сгруппировать по колонке 'department' и посчитать среднюю зарплату в каждом отделе 
df.groupby('department')['salary'].mean() 
# Сгруппировать по нескольким колонкам и применить несколько агрегаций 
df.groupby(['department', 'role']).agg({'salary': ['mean', 'min', 'count'], 'age': 'median'}) 
2. Применение функции к колонке: 
Есть несколько способов, от самого быстрого к самому медленному: 
• Векторизованные операции (лучший способ): Используйте встроенные 
операции pandas/NumPy, которые работают со всем столбцом сразу. 
python 
df['new_col'] = df['old_col'] * 2  # Быстро и идиоматично 
• Метод .map() (для Series): Для преобразования значений по словарю или 
простой функции. 
python 
df['category_code'] = df['category'].map(category_dict) 
• Метод .apply() (самый гибкий, но медленный): Используется, когда нет 
векторизованной альтернативы. Применяет функцию к каждому элементу Series 
или к каждой строке/столбцу DataFrame. 
python 
df['squared'] = df['value'].apply(lambda x: x**2)  # Для Series 
df['text_length'] = df['text_column'].apply(len)    # Использование встроенной функции 
• Метод .assign(): Удобный способ создания новых колонок в цепочке вызовов. 
python 
df = df.assign(log_value = lambda x: np.log(x['value'])) 
Ключ: Всегда старайтесь использовать векторизованные операции, так как они на 
порядки быстрее .apply() с циклом на Python. 
85. Для чего используется библиотека Scikit-learn? 
Scikit-learn — это основная библиотека Python для классического (non-deep) 
машинного обучения. Она предоставляет согласованный (consistent) и простой API 
для всего цикла работы с моделью. 
Для чего используется (основные модули): 
1. Предобработка данных (sklearn.preprocessing, sklearn.feature_extraction): 
Кодирование категориальных признаков, масштабирование, работа с текстом 
(TF-IDF, CountVectorizer). 
2. Выбор и преобразование признаков (sklearn.feature_selection, 
sklearn.decomposition): Отбор фич, PCA, SVD. 
3. Классические алгоритмы ML (сквозной API fit/predict/transform): Линейные 
модели, SVM, ансамбли (Random Forest, Gradient Boosting*), кластеризация (K
Means), байесовские методы. 
4. Инструменты оценки и настройки (sklearn.model_selection): Кросс
валидация, разделение выборки, поиск по сетке (GridSearchCV). 
5. Метрики (sklearn.metrics): Все стандартные метрики классификации, 
регрессии, кластеризации. 
Роль в экосистеме: 
• Промышленный стандарт для задач, где достаточно "мелкого" ML или 
инженерных признаков. Его используют в продакшене, часто как часть 
пайплайна (sklearn.pipeline.Pipeline) для создания воспроизводимых цепочек 
преобразований и моделей. 
• XGBoost/LightGBM/CatBoost часто имеют scikit-learn-совместимые 
интерфейсы (через XGBClassifier и т.д.), что позволяет интегрировать их в те 
же пайплайны. 
• Отличие от PyTorch/TensorFlow: Scikit-learn — для задач, где фичи создает 
инженер. PyTorch/TF — для глубокого обучения, где нейросеть сама учится 
извлекать признаки из сырых данных (изображения, текст). 
Итог: Scikit-learn — это не "учебная" библиотека, а профессиональный, надежный 
инструмент для широкого спектра практических задач ML, известный своим чистым 
дизайном и качеством реализации.  
86. Для чего используются библиотеки PyTorch и TensorFlow? В чем их 
основное различие (динамический vs статический граф)? 
PyTorch и TensorFlow — это две основные библиотеки для глубокого обучения 
(нейронных сетей). Они позволяют легко создавать архитектуры сетей, автоматически 
считать градиенты (что необходимо для обучения) и эффективно использовать 
видеокарты (GPU). 
Ключевое различие (динамический vs статический граф): 
Чтобы понять разницу, нужно знать, что такое вычислительный граф. 
• Вычислительный граф — это представление всех вычислений в вашей модели 
в виде графа (узлов и связей). Узлы — это операции (сложение, умножение, 
функция активации), а связи — это данные (тензоры). Библиотеки используют 
такие графы для оптимизации и расчета градиентов. 
• Статический граф (классический TensorFlow 1.x): Граф строится один раз, 
заранее, до начала обучения. Вы сначала объявляете все операции ("здесь 
будет вход", "здесь умножение"), а потом в отдельном шаге запускаете этот 
граф, подавая в него данные. Это как составить подробный план-чертеж всей 
стройки до того, как завезти материалы. 
o Плюсы: Граф можно сильно оптимизировать, легко развернуть на 
мобильных устройствах. 
o Минусы: Сложно отлаживать, код менее интуитивный. 
• Динамический граф / Define-by-Run (PyTorch, TensorFlow 2.x в eager
режиме): Граф строится на лету, в момент выполнения каждой операции 
вашего кода. Вы написали строку c = a + b — она сразу выполнилась, и в граф 
добавился узел сложения. Это как строить дом, принимая решения прямо на 
стройплощадке. 
o Плюсы: Очень естественно для разработки, легко отлаживать 
стандартными средствами Python. 
o Минусы: Меньше возможностей для предварительной оптимизации. 
Современная ситуация (сближение): 
• TensorFlow 2.x теперь по умолчанию работает в режиме динамического графа 
(Eager Execution), как PyTorch, что упрощает разработку. Но он сохранил 
возможность "замораживать" код в статический граф с помощью декоратора 
@tf.function для ускорения в продакшене. 
• PyTorch остался верен динамическому подходу, что сделало его фаворитом в 
исследованиях. Для продакшена он тоже умеет создавать статические графы 
через TorchScript. 
Итог для выбора: 
• PyTorch: Часто предпочитают для исследований и прототипирования из-за 
гибкости и простоты отладки. 
• TensorFlow: Часто выбирают для промышленного развертывания 
(продакшена) из-за мощного набора инструментов (TensorFlow Serving, 
TensorFlow Lite) и лучшей поддержки специализированных процессоров Google 
(TPU).
Алгоритмы ML — понимание и Применение 
�1. Общие понятия 
1. Что такое обучение с учителем и без учителя? 
Обучение с учителем (Supervised Learning): 
• Определение: Модель обучается на размеченных данных, где каждому 
входному примеру (например, изображению) сопоставлен правильный 
ответ (таргет). 
• Пример: Классификация изображений. Например, если у нас есть 
изображения кошек и собак, модель будет учиться различать их, имея 
метки (таргеты) для каждого изображения. 
• Методы: Логистическая регрессия, SVM, нейронные сети, деревья 
решений. 
Обучение без учителя (Unsupervised Learning): 
• Определение: Модель обучается на неразмеченных данных, где нет 
явных меток. Модель должна сама найти структуру или закономерности в 
данных. 
• Пример: Кластеризация (например, группировка пользователей по 
поведению на сайте) или снижение размерности (например, уменьшение 
размерности данных с помощью PCA). 
• Методы: Кластеризация (K-means, DBSCAN), снижение размерности (PCA), 
автоэнкодеры. 
Обучение с подкреплением (Reinforcement Learning): 
• Определение: Модель обучается, взаимодействуя с окружающей средой и 
получает награды или наказания за выполненные действия. Задача — 
максимизировать долгосрочную награду. 
• Пример: Модели, обучающиеся играть в игры (например, AlphaGo), где 
модель получает награду за победу и наказание за проигрыш. 
• Методы: Q-learning, Deep Q-Networks (DQN), Policy Gradient методы. 
2. Что такое признаки (features) и целевая переменная (target)? 
Признаки (Features): 
• Определение: Признаки (или фичи) — это входные данные, которые 
модель использует для предсказания. Каждый признак может быть 
числовым или категориальным и описывает определённую характеристику 
объекта. 
• Пример: В задаче предсказания роста человека, фичами могут быть: 
o Вес, 
o Размер стопы, 
o Возраст. 
• В нейронных сетях фичами могут быть: 
o Пиксели изображения (для задач компьютерного зрения), 
o Слова в тексте (для задач обработки естественного языка), 
o Мелкие признаки, такие как коэффициенты в свертках (для 
сверточных нейронных сетей). 
Целевая переменная (Target): 
• Определение: Целевая переменная (таргет) — это значение, которое мы 
пытаемся предсказать на основе признаков. Это результат, с которым 
модель сравнивает свои предсказания во время обучения. 
• Пример: В задаче с ростом человека: 
o Таргет — это сам рост человека, который модель пытается 
предсказать, используя признаки (вес, размер стопы). 
В задаче классификации (например, классификация изображений): 
o Таргет — это категория или метка (например, "собака", "кошка"). 
3. Что такое переобучение (overfitting)? 
Определение: 
• Переобучение (overfitting) — это ситуация, когда модель слишком хорошо 
подстраивается под тренировочные данные и теряет способность 
обобщать на новые, невиденные данные. 
• Модель может запомнить даже шум и малозначимые паттерны в 
тренировочных данных, что ухудшает её способность делать точные 
предсказания на новых примерах. 
Причины переобучения: 
• Маленький размер датасета: Если данных слишком мало, модель может 
«запомнить» их и не научиться обобщать. 
• Сложная модель: Глубокие нейросети с большим количеством параметров 
имеют высокую способность к запоминанию и могут переобучаться, если 
данных недостаточно. 
• Чрезмерное количество эпох: Если модель обучается слишком долго, она 
может начать переобучаться, и её ошибка на валидации начнёт 
увеличиваться, в то время как ошибка на тренировке будет уменьшаться. 
Признаки переобучения: 
• Точность на тренировочных данных продолжает расти (приближается к 
100%), но точность на валидационных данных начинает падать. 
Методы борьбы с переобучением: 
1. Аугментация данных: Увеличение разнообразия тренировочных данных с 
помощью различных трансформаций изображений (например, флипы, 
кропы, блюры). 
2. Регуляризация: Добавление штрафов на слишком большие веса 
(например, L1/L2 регуляризация) или использование Dropout, чтобы 
случайным образом отключать нейроны и предотвращать слишком 
сильную зависимость от некоторых признаков. 
3. Ранняя остановка (Early Stopping): Остановка обучения, когда 
производительность на валидационных данных начинает ухудшаться, даже 
если точность на тренировочных данных продолжает расти. 
4. Что такое недообучение (underfitting)? 
Определение: 
• Недообучение (underfitting) — это ситуация, когда модель не обучилась 
достаточно хорошо и не может захватить важные закономерности в 
данных. 
• Модель недостаточно сложная или её обучение завершено слишком рано, 
из-за чего она не достигает высокого уровня точности. 
Причины недообучения: 
1. Недостаточно сложная модель: Например, использование линейной 
модели для задачи, которая требует более сложной (например, 
использование нейросети вместо линейной регрессии для сложной 
зависимости). 
2. Недостаточное количество эпох: Если модель не обучалась достаточно 
долго и не успела извлечь все полезные паттерны из данных. 
3. Неоптимальные гиперпараметры: Например, слишком высокий 
коэффициент регуляризации, который слишком сильно ограничивает 
модель. 
4. Нехватка данных: Модели не хватает информации, чтобы сделать точные 
предсказания, и она не может научиться. 
Признаки недообучения: 
• Точность модели остаётся низкой как на тренировочных данных, так и на 
валидационных, не происходит значительного улучшения в процессе 
обучения. 
• Лосс не снижается должным образом или снижается очень медленно. 
Методы борьбы с недообучением: 
1. Увеличение сложности модели: Например, использование более 
глубокой нейросети или более мощных моделей. 
2. Увеличение количества эпох: Позволяет модели научиться извлекать 
более сложные зависимости из данных. 
3. Использование большего объёма данных: Может помочь модели лучше 
понять различные паттерны в данных. 
5. Чем отличается модель с высокой и низкой вариансой? 
Варианс (Variance): 
• Определение: Варианс — это мера того, как сильно изменяются 
предсказания модели при изменении обучающих данных. Чем выше 
варианс, тем больше модель "скачет" от одного набора данных к другому. 
Модель с высокой вариансой: 
• Характеристика: Такая модель сильно реагирует на изменения в 
обучающих данных. Она хорошо подстраивается под тренировочные 
данные (переобучается), но плохо обобщает на новых данных. 
• Признак: Высокий варианс связан с переобучением (overfitting), когда 
модель слишком точно подгоняет себя под шум или случайные колебания в 
тренировочных данных. 
• Пример: Глубокая нейросеть на маленьком датасете, которая даёт высокую 
точность на тренировочных данных, но плохо работает на тестовых данных. 
Модель с низкой вариансой: 
• Характеристика: Модель меньше чувствительна к изменениям в данных и 
имеет меньший разброс в предсказаниях. Она будет более стабильной, но 
может не уловить все зависимости, что приведёт к недообучению. 
• Признак: Низкий варианс связан с недообучением (underfitting), когда 
модель слишком простая и не может захватить важные паттерны в данных. 
• Пример: Линейная регрессия на сложной задаче, где данные имеют 
нелинейную зависимость. 
Важность в контексте bias-variance tradeoff: 
• Bias (смещение) — это ошибка, которая возникает, когда модель слишком 
проста и не может правильно обобщать (недообучение). 
• Variance (варианс) — это ошибка, возникающая, когда модель слишком 
сложная и переобучается на тренировочных данных. 
• Bias-variance tradeoff — это баланс между смещением и вариансом. Цель 
модели — минимизировать оба, но обычно уменьшение одного ведёт к 
увеличению другого. 
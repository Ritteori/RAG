�
�3. Деревья решений и ансамбли 
21.Как устроено дерево решений? 
Дерево решений — устройство и принцип работы: 
1. Структура: 
a. Корень (root) — первый узел, с которого начинается разбиение. 
b. Внутренние узлы (internal nodes) — точки разбиения по признакам. 
c. Листья (leaves) — конечные узлы, где принимается решение (класс 
или предсказанное значение). 
2. Принцип работы: 
a. Дерево рекурсивно делит данные по признакам, чтобы 
минимизировать неопределённость внутри каждого подмножества. 
b. Критерии разбиения: энтропия, индекс Джини, 
среднеквадратичная ошибка (для регрессии). 
3. Если-иначе: 
a. Каждое решение внутри дерева эквивалентно условию if-else: 
«Если признак X > порог, идем в левое поддерево, иначе — в правое». 
4. Конечное решение: 
a. В листьях хранится предсказанный класс (для классификации) или 
среднее значение (для регрессии). 
22.Что такое критерии разделения (например, энтропия, Gini)? 
Критерии разделения в деревьях решений: 
1. Энтропия (Entropy): 
a. Мера неопределённости или «смешанности» данных в узле. 
b. Формула для узла с K классами: 
Чем меньше значение энтропии, тем более «чистый» узел. 
2. Индекс Джини (Gini Index): 
• Альтернатива энтропии, проще в вычислении: 
• Меньший Gini = более однородный узел. 
Идея: дерево выбирает разбиение признака, которое максимально снижает 
энтропию или Gini, т.е. делает узлы более «чистыми». 
23.Как дерево решает, когда остановиться? 
Когда дерево решает остановиться: 
• Теоретически: дерево продолжает разбиения, пока в узле не окажется 
объекты одного класса или пока не закончились признаки. 
• Практически: остановка нужна, чтобы избежать переобучения. 
Используются ограничения/гиперпараметры: 
o max_depth — максимальная глубина дерева; 
o min_samples_split — минимальное число объектов для разделения 
узла; 
o min_samples_leaf — минимальное число объектов в листе; 
o max_features — ограничение числа признаков, по которым ищется 
лучшее разбиение. 
• Эти параметры помогают балансировать точность и обобщающую 
способность модели. 
Пример: если max_depth=3, дерево остановится на 3 уровне даже если узлы не 
чистые. 
24.Что такое переобучение дерева решений? 
• Определение: переобучение (overfitting) возникает, когда дерево слишком 
точно подстраивается под тренировочные данные, запоминая шум и 
случайные паттерны вместо общих закономерностей. 
• Признаки: 
o Отличная точность на тренировочных данных, но низкая на 
валидационных или тестовых; 
o Дерево очень глубокое, листья содержат по 1–2 объекта; 
o Чувствительно к малейшим изменениям в данных. 
• Как избежать: 
o Ограничение глубины (max_depth); 
o Минимальное число объектов для разбиения (min_samples_split) 
или в листе (min_samples_leaf); 
o Использование ансамблей (Random Forest, Boosting). 
Пример: дерево без ограничений на глубину запоминает все отдельные объекты и 
не обобщает новые данные. 
25.Что такое Random Forest? 
• Определение: ансамбль из множества независимых деревьев решений, 
где каждый дерево обучается на случайной подвыборке данных и 
случайном подмножестве признаков. 
• Как работает: 
o Строятся N деревьев (обычно ≥100); 
o Каждое дерево делает предсказание; 
o Для классификации: итоговый класс выбирается голосованием 
(majority vote); 
o Для регрессии: предсказания усредняются. 
• Преимущества: 
o Устойчивость к шуму и выбросам; 
o Снижение переобучения по сравнению с одиночным деревом; 
o Хорошая обобщающая способность без жесткой настройки 
отдельных деревьев. 
Пример: 100 деревьев классифицируют объект. 70 деревьев дали класс 2, 30 — 
класс 1 → итоговый класс = 2. 
26.Как работает метод бэггинга? 
• Главная идея: уменьшить варианс модели без сильного увеличения 
смещения. 
• Почему работает: одиночные модели (особенно деревья решений) сильно 
чувствительны к шуму в данных. Объединяя предсказания нескольких 
моделей, каждая из которых обучена на немного разных данных, мы 
«сглаживаем» случайные ошибки. 
• Процесс: 
o Bootstrap: создаём N подвыборок исходного датасета с 
возвращением (может попасть один и тот же объект несколько раз). 
o Обучение отдельных моделей: каждая подвыборка даёт своё 
дерево решений (или другую базовую модель). 
o Агрегация: 
▪ Для классификации: большинство голосов (majority voting); 
▪ Для регрессии: среднее предсказание. 
• Подковырки / важные нюансы: 
o Если базовые модели слишком простые (high bias), бэггинг мало 
помогает — уменьшает только variance, bias остаётся. 
o Бэггинг эффективен на нестабильных алгоритмах, например, 
дерево решений, а на линейной регрессии эффект минимален. 
o Количество моделей N — гиперпараметр: слишком мало — мало 
эффекта; слишком много — рост вычислительной нагрузки без 
значимого прироста. 
o Случайные подвыборки делают результат стохастическим, поэтому 
иногда повторное обучение даёт чуть разные результаты. 
o Random Forest — это фактически бэггинг деревьев + случайный 
выбор признаков при разбиении (для дополнительного снижения 
корреляции деревьев). 
Пример: 
• Исходный датасет: 1000 объектов, 20 признаков. 
• N = 10 подвыборок по 1000 объектов с возвращением. 
• На каждой подвыборке строим дерево. 
• Итоговое предсказание для объекта: голосование (классификация) или 
усреднение (регрессия). 
27.Что такое градиентный бустинг? 
• Главная идея: строим ансамбль деревьев последовательно, каждое новое 
дерево пытается исправить ошибки предыдущего. 
• Важное отличие от бэггинга / Random Forest: 
o В бэггинге деревья независимые и голосуют; 
o В градиентном бустинге деревья зависят друг от друга, каждое 
корректирует ошибку предыдущего. 
Как работает пошагово: 
1. Начальное предсказание: обычно среднее значение таргета (регрессия) 
или логиты для классификации. 
a. Пример (регрессия): предсказание  
2. Вычисляем ошибки (residuals): 
a. Для каждого объекта:  
b. Эти ошибки — это число, которое мы можем использовать как таргет 
для следующего дерева. 
3. Строим новое дерево на этих residuals: 
a. Оно пытается предсказать отклонение, а не сам класс. 
b. После обучения корректируем предсказание: 
где η — learning rate, T1 (x) — предсказание дерева. 
4. Повторяем: строим дерево 2 на ошибках дерева 1, дерево 3 на ошибках 
суммы деревьев 1+2, и так далее. 
5. Итоговое предсказание: 
a. Регрессия: сумма предсказаний всех деревьев, скорректированных 
learning rate 
b. Классификация: обычно логиты суммируются, потом пропускаются 
через sigmoid (бинарная) или softmax (многоклассовая) 
Подковырки / важные нюансы: 
• Даже в классификации на каждом шаге дерево предсказывает числа, а не 
классы. Это логиты или ошибки, которые потом агрегируются. 
• Learning rate (η) очень важен: маленький — обучение стабильное, но 
медленное; большой — быстро, но риск переобучения. 
• Количество деревьев — баланс между bias и variance: слишком мало — 
недообучение, слишком много — переобучение.
28.Как Boosting улучшает производительность моделей? 
Последовательное обучение слабых моделей 
• В отличие от Random Forest, где деревья обучаются независимо и потом 
усредняются, в Boosting каждое дерево зависит от предыдущих. 
• Каждое новое дерево «смотрит» на ошибки предыдущего: на объектах, где 
предыдущие деревья ошиблись, даётся больший вес, чтобы новая модель 
их исправила. 
Что передаётся следующей модели 
• В классификации это скорее «сырые предсказания» или логиты, а не 
финальный класс. 
• Ошибка вычисляется как разница между таргетом и предсказанием 
предыдущего ансамбля. 
• Новое дерево учится аппроксимировать именно эту ошибку. 
• Это похоже на градиентный спуск, но вместо весов нейросети мы 
оптимизируем выходные значения деревьев. 
Итеративное улучшение 
• На каждом шаге комбинируем предыдущие предсказания с текущим 
деревом через взвешенную сумму: 
где Fm (x) — текущее суммарное предсказание, hm (x) — предсказание нового 
дерева, η — learning rate. 
• В итоге ошибки уменьшаются экспоненциально на сложных объектах, а 
на простых моделям не нужно тратить лишнюю мощность. 
Подковырки, о которых часто забывают 
• Boosting не увеличивает дисперсию так сильно, как Random Forest, 
потому что деревья слабые (обычно глубина 1–3). 
• Learning rate (η) критичен: слишком большой → переобучение, слишком 
маленький → медленная сходимость. 
• Ошибки передаются как веса для объектов (в AdaBoost) или как 
градиенты потерь (в Gradient Boosting). 
• Итоговый прогноз не обязательно простой средний, а взвешенный с 
учётом качества деревьев. 
На практике 
• AdaBoost: веса ошибок напрямую увеличивают влияние неправильно 
предсказанных объектов. 
• Gradient Boosting / XGBoost / LightGBM: вычисляют градиенты функции 
потерь и используют их как «правильное направление» для следующего 
дерева.
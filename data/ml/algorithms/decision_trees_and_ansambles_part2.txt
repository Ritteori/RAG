29.Чем XGBoost и LightGBM отличаются от классического 
градиентного бустинга? 
Коротко в двух строчках 
• Классический Gradient Boosting (альфа-версия) — последовательное 
обучение слабых моделей (обычно shallow trees), каждое дерево учится на 
ошибках предыдущих. 
• XGBoost — это «продвинутый» Gradient Boosting с регуляризацией, второй 
порядковой информацией (Newton), оптимизациями по скорости и памяти, 
надёжной реализацией (парралелизация, out-of-core, GPU). 
• LightGBM — ещё более быстрый и экономный по памяти вариант с 
гистограммами, листово-ориентированным ростом деревьев (leaf-wise), 
GOSS и EFB — алгоритмы для ускорения и уменьшения размерности (но с 
нюансами по переобучению). 
1) Что внутри Gradient Boosting (основа) 
Идея: модель Fm (x) строится как сумма предыдущих моделей: 
где hm  — новое дерево, η — learning rate. hm  обучается аппроксимировать 
градиент (или residual) функции потерь по текущему предсказанию. Для многих 
реализаций используют второй порядок (градиент + гессиан) — это даёт более 
быстрое и точное обновление (Newton step). 
Общий оптимизационный вид (XGBoost style): 
где Ω — регуляризационный член для деревьев. 
2) Что привнёс XGBoost (основные фишки) 
Ключевые особенности XGBoost: 
• Тейлор-разложение 2-го порядка в оптимизации (используются и 
градиенты, и гессианы), что даёт точные апдейты и формулу «gain» при 
поиске сплитов. 
• Оптимизированный поиск разбиений: exact и approx (приближённый 
подсчёт с букетами/гистограммами), а также sparsity-aware (обработка 
пропусков). 
• Параллельизация и кеш-ориентированная оптимизация → хорошая 
скорость на CPU. 
• Поддержка out-of-core (работа с данными, которые не влезают в память) и 
GPU (ускорение расчёта градиентов/сплитов). 
• Проверенная стабильность и широкие возможности настройки (gamma, 
max_depth, min_child_weight, reg_alpha/reg_lambda, subsample, 
colsample_bytree и т.д.). 
Важная формула (gain при сплите — откуда видно, почему регуляризация важна): 
где G,H — суммы градиентов/гессианов в левой/правой части. 
Подковырки XGBoost: 
• По умолчанию XGBoost делает level-wise (глубина/уровень) рост дерева — 
это более «равномерно» и контролируемо по сложности, но медленнее. 
• Хорошая регуляризация — ключ к стабильным результатам; без неё легко 
переобучиться при больших деревьях и малом learning_rate. 
• XGBoost исторически требователен к памяти при exact split, поэтому 
используют hist/approx. 
3) Что привнёс LightGBM (основные фишки) 
LightGBM оптимизирован для скорости на больших датасетах: 
• Histogram-based learning (список бинов, а не точные значения) — резко 
экономит память и ускоряет вычисления. 
• Leaf-wise (best-first) рост деревьев: на каждом шаге выбирается лист с 
наибольшим приростом и он расщепляется. Это даёт более быстрое 
уменьшение ошибки, но может приводить к глубоким узлам (и 
переобучению), если не ограничивать параметры. 
• GOSS (Gradient-based One-Side Sampling) — при обучении оставляют все 
объекты с большими градиентами и случайно подбирают небольшую долю 
с малыми градиентами (с компенсацией веса). Это снижает число строк, 
ускоряет обучение, при этом важные объекты (с большими ошибками) 
остаются. 
• EFB (Exclusive Feature Bundling) — сжимает практически 
«взаимоисключающие» (почти разрежённые) признаки в один фич-бандл, 
чтобы уменьшить количество признаков. 
• Поддержка категориальных фич (встроенная обработка, быстрее, чем 
one-hot) — LightGBM умеет принимать список категорий и оптимизирует 
сплиты. 
• Очень быстрая тренировка на больших данных, небольшая память. 
Подковырки LightGBM: 
• Leaf-wise даёт лучшие train-loss, но в мелких/шумных датасетах — риск 
переобучения. Обязательно контролируй num_leaves, min_data_in_leaf, 
max_depth. 
• GOSS и EFB — круто для скоростного прироста, но они вводят эвристики; на 
очень маленьких датасетах их применять не стоит. 
• LightGBM по умолчанию использует много потоков — будь осторожен с 
reproducibility (параметры seed, deterministic flags). 
• Параметр max_bin влияет на качество/скорость: слишком малый — потеря 
точности, слишком большой — больше памяти. 
4) Сравнительная табличка (ключевые отличия) 
Классический 
Аспект 
Стратегия 
роста 
дерева 
Сплит
поиск 
Регуляриза
ция 
GB 
(sklearn.gbm) 
обычно level/по 
умолчанию 
точный 
простая 
XGBoost 
level-wise (или hist) 
exact/approx/hist 
L1/L2 + leaf penalty γ 
Ускорения нет/ограниченно параллельность, 
GPU, out-of-core 
LightGBM 
leaf-wise (histogram) 
histogram (fast) 
L1/L2 + параметры 
детерминизма 
GOSS, EFB, histogram, 
быстрый multi-thread 
one-hot 
встроенная поддержка 
Категориа
льные 
Риск 
переобуче
ния 
Когда 
выбрать 
recent support 
(workarounds) 
зависит 
простой/учебны
й 
контролируемый 
когда нужна 
стабильность/точнос
ть, GPU 
выше при некорректной 
настройке (leaf-wise) 
большие dataset, фичи 
много, хочется скорости 
5) Практические «подковырки» и советы по настройке (очень важные) 
1. Всегда ставь small learning_rate (0.01–0.1) и увеличивай n_estimators — 
это безопаснее. Затем используй early_stopping_rounds (cv или 
валидация). 
2. Для LightGBM следи за num_leaves: примерно num_leaves ≈ 
2^{max_depth}. Большие num_leaves дают сложные деревья → risk of 
overfit. 
3. min_child_samples / min_child_weight / min_data_in_leaf — ключ к 
предотвращению tiny leaves (особенно в leaf-wise LightGBM). 
4. subsample / colsample_bytree / feature_fraction — снижают корреляцию 
деревьев и переобучение (как в RF). 
5. XGBoost: gamma — минимальный gain для сплита; ставить >0 заставляет 
делать только «существенные» сплиты. 
6. Регуляризационные параметры: reg_alpha (L1), reg_lambda (L2) — 
полезны при шумных данных. 
7. Категориальные признаки: LightGBM может принять 
categorical_feature целиком. XGBoost исторически требовал one-hot 
или target encoding, но последние версии добавили native categorical 
methods — проверяй docs. 
8. Missing values: обе библиотеки умеют «развешивать» пропуски в сплитах 
(sparsity aware), т.е. не обязательно заполнять NaN. 
9. Feature importance: есть разные типы важности — gain, split count, cover. 
gain часто полезнее, но может переоценивать численные встречи. Не 
путай с causal interpretation. 
10. Reproducibility: установить random_state, deterministic режимы и 
num_threads=1 для воспроизводимости; GPU может давать небезупречную 
детерминированность. 
11. GPU: XGBoost — стабильная GPU поддержка; LightGBM тоже, но настройка 
сборки для GPU может быть хитроватой. 
12. Small datasets: LightGBM + leaf-wise часто переобучает; лучше XGBoost 
(level-wise) или уменьшить num_leaves/увеличить min_data_in_leaf. 
13. Class imbalance: используйте scale_pos_weight (XGBoost) или 
is_unbalance/class_weight (LightGBM) и/или focal loss или sampling. 
6) Формула регуляризации XGBoost (чтобы понимать, откуда берутся 
hyperparams) 
Объектив: 
— γ штрафует за число листьев (т.е. за сложность дерева), λ — L2 на веса листьев. 
Gain formula (см. выше) показывает, что сплит делают только если прирост 
больше γ. Это практическая регуляризация — полезно знать на собеседовании. 
7) Когда что выбирать (практические рекомендации) 
• LightGBM — если у тебя very large dataset (много строк), много фичей, важна 
скорость и память. Отлично если есть категориальные признаки: 
встроенная поддержка. Но будь аккуратен с leaf-wise на малых/шумных 
данных. 
• XGBoost — если тебе нужна стабильность и контролируемость, хорошая 
поддержка GPU, немного меньший риск переобучения «из коробки» (при 
разумных настройках). Часто даёт сопоставимый/лучший результат в 
соревнованиях при тщательной настройке. 
• Если много категориальных и мало времени на предобработку → подумай 
про CatBoost (он специально для категорий), но это отдельная история. 
8) Частые «ловушки» (возьми на память) 
1. Не ставить early_stopping → легко переобучиться. 
2. Для LightGBM ставить слишком большой num_leaves → глубокие листья → 
переобучение. 
3. Отсутствие регуляризации в XGBoost (gamma, reg_lambda) → модель 
«вырастет» и переобучит. 
4. Ожидать, что GPU избавит от тонкой тюнинговки — нет, параметры сами 
по себе влияют на качество. 
5. Сравнивать важности фич между разными алгоритмами — некорректно; 
разные определения importance дадут разные ранжирования. 
6. Использовать default параметры на production данных без проверки — 
риск плохой generalization. 
9) Короткая шпаргалка для собеседования (что сказать) 
• Gradient Boosting: последовательное исправление ошибок, суммирование 
слабых моделей. 
• XGBoost: second-order approx (градиент+гессиан), регуляризация γ,λ, fast 
parallel, hist/approx, GPU/out-of-core. 
• LightGBM: histogram + leaf-wise growth, GOSS + EFB (ускорения и 
уменьшение признаков), встроенная поддержка категориальных 
признаков, быстрее и экономичнее, но leaf-wise — риск overfit. 
• Практически: tune learning_rate, n_estimators, max_depth/num_leaves, 
min_child_samples, subsample, colsample_bytree, reg_alpha/lam, 
early_stopping. 
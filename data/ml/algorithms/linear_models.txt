2. Линейные модели 
11.Что такое линейная регрессия? 
Определение: 
• Линейная регрессия — это метод обучения с учителем, который 
используется для предсказания числовых значений. Она находит 
линейную зависимость между входными признаками (например, площадь 
дома, местоположение, площадь участка) и целевой переменной 
(например, цена дома). 
Принцип работы: 
• Линейная регрессия строит модель в виде линии (или гиперплоскости для 
многомерных данных), которая минимизирует ошибку между 
предсказанными и реальными значениями. 
• Модель представляется в виде уравнения: 
y=w1 x1 +w2 x2 +⋯+wn xn +b 
Где: 
o y — предсказанное значение (например, цена дома), 
o x1 ,x2 ,…,xn  — признаки (например, площадь, количество комнат, 
местоположение), 
o w1 ,w2 ,…,wn  — веса (коэффициенты), которые мы ищем, 
o b — смещение (интерцепт). 
Обучение модели: 
• Во время обучения линейная регрессия ищет оптимальные веса (w1 ,w2 
,…,wn ) и сдвиг b, которые минимизируют ошибку предсказания. 
• Для этого используется метод наименьших квадратов (OLS), который 
минимизирует сумму квадратов отклонений предсказанных значений от 
реальных. 
Аналитическое решение: 
• В линейной регрессии можно найти оптимальные веса с помощью 
аналитической формулы: 
w=(XTX)−1XTy 
Где: 
o X — матрица признаков, 
o y  вектор целевых значений, 
o w — вектор оптимальных весов. 
Пример: 
• Рассмотрим задачу предсказания цены дома. У нас есть такие признаки, 
как площадь дома, место расположения, площадь участка и соседи. 
Модель линейной регрессии будет пытаться найти такие коэффициенты для 
этих признаков, чтобы минимизировать ошибку между предсказанными и 
реальными ценами. 
12.Как работает метод наименьших квадратов? 
Определение: 
• Метод наименьших квадратов (OLS) — это метод, используемый для 
нахождения оптимальных коэффициентов в линейной регрессии. Задача 
состоит в том, чтобы минимизировать сумму квадратов отклонений между 
предсказанными значениями модели и реальными значениями. 
Принцип работы: 
• В линейной регрессии модель пытается построить прямую, которая будет 
наиболее точно описывать зависимость между признаками и целевой 
переменной. 
• Чтобы измерить, насколько хорошо прямая описывает данные, 
вычисляется разница между реальными значениями и предсказанными 
значениями. Эта разница называется ошибкой: 
где L(w) — это функция потерь, которую мы минимизируем, а w — это 
коэффициенты модели. 
Связь с MSE: 
• MSE (Mean Squared Error) — это среднее значение суммы квадратов 
отклонений. В линейной регрессии MSE является функцией потерь, 
которую мы минимизируем: 
• В методе наименьших квадратов мы также минимизируем сумму 
квадратов ошибок — это просто общее название для подхода, который 
используется для поиска оптимальных параметров в линейной регрессии. 
Метод наименьших квадратов и максимизация правдоподобия: 
• OLS (метод наименьших квадратов) можно интерпретировать как частный 
случай метода максимального правдоподобия для линейной регрессии. 
• В контексте максимального правдоподобия предполагается, что ошибки 
(разница между предсказаниями и реальными значениями) подчиняются 
нормальному распределению с нулевым средним и постоянной 
дисперсией. 
• Таким образом, задача минимизации суммы квадратов ошибок 
эквивалентна задаче максимизации правдоподобия в случае 
нормального распределения ошибок. 
Пример: 
• Рассмотрим задачу, где нужно предсказать цену дома на основе его 
площади. Каждая точка на графике будет представлять пару значений: 
площадь дома (в качестве признака) и цена дома (в качестве целевой 
переменной). Метод наименьших квадратов найдет такую прямую, которая 
минимизирует сумму квадратов отклонений между предсказанными и 
реальными значениями. 
13.Что такое регуляризация L1 и L2? 
1. Базовый OLS (без регуляризации) 
Мы минимизируем сумму квадратов ошибок (MSE): 
2. L2-регуляризация (Ridge regression) 
Добавляем штраф за «большие» веса (сумму квадратов весов): 
•   λ≥0 — коэффициент регуляризации. 
•   Чем больше λ, тем сильнее веса «стягиваются» к нулю. 
•   Но никогда полностью не обнуляются (если только λ→∞). 
•   Решение аналитическое: 
3. L1-регуляризация (Lasso regression) 
Теперь штраф — это сумма модулей весов: 
•   В отличие от L2, здесь некоторые веса становятся ровно нулевыми. 
•   Это происходит потому, что функция штрафа ∣w∣ имеет «излом» в точке 
w=0. 
→ во время оптимизации проще «прыгнуть» в ноль, чем зависнуть около 
нуля. 
•   Таким образом, Lasso выполняет feature selection (автоматический 
отбор признаков). 
4. Разница в геометрии 
• Для Ridge уровень штрафа (constraint) — круг (сфера), поэтому веса плавно 
уменьшаются. 
• Для Lasso constraint — ромб (diamond shape), из-за углов оптимум часто 
оказывается ровно на оси → некоторые веса = 0. 
5. Плюсы и минусы 
Ridge (L2): 
•    Снижает дисперсию модели, хорошо работает при 
мультиколлинеарности. 
•    Решение аналитическое, стабильно. 
•   Не обнуляет веса → не делает отбор признаков. 
Lasso (L1): 
•    Может занулять веса → автоматический feature selection. 
•    Дает разреженные модели ( sparse ). 
•   При сильно скоррелированных признаках оставляет только один, 
другие зануляет → может потерять информацию. 
•   Решения находятся численно (методы оптимизации), нет простой 
аналитической формулы. 
Итого: 
• Если нужно стабильное сглаживание весов → Ridge. 
• Если нужно отобрать признаки и занулить лишнее → Lasso. 
• Часто используют ElasticNet — комбинацию L1 и L2. 
14.Что такое логистическая регрессия? 
Логистическая регрессия — это базовый метод бинарной классификации, 
который моделирует вероятность принадлежности объекта к классу 1. 
Эта функция — Binary Cross-Entropy (BCE), она же функция правдоподобия в 
статистической формулировке. 
Решение принимается по порогу (обычно 0.5): 
15.Как связана логистическая регрессия и сигмоидная функция? 
16.Как работает кросс-энтропийная функция потерь? 
Кросс-энтропийная функция потерь (Cross-Entropy Loss) — это стандартная 
функция потерь для задач классификации. 
Она сравнивает предсказанное моделью распределение вероятностей y^  (после 
softmax) с истинным распределением y (one-hot encoding для классов). 
Интуиция: 
• Если модель уверена в правильном классе (y^ true ≈1) → лосс близок к 0. 
• Если модель уверена в неверном классе (y^ true ≈0) → лосс стремится к ∞. 
17.Как интерпретировать коэффициенты регрессии? 
Коэффициенты (веса) в регрессии — это параметры модели, которые мы 
оптимизируем во время обучения, чтобы минимизировать функцию потерь. Они 
определяют, насколько сильно каждый признак влияет на итоговое 
предсказание. 
• В линейной регрессии коэффициент wi  показывает, насколько изменится 
предсказание y^ , если соответствующий признак xi  увеличится на 1, при 
условии, что остальные признаки остаются неизменными. 
o Пример: если w2 = 3, то при увеличении x2  на единицу прогноз 
увеличится на 3. 
• В логистической регрессии коэффициенты интерпретируются через 
логарифм шансов (log-odds). Если коэффициент wi  равен, например, 0.7, 
это значит: при увеличении признака xi  на 1, логарифм отношения 
вероятности класса «1» к классу «0» увеличится на 0.7. 
o В терминах вероятности: шанс события (odds) умножается на e0.7. 
o Таким образом, коэффициенты логистической регрессии 
определяют не прямое изменение вероятности, а изменение 
шансов, что делает модель более гибкой для классификации. 
18.Какие предположения делает линейная регрессия? 
Предположения линейной регрессии: 
1. Линейность. 
Зависимость между признаками X и целевой переменной y можно 
аппроксимировать линейной функцией: 
y=w1 x1 +w2 x2 +⋯+wn xn +b+ε  
2. Нормальность ошибок. 
Ошибки ε распределены нормально с математическим ожиданием 0: 
ε∼N(0,σ2)  
3. Гомоскедастичность. 
Дисперсия ошибок постоянна для всех значений признаков. 
Нарушение этого предположения (гетероскедастичность) ведёт к тому, что 
веса становятся неустойчивыми. 
4. Независимость ошибок. 
Ошибки не должны быть скоррелированы между собой (важно, например, 
при временных рядах). 
5. Отсутствие мультиколлинеарности. 
Признаки не должны быть сильно скоррелированы друг с другом, иначе 
веса становятся нестабильными (решение плохо определено). 
19.В каких случаях линейные модели работают плохо? 
Ситуации, когда линейные модели работают плохо: 
1. Нелинейные зависимости. 
Если зависимость между признаками X и целевой переменной y не 
аппроксимируется прямой линией, модель будет недообученной. 
a. Пример: «круг в круге» для классификации. 
2. Сильная мультиколлинеарность. 
Когда признаки сильно скоррелированы между собой, оценки 
коэффициентов становятся нестабильными. 
3. Шумные данные и выбросы. 
Линейные модели чувствительны к выбросам, особенно при 
использовании MSE. 
4. Высокая размерность без достаточного числа объектов. 
Если признаков много, а наблюдений мало, веса могут переобучиться или 
стать плохо определёнными. 
5. Сложные взаимодействия признаков. 
Линейная модель не учитывает сложные комбинации признаков без 
ручного добавления полиномиальных или взаимодействующих признаков. 
Итого: линейные модели просты и интерпретируемы, но плохо подходят для 
сложных, нелинейных и шумных данных. 
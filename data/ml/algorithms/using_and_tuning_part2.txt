45.Как бороться с переобучением? 
Переобучение (overfitting) — ситуация, когда модель хорошо запоминает 
обучающую выборку, но плохо обобщает на новых данных. В ML применяют 
несколько стратегий борьбы с этим: 
1. Ограничение сложности модели 
• Для деревьев решений: 
o max_depth — максимальная глубина дерева; 
o min_samples_split — минимальное число объектов для разбиения; 
o min_samples_leaf — минимальное число объектов в листе; 
o max_features — ограничение числа признаков для выбора сплита. 
• Для линейных моделей: 
o ограничение веса коэффициентов через регуляризацию (L1, L2). 
Подковырка: если модель слишком простая → недообучение. Поэтому важно 
найти баланс (bias-variance tradeoff). 
2. Регуляризация 
• L1 (Lasso) — обнуляет лишние коэффициенты, оставляя только значимые 
признаки. 
• L2 (Ridge) — «сжимает» веса, но не зануляет, снижает чувствительность к 
шуму. 
• Elastic Net — комбинация L1 и L2. 
В деревьях классической регуляризации почти нет, поэтому её роль 
выполняют гиперпараметры (см. п.1). 
3. Работа с данными 
• Увеличение датасета (сбор новых данных). 
• Очистка шумных объектов. 
• Feature engineering — создание более информативных признаков. 
Чем больше данных, тем сложнее модели «запомнить» все, и она вынуждена 
учиться обобщать. 
4. Методы обучения 
• Кросс-валидация — помогает правильно оценить модель и подобрать 
гиперпараметры. 
• Ранняя остановка — в бустинге можно остановить обучение до того, как 
модель начнёт «запоминать» шум. 
• Ансамблирование (Random Forest, Bagging, Boosting) — несколько слабых 
моделей вместе устойчивее, чем одна. 
5. Обработка признаков 
• Удаление сильно коррелированных признаков. 
• Нормализация/стандартизация (для линейных моделей и KNN). 
Итог: 
Методы борьбы с переобучением можно разделить на: 
• управление сложностью модели; 
• регуляризацию параметров; 
• увеличение и очистку данных; 
• корректное обучение и валидацию. 
46.Что такое регуляризация и почему она важна? 
�
� Почему регуляризация важна? 
• Предотвращает переобучение — модель не подгоняется слишком точно 
под тренировочные данные. 
• Снижает дисперсию предсказаний (модель становится устойчивее). 
• Повышает обобщающую способность. 
• Упрощает модель (особенно L1 → делает её более интерпретируемой). 
Подковырка: 
• Слишком сильная регуляризация (λ\lambdaλ большое) → недообучение. 
• Слишком слабая регуляризация (λ→0\lambda \to 0λ→0) → переобучение. 
47.Как интерпретировать важность признаков (feature importance)? 
• Feature importance = количественная оценка того, насколько данный 
признак влияет на работу обученной модели. 
• В разных моделях это вычисляется по-разному: 
o Линейные модели (линейная регрессия, логистическая регрессия): 
важность признака = вес коэффициента (после нормализации 
признаков). 
o Деревья решений / Random Forest / Gradient Boosting: важность 
признака = суммарное уменьшение неопределённости (например, 
уменьшение Gini impurity или MSE) при использовании признака в 
разбиениях. 
o Permutation importance: перемешиваем значения одного признака 
и смотрим, насколько ухудшилось качество модели → чем сильнее 
падение, тем важнее признак. 
o SHAP / LIME: методы интерпретации сложных моделей, которые 
оценивают вклад признаков в конкретные предсказания. 
�
� Почему это важно 
• Помогает понять, какие переменные реально влияют на таргет, а какие — 
«шум». 
• Может выявить лишние признаки → уменьшить размерность и ускорить 
обучение. 
• Дает интерпретируемость: можно объяснить заказчику, почему модель 
приняла то или иное решение. 
48.Что такое PCA и зачем он нужен? 
PCA (Principal Component Analysis) — метод линейного сокращения размерности 
и извлечения главных направлений вариации в данных. 
Главная цель PCA — найти новые ортогональные координаты (главные 
компоненты), в которых данные упорядочены по убыванию дисперсии. Это даёт 
несколько полезных эффектов: 
• уменьшение размерности при минимально возможной потере дисперсии 
(информации), 
• удаление линейной мультиколлинеарности, 
• упрощение визуализации (2D/3D проекции), 
• шумоподавление (оставить только компоненты с большой дисперсией), 
• ускорение дальнейшего обучения и уменьшение объёма данных. 
Интуиция (коротко) 
Представь облако точек в высоко-мерном пространстве. PCA находит 
направление, вдоль которого данные «растянуты» сильнее всего (максимальная 
дисперсия) — это первая главная компонента. Затем ищет следующее 
направление, перпендикулярное первому, где дисперсия максимальна, и т.д. 
Если в первых 2–3 компонентах сосредоточено большая часть дисперсии, можно 
заменить исходные признаки на эти компоненты. 
Практические моменты / подковырки (что важно знать) 
1. Нужно ли стандартизовать? 
Да, почти всегда нужно стандартизировать (z-score), если признаки имеют 
разные масштабы. Иначе признаки с большим разбросом «перетянут» 
компоненты. 
2. PCA — линейный метод. 
Если данные лежат на сильно нелинейной «маннфолд» (например, 
спираль), PCA плохо подойдёт. Для нелинейной проекции есть Kernel PCA, 
t-SNE, UMAP. 
3. Интерпретируемость. 
Компоненты — линейные комбинации исходных признаков (загрузки, 
loadings). Можно смотреть на веса (коэффициенты в векторе vj ) чтобы 
понять вклад признаков. Но сами компоненты часто труднее 
интерпретировать, чем оригинальные признаки. 
4. Выбор числа компонент (k). 
a. Правило: выбрать k так, чтобы кумулятивная объяснённая дисперсия 
≥ 0.8–0.95 (в зависимости от задачи). 
b. Альтернатива: смотреть scree plot (кривая собственных чисел) и 
искать «локоть». 
5. PCA и шум: 
Часто компоненты с малыми собственными числами — это шум; 
отбрасывая их, вы уменьшаете шум и улучшаете устойчивость модели. 
6. Whitening: 
Доп. трансформация, делает компоненты независимыми и с единичной 
дисперсией (полезно для некоторых алгоритмов), но разрушает 
масштаб/интерпретацию. 
7. PCA для предобработки vs фича-инжиниринга: 
a. PCA полезен перед кластеризацией, скорингом, визуализацией. 
b. Но для интерпретируемости моделей (например, регрессии, где 
нужно объяснить влияние признаков) лучше осторожно 
использовать PCA или вообще избегать. 
8. Вычислительная сторона: 
SVD сложен O(min(n,p)·n·p). Для больших данных используют 
IncrementalPCA, randomized SVD или пакеты с оптимизациями. 
9. Неподходящие типы данных: 
a. PCA требует числовых признаков. Категориальные признаки нужно 
кодировать аккуратно (one-hot может увеличить размерность), но 
PCA на one-hot обычно бесполезен. 
b. Пропуски нужно обработать заранее. 
Применения PCA (где его используют) 
• сжатие данных (компрессия), 
• визуализация высоко-мерных данных (2D/3D), 
• удаление коллинеарности перед линейными моделями, 
• ускорение обучения моделей (меньше признаков), 
• шумоподавление (удаление компонент с малой дисперсией), 
• предобработка перед кластеризацией. 
Когда не стоит использовать PCA / ловушки 
• если тебе важна интерпретируемость оригинальных признаков, PCA 
скрывает прямую связь (линейные комбинации сложнее объяснить), 
• при сильной нелинейной структуре данных — PCA даст плохую проекцию, 
• при сильном дисбалансе масштабов — забывать стандартизацию нельзя, 
• если в данных много категориальных признаков — PCA не подходит 
напрямую. 
Краткие рекомендации (чтобы запомнить) 
• Всегда думай: «Зачем я применяю PCA?» — 
ускорение/шум/визуализация/декорреляция? 
• Стандартизируй данные перед PCA (если признаки в разных единицах). 
• Смотри на explained_variance_ratio_ и scree plot для выбора k. 
• Используй IncrementalPCA или randomized_svd для больших датасетов. 
• Если нужна интерпретируемость — сначала подумай, не лучше ли отобрать 
признаки вручную (feature selection), а не применять PCA. 
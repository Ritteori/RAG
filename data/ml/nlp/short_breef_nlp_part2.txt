65. Что такое TF-IDF? Как он работает и зачем нужен? 
TF-IDF (Term Frequency-Inverse Document Frequency) — это статистическая мера, 
которая показывает важность слова в документе относительно всей коллекции 
документов. 
Простая аналогия: 
Если бы слова были людьми на вечеринке: 
• Часто упоминаемый человек (TF) = популярный 
• Уникальный человек (IDF) = тот, кого знают только здесь 
• TF-IDF = самый запоминающийся гость (и популярный, и уникальный) 
Как работает TF-IDF 
1. TF (Term Frequency) — Частота слова 
"Насколько часто слово встречается в ЭТОМ документе?" 
# Формула: TF = (число раз слово встретилось в документе) / (всего слов в документе) 
# Пример: 
документ = "кот любит рыбу кот счастлив" 
TF("кот") = 2/5 = 0.4 
TF("любит") = 1/5 = 0.2 
2. IDF (Inverse Document Frequency) — Обратная частота документа 
"Насколько слово РЕДКОЕ во всей коллекции документов?" 
# Формула: IDF = log( (всего документов) / (число документов со словом) ) 
# Пример (3 документа): 
док1: "кот любит рыбу" 
док2: "кот смотрит окно"  
док3: "собака бежит" 
IDF("кот") = log(3/2) ≈ 0.18   # Есть в 2 из 3 документов 
IDF("рыбу") = log(3/1) ≈ 0.48  # Есть только в 1 документе → более уникальное! 
3. TF-IDF = TF × IDF 
Комбинируем: частота слова × его уникальность 
# Для документа 1: 
TF-IDF("кот") = 0.4 × 0.18 ≈ 0.072 
TF-IDF("рыбу") = 0.2 × 0.48 ≈ 0.096  # Выше вес, хотя реже встречается! 
Практический пример 
Документы: 
• Док1: "кот любит рыбу" 
• Док2: "кот смотрит окно" 
• Док3: "собака бежит" 
TF-IDF матрица: 
кот   любит  рыбу  смотрит окно  собака  бежит 
Док1:  0.072  0.48   0.48    0       0      0      0 
Док2:  0.072  0      0       0.48    0.48   0      0   
Док3:  0      0      0       0       0      0.48   0.48 
Что видим: 
• Слово "кот" имеет низкий TF-IDF — оно частое, но не уникальное 
• Слова "рыбу", "смотрит", "собака" имеют высокий TF-IDF — они редкие и 
важные 
Зачем нужен TF-IDF? 
1. Улучшение Bag of Words: 
• BoW: Все слова одинаково важны 
• TF-IDF: Редкие слова получают больший вес 
2. Поисковые системы: 
# Запрос: "любимая рыба" 
# Документы ранжируются по TF-IDF слов "любимая" и "рыба" 
3. Классификация текстов: 
• Спам-фильтры (слова "акция", "бесплатно" имеют высокий TF-IDF в спаме) 
• Тематическое моделирование 
4. Рекомендательные системы: 
• Поиск похожих документов по TF-IDF векторам 
Преимущества перед BoW 
# BoW проблема: 
док1: "алгоритм машинного обучения" = [1, 1, 1]   
док2: "курс машинного обучения" = [1, 1, 1]  # Тот же вектор! 
# TF-IDF решение: 
док1: "алгоритм"(высокий), "машинного"(средний), "обучения"(средний) 
док2: "курс"(высокий), "машинного"(средний), "обучения"(средний)  # Разные 
векторы! 
Ограничения TF-IDF 
• Не учитывает семантику: "кот" и "кошка" = разные слова 
• Не учитывает порядок слов: "план выполнен" vs "выполнен план" 
• Плохо работает с очень короткими текстами 
Современные альтернативы: 
• Word2Vec, BERT — учитывают смысл слов 
• BM25 — улучшенная версия для поиска 
Вывод: TF-IDF — это "умный" Bag of Words, который понимает, что редкие слова 
важнее частых. Отличный инструмент для начала работы с текстами! 
66. Как с помощью нейросетей представляют слова (Word Embeddings)? 
(Тут можно про Word2Vec). 
Word Embeddings — это плотные векторные представления слов, где семантически 
близкие слова имеют близкие векторы в многомерном пространстве. 
Как работает Word2Vec: 
1. Архитектуры: 
Skip-gram:  
• Вход: Одно слово (one-hot vector)  
• Выход: Вероятности слов в его контексте  
• Пример: "кот" → ["сидит", "на", "ковре"] 
CBOW (Continuous Bag of Words):  
• Вход: Несколько слов контекста  
• Выход: Вероятность центрального слова  
• Пример: ["сидит", "на", "ковре"] → "кот" 
2. Процесс обучения: 
# Пример Skip-gram: 
input: [0,0,1,0,...,0]  # one-hot "кот" 
hidden_layer = embedding_matrix × input  # получаем embedding (300-500 измерений) 
output = softmax(hidden_layer × context_matrix)  # предсказываем контекстные слова 
# После обучения используем embedding_matrix как представления слов 
3. Ключевые особенности: 
• Размерность: Обычно 100-500 измерений (не 512-768 как в BERT) 
• Окно контекста: Обычно 2-10 слов в обе стороны 
• Оптимизация: Negative Sampling ускоряет обучение 
4. Результат — семантические отношения: 
# Известный пример: 
vector("король") - vector("мужчина") + vector("женщина") ≈ vector("королева") 
vector("Париж") - vector("Франция") + vector("Италия") ≈ vector("Рим") 
5. Преимущества перед one-hot: 
• Семантика: Похожие слова имеют близкие векторы 
• Размерность: 300-500 vs 50,000+ в one-hot 
• Обобщение: Может работать с новыми словами 
Ограничения: 
• Одно представление для каждого слова (нет учета контекста) 
• Не понимает морфологию 
• Заменен современными контекстными embedding'ами (BERT, ELMo) 
Современное развитие: 
Word2Vec был прорывом, но сегодня используются трансформеры, которые создают 
контекстные эмбеддинги — разные векторы для одного слова в разных контекстах. 
67. Что такое модель языкового моделирования (Language Model)? 
Приведите пример (GPT). 
Языковая модель (Language Model) — это модель, которая обучается предсказывать 
вероятность следующего токена в последовательности на основе предыдущих токенов. 
Формально, для последовательности слов [w₁, w₂, ..., wₙ] LM вычисляет: 
p(w₁, w₂, ..., wₙ) = Π p(wᵢ | w₁, w₂, ..., wᵢ₋₁) 
Ключевые типы языковых моделей: 
1. N-gram модели: 
• Используют только предыдущие N-1 слов 
• Статистические, основанные на частотах 
• Быстрые, но ограниченный контекст 
2. RNN/LSTM модели: 
• Могут запоминать длинные зависимости 
• Обрабатывают последовательность пошагово 
• Проблема исчезающего градиента 
3. Transformer модели (GPT): 
GPT (Generative Pre-trained Transformer) 
Архитектура: 
• Только декодер трансформера (без энкодера) 
• Маскированное самовнимание - видит только предыдущие токены 
• Positional encoding - учитывает порядок слов 
Как работает генерация: 
# Авторегрессивная генерация 
input = "Сегодня хорошая" 
for i in range(max_length): 
next_token = model.predict(input)  # предсказывает следующее слово 
input += next_token  # добавляет к контексту 
# Результат: "Сегодня хорошая погода для прогулки" 
Ключевые особенности GPT: 
• Предобучение: Обучается на огромных корпусах текста (BooksCorpus, 
Wikipedia) 
• Трансферное обучение: Дообучение на конкретных задачах 
• Контекстное понимание: Учитывает весь предыдущий контекст 
Пример работы: 
Вход: "Кошка сидит на" 
GPT: ["столе" (p=0.7), "полу" (p=0.2), "крыше" (p=0.1)] 
Эволюция GPT: 
• GPT-1: 117M параметров 
• GPT-2: 1.5B параметров, лучшая генерация 
• GPT-3: 175B параметров, few-shot learning 
• GPT-4: Мультимодальность, улучшенное reasoning 
Применение языковых моделей: 
• Генерация текста 
• Автодополнение 
• Машинный перевод 
• Классификация текста 
• Вопрос-ответ 
Важно: Современные LM типа GPT — это основа большинства NLP-приложений, 
демонстрирующие способность к обобщению и решению множества задач.
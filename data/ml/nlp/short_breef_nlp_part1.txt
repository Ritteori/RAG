Раздел 5: Natural Language Processing (NLP) 
62. Что такое токенизация (Tokenization)? Какие виды бывают? 
Токенизация — процесс разбиения текста на минимальные значимые единицы 
(токены), которые могут быть словами, подсловами или символами. 
Основные виды токенизации: 
1. Word-based (словесная): 
# Пример: "I don't like NLP" → ["I", "don't", "like", "NLP"] 
• Плюсы: Простая интерпретация 
• Минусы: Большой размер словаря, проблемы с OOV (out-of-vocabulary) 
2. Character-based (побуквенная): 
# Пример: "cat" → ["c", "a", "t"] 
• Плюсы: Маленький словарь, нет OOV 
• Минусы: Длинные последовательности, теряется семантика 
3. Subword-based (подсловная): 
# Пример: "unhappily" → ["un", "happ", "ily"] 
Популярные алгоритмы подсловной токенизации: 
Byte Pair Encoding (BPE): 
• Начинается с символов, итеративно объединяет самые частые пары 
• Используется в GPT, RoBERTa# Пример обучения: 
# Исходно: l o w e r, l o w e s t, n e w e r, w i d e r 
# После обучения: low, lower, newest, wider 
WordPiece: 
• Похож на BPE, но объединяет пары с максимальной вероятностью 
• Используется в BERT# "playing" → ["play", "##ing"] 
SentencePiece: 
• Токенизация без предварительной сегментации 
• Работает непосредственно с raw text 
Сравнение современных подходов: 
Модель 
Метод токенизации 
BERT 
WordPiece 
Размер словаря 
30,000 
GPT-2/3 
BPE 
50,000+ 
T5 
SentencePiece 
32,000 
Ключевые преимущества подсловной токенизации: 
• Эффективная работа с OOV словами 
• Баланс между семантикой и размером словаря 
• Возможность обрабатывать редкие и составные слова 
Пример обработки OOV: 
# Слово "ChatGPT" которого нет в словаре: 
Word-based: [UNK]  # Провал 
Subword-based: ["Chat", "G", "PT"]  # Успех 
Важно: Выбор токенизатора напрямую влияет на качество модели и должно 
соответствовать задаче и языку. 
63. Что такое стемминг (Stemming) и лемматизация (Lemmatization)? В 
чем разница? 
Оба метода используются для приведения слов к их базовой форме, чтобы 
уменьшить размер словаря и улучшить обработку текста. 
Пример проблемы: 
"бежать", "бегу", "бежишь", "бежал", "бегаю"  
→ это все формы одного слова "бежать" 
1. Стемминг (Stemming) 
Что это: Грубый, эвристический метод "отрезания" окончаний и суффиксов. 
Как работает: Использует простые правила для удаления аффиксов 
# Примеры стемминга (алгоритм Porter Stemmer): 
"running" → "run" 
"happily" → "happi"  # Не идеально! 
"better" → "better"  # Не меняется 
Популярные алгоритмы: 
• Porter Stemmer (для английского) 
• Snowball Stemmer (многозазычный) 
Плюсы: Быстрый, простой в реализации Минусы: Может создавать несуществующие 
слова ("happi") 
2. Лемматизация (Lemmatization) 
Что это: Более сложный метод, который приводит слова к их словарной форме 
(лемме) с учетом контекста и морфологического анализа. 
Как работает: Использует словари и знания о языке 
# Примеры лемматизации: 
"running" → "run" 
"happily" → "happy"  # Корректно! 
"better" → "good"    # Учитывает сравнительную степень 
"is" → "be"
          # Возвращает инфинитив 
Требует: 
• Морфологического словаря 
• Часто тегов части речи (POS tagging) 
Плюсы: Точный, дает реальные слова Минусы: Медленнее, сложнее в реализации 
Сравнение на примере 
Слово 
"running" 
Стемминг 
"run" 
Лемматизация 
"run" 
"happily" 
"happili" 
"happy" 
"better" 
"better" 
"good" 
"mice" 
"mic" 
"mouse" 
"is" 
"is" 
"be" 
# Практический пример: 
text = "The cats are running happily" 
# Стемминг: 
["the", "cat", "ar", "run", "happili"] 
# Лемматизация:   
["the", "cat", "be", "run", "happy"] 
Когда что использовать? 
Стемминг: 
• Когда нужна скорость 
• Для поисковых систем (поиск по ключевым словам) 
• Предварительная обработка больших объемов текста 
Лемматизация: 
• Когда важна точность (анализ тональности, чат-боты) 
• Для лингвистического анализа 
• Когда нужны реальные слова на выходе 
В современных NLP: Оба метода стали менее популярны с приходом word 
embeddings (Word2Vec, BERT), которые сами умеют работать с разными формами 
слов. 
Вывод: Стемминг — быстрый и грубый, лемматизация — медленный и точный метод 
приведения слов к базовой форме. 
64. Что такое мешок слов (Bag of Words / BoW)? Какие у него 
недостатки? 
Мешок слов — это простейшая модель представления текста в виде числового 
вектора, где порядок слов полностью игнорируется, а учитывается только частота 
их появления. 
Основная идея: Текст = неупорядоченный "мешок" слов 
Как работает BoW: шаг за шагом 
1. Создание словаря: 
# Исходные тексты: 
doc1 = "кот сидит на ковре" 
doc2 = "кот смотрит на окно" 
# Словарь (все уникальные слова): 
vocab = ["кот", "сидит", "на", "ковре", "смотрит", "окно"] 
2. Векторизация текстов: 
# Преобразование в векторы (частота слов): 
doc1_vector = [1, 1, 1, 1, 0, 0]  # кот(1), сидит(1), на(1), ковре(1), смотрит(0), окно(0) 
doc2_vector = [1, 0, 1, 0, 1, 1]  # кот(1), сидит(0), на(1), ковре(0), смотрит(1), окно(1) 
3. Матричное представление: 
кот сидит на ковре смотрит окно 
doc1:   1     1   1    1      0    0 
doc2:   1     0   1    0      1    1 
Разновидности BoW 
1. Binary BoW: Только факт наличия слова (0 или 1) 
doc1_binary = [1, 1, 1, 1, 0, 0]  # Та же схема, но без учета частоты 
2. Count-based BoW: Учет частоты слов 
doc3 = "кот кот сидит" 
doc3_vector = [2, 1, 0, 0, 0, 0]  # "кот" встречается 2 раза 
3. N-grams BoW: Учет последовательностей слов 
# Биграммы (2 слова подряд): 
doc1_bigrams = ["кот сидит", "сидит на", "на ковре"] 
Недостатки Bag of Words 
1. Потеря семантики и порядка 
# Разные предложения → одинаковые векторы: 
text1 = "пользователь любит систему" = [1, 1, 1] 
text2 = "система любит пользователя" = [1, 1, 1]  # Тот же вектор! 
2. Проблема разреженности (Sparsity) 
# Для словаря из 10,000 слов каждый документ → вектор из 10,000 элементов 
# Но большинство элементов = 0 
vector = [0, 0, 1, 0, 0, 0, ..., 1, 0, 0]  # 99.9% нулей 
3. Не учитывает важность слов 
# Слова "the" и "algorithm" имеют одинаковый вес: 
document = "the quick brown fox"  # "the" = 1, "algorithm" = 1 
4. Размер словаря экспоненциально растет 
• 1,000 документов → ~10,000 уникальных слов 
• 1,000,000 документов → ~500,000 уникальных слов 
5. Чувствительность к опечаткам и разным формам 
# Разные векторы для одного слова: 
"running", "runs", "ran" → [1, 0, 0], [0, 1, 0], [0, 0, 1] 
Практическое применение 
Где еще используется: 
• Naive Bayes классификатор (спам-фильтры) 
• Латентный семантический анализ (LSA) 
• Быстрое прототипирование NLP-моделей 
Современная альтернатива: 
# Word Embeddings решают основные проблемы BoW: - Учитывают семантику (похожие слова → похожие векторы) - Фиксированная размерность (например, 300 вместо 50,000) - Учитывают контекст 
Пример сравнения: 
# BoW: 
"король" = [0, 0, 1, 0, ..., 0] 
"монарх" = [0, 1, 0, 0, ..., 0]  # Совсем разные векторы 
# Word Embeddings: 
"король" ≈ [0.2, 0.8, -0.1, ..., 0.4] 
"монарх" ≈ [0.3, 0.7, -0.2, ..., 0.3]  # Близкие векторы 
Вывод: Несмотря на недостатки, BoW остается полезным для: 
• Базовых классификационных задач 
• Быстрого прототипирования 
• Обучения простых моделей на маленьких данных 
Раздел 4: Computer Vision (CV) 
57. Что такое аугментация данных (Data Augmentation)? Зачем она 
нужна? Приведи примеры для изображений. 
Data Augmentation — техника искусственного расширения тренировочного набора 
данных путём применения случайных преобразований к исходным изображениям. 
Основные цели: 
1. Борьба с переобучением  
a. Служит мощным методом регуляризации 
b. Помогает модели обобщаться на незнакомых данных 
2. Повышение устойчивости модели  
a. Учит модель инвариантности к: освещению, поворотам, масштабу 
b. Делает модель робастной к реальным условиям (шум, частичные 
occlusion) 
3. Решение проблемы дисбаланса классов  
a. Увеличение представительства миноритарных классов 
b. Генерация дополнительных примеров для редких случаев 
Популярные методы для изображений: 
# Базовые преобразования 
RandomHorizontalFlip(p=0.5) 
RandomRotation(degrees=15) 
RandomResizedCrop(size=224) 
ColorJitter(brightness=0.2, contrast=0.2) 
# Продвинутые техники 
CutMix()  # Комбинация двух изображений 
MixUp()   # Интерполяция между изображениями 
RandomErasing()  # Случайное закрытие областей 
Пример эффективности: 
Модель, обученная с аугментацией, корректно распознаёт кошку даже если она: 
• Повёрнута на 30 градусов 
• Частично закрыта предметом 
• Снята при плохом освещении 
• Обрезана по краю кадра 
Важно: Аугментация применяется только к тренировочным данным. Валидация и тест 
используют оригинальные изображения. 
58. Как работает transfer learning (transfer learning)? Почему это 
эффективно? 
Transfer Learning — техника использования знаний, полученных на одной задаче, для 
улучшения обобщения на другой задаче. 
Как работает на практике: 
1. Берём предобученную модель (обычно на ImageNet) 
a. Примеры: ResNet, VGG, EfficientNet 
b. Модель уже научилась базовым фичам: границы, текстуры, простые 
формы 
2. Адаптируем под новую задачу: 
# Стратегия 1: Feature Extraction 
model = resnet50(pretrained=True) 
for param in model.parameters():  # Замораживаем все веса 
param.requires_grad = False 
model.fc = nn.Linear(2048, num_our_classes)  # Меняем только классификатор 
# Стратегия 2: Fine-Tuning 
for param in model.parameters():  # Сначала замораживаем 
param.requires_grad = False 
model.fc = nn.Linear(2048, num_our_classes)  # Меняем классификатор 
# Позже размораживаем несколько последних слоёв для дообучения 
Почему это эффективно: 
1. Экономия данных: Работает даже с малыми датасетами (100-1000 
изображений) 
2. Экономия времени: Обучение занимает часы вместо дней/недель 
3. Лучшее качество: Предобученные фичи уже оптимизированы для визуальных 
задач 
Типичный пайплайн: 
1. Замораживаем все слои, обучаем только новый классификатор 
2. Размораживаем несколько последних сверточных блоков 
3. Обучаем с очень маленьким learning rate (обычно 1/10 от исходного) 
Пример эффективности: 
• Без transfer learning: 70% accuracy при 10к изображений, 2 дня обучения 
• С transfer learning: 90% accuracy при 500 изображениях, 2 часа обучения 
Ограничения: 
• Работает лучше, когда исходная и целевая задачи схожи 
• Может потребовать адаптации при сильном различии доменов (например, 
медицинские снимки vs. обычные фото) 
Важно: Transfer learning стал стандартом де-факто в компьютерном зрении и NLP 
благодаря своей эффективности. 
59. Что такое семантическая сегментация (Semantic Segmentation)? 
Какой архитектурой ее решают? (U-Net, FCN). 
Семантическая сегментация — задача классификации каждого пикселя изображения 
по заданному набору классов. В отличие от instance segmentation, не различает 
отдельные объекты одного класса (все машины будут одного цвета). 
Ключевые архитектуры: 
1. U-Net: 
a. U-образная структура: Энкодер (сжатие) → Ботлнек → Декодер 
(расширение) 
b. Skip-connections: Конкатенация карт признаков энкодера с 
соответствующими слоями декодера 
c. Преимущества: Сохраняет детали, хорошо работает с малым 
количеством данных 
2. FCN (Fully Convolutional Network): 
a. Замена полносвязных слоев на сверточные (1x1) 
b. Транспонированные свертки для увеличения разрешения 
c. Skip-connections между разными уровнями детализации 
3. DeepLab: 
a. Atrous convolution (dilated) — увеличение рецептивного поля без потери 
разрешения 
b. ASPP — параллельная обработка в разных масштабах 
c. CRF — постобработка для уточнения границ 
Технические детали U-Net: 
# Типичная структура 
encoder1 → pool → encoder2 → pool → encoder3 → pool → encoder4 → pool → 
bottleneck 
↑ 
decoder1 ← upsample ← decoder2 ← upsample ← decoder3 ← upsample ← decoder4 
# Skip-connection: concat(encoder_output, decoder_input) 
Пример вывода: 
• Вход: изображение 512×512×3 
• Выход: маска 512×512×N (где N — число классов) 
• Каждый пиксель имеет цвет соответствующего класса 
Области применения: 
• Медицинская визуализация (сегментация органов) 
• Автономное вождение (дорога, пешеходы, знаки) 
• Спутниковые снимки (типы земного покрова) 
Важно: Качество оценивается по IoU (Intersection over Union) и pixel accuracy. 
60. Что такое детекция объектов (Object Detection)? В чем разница 
между двухстадийными (R-CNN, Fast R-CNN) и одностадийными 
(YOLO, SSD) детекторами? 
Детекция объектов — задача одновременного обнаружения и классификации 
объектов на изображении, включающая: 
• Определение ограничивающих рамок (bounding boxes) 
• Классификацию объектов внутри этих рамок 
Двухстадийные детекторы (R-CNN, Fast R-CNN, Faster R-CNN) 
Структура и принцип работы: 
1. Стадия 1: Генерация регионов (Region Proposal) 
# Пример: RPN (Region Proposal Network) в Faster R-CNN 
input_image → Backbone(CNN) → Feature_map → RPN → 2000 регионов-кандидатов 
a. R-CNN: Использует Selective Search (~2000 регионов) 
b. Fast R-CNN: Улучшенная обработка регионов 
c. Faster R-CNN: RPN генерирует регионы напрямую из feature map 
2. Стадия 2: Классификация и регрессия рамок 
# Для каждого региона: 
RoI_Pooling → FC_layers →  
├── Classification_head (softmax) 
└── Bbox_regression_head (уточнение координат) 
Преимущества: 
• Высокая точность обнаружения 
• Хорошо работают с маленькими объектами 
• Меньше ложных срабатываний 
Недостатки: 
• Медленная скорость (2-5 FPS в ранних версиях) 
• Сложная архитектура 
• Большой объем вычислений 
Одностадийные детекторы (YOLO, SSD) 
Структура и принцип работы: 
1. Единая сеть "за один проход" 
# YOLO v3 архитектура 
input_image → Darknet-53_backbone →  
├── Head_1 (13×13×255)  # Крупные объекты 
├── Head_2 (26×26×255)  # Средние объекты   
└── Head_3 (52×52×255)  # Мелкие объекты 
2. Прямое предсказание: 
a. YOLO: Делит изображение на сетку, каждая ячейка предсказывает 
несколько bbox + классы 
b. SSD: Использует anchor boxes на разных feature maps 
Ключевые особенности YOLO: 
# Формат предсказания YOLO 
prediction = [x, y, w, h, confidence, class₁, class₂, ...] 
# где: 
# x,y - центр bbox относительно ячейки 
# w,h - ширина/высота относительно всего изображения 
# confidence - уверенность в наличии объекта 
Преимущества: 
• Высокая скорость (45-150 FPS) 
• Проще архитектура 
• Хорошее качество в реальном времени 
Недостатки: 
• Труднее обнаруживать маленькие объекты 
• Больше ложных срабатываний 
• Меньшая точность чем у двухстадийных 
Сравнительная таблица 
Параметр 
Двухстадийные 
Скорость 
Медленные (5-15 FPS) 
Одностадийные 
Быстрые (45-150 FPS) 
Точность 
Высокая (80%+ mAP) 
Средняя-высокая (70-80% mAP) 
Память 
Большой расход 
Эффективные 
Обучение 
Сложное 
Относительно простое 
Использование 
Точные системы 
Real-time приложения 
Эволюция: 
• 2014: R-CNN (2-stage) 
• 2015: Fast R-CNN, YOLO v1 (1-stage)  
• 2016: Faster R-CNN, SSD 
• 2018: YOLO v3, RetinaNet 
• 2020: YOLO v4/v5, EfficientDet 
Современный выбор: 
• Для точности: Faster R-CNN, Cascade R-CNN 
• Для скорости: YOLO v5/v7, SSD 
• Баланс: RetinaNet, EfficientDet 
Метрики качества: mAP (mean Average Precision), IoU (Intersection over Union), FPS 
(Frames Per Second) 
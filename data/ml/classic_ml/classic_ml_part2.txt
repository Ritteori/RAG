8. Что такое кросс-валидация (Cross-Validation)? Зачем она нужна? 
Объясните k-fold CV. 
Зачем нужна: 
1. Чтобы получить независимую и устойчивую оценку обобщающей 
способности (generalization) модели. 
2. Чтобы более эффективно и надежно подбирать гиперпараметры модели, не 
полагаясь на одно случайное разбиение. 
3. Максимально эффективно использовать ограниченные данные для обучения и 
оценки. 
Как работает k-Fold Cross-Validation: 
1. Исходные данные случайно shuffling и разбиваются на k одинаковых по 
размеру фолдов (folds). 
2. Модель обучается k раз. Каждый раз в качестве тренировочной выборки 
используются k-1 фолдов, а один оставшийся фолд — в качестве тестовой. 
3. На каждом прогоне вычисляется выбранная метрика качества. 
4. Итоговой оценкой модели является среднее значение всех k метрик. 
Стандартное отклонение показывает устойчивость модели. 
# Пример: 5-Fold CV дает 5 значений accuracy 
[0.82, 0.85, 0.80, 0.83, 0.81] 
Final Score = 0.822 ± 0.017 
Основные типы: 
• Stratified K-Fold: Сохраняет процентное соотношение классов каждого фолда 
таким же, как и в исходном наборе. Критично для несбалансированных данных. 
• Leave-One-Out (LOO): k = n (число объектов в датасете). Каждый объект по 
очереди становится тестовой выборкой. Чрезвычайно затратен, но дает почти 
несмещенную оценку. 
• Time Series Split: Используется для временных рядов, где важно не 
предсказывать "прошлое по будущему". 
Линейные модели:  
9. Как работает линейная регрессия? Как находится оптимальные веса?  
Как работает: Модель предполагает линейную зависимость целевой переменной y от 
признаков X: y_pred = w_1*x_1 + w_2*x_2 + ... + w_n*x_n + b или в матричной форме: 
y_pred = X * w + b 
Цель обучения: Найти такие веса w и смещение b, которые минимизируют функцию 
потерь. 
• MSE (Mean Squared Error) — самая распространенная: MSE = (1/n) * Σ(y_i - 
y_pred_i)² 
Способы нахождения оптимальных весов: 
1. Градиентный спуск (Gradient Descent) — итеративный метод: 
a. Вычисляем градиент функции потерь по каждому весу (производная, 
показывающая направление наискорейшего роста). 
b. Обновляем веса в направлении, обратном градиенту: w = w - α * ∇L(w), 
где α — скорость обучения. 
c. Процесс повторяется до схождения. 
2. Нормальное уравнение (Analytical Solution) — точное решение: 
a. w = (X^T * X)^(-1) * X^T * y 
b. Плюсы: Точное, не нужно подбирать α. 
c. Минусы: Вычислительно дорогое для большого числа признаков (>10k), 
требует невырожденной матрицы (X^T * X). 
10. Что такое градиентный спуск (Gradient Descent)? Объясни 
интуитивно.  
Интуитивное объяснение: Представьте, что вы стоите на склоне холма в густом 
тумане и хотите спуститься вниз. Вы не видите всю дорогу, но можете почувствовать, 
под какими ногами земля уходит вниз сильнее всего. Делая небольшие шаги в этом 
направлении, вы постепенно спуститесь в долину (минимум функции потерь). 
Технически: 
1. Градиент — это вектор, состоящий из частных производных функции потерь 
по каждому параметру модели. Он показывает направление наискорейшего 
роста функции. 
2. Антиградиент — движение в направлении, обратном градиенту, что ведет к 
наискорейшему спуску. 
3. Learning Rate (α) — "размер шага". Слишком большой α — перешагнем 
минимум, слишком маленький — обучение будет очень медленным. 
4. Процесс: На каждой итерации вычисляются градиенты через backpropagation, и 
веса обновляются по формуле: w = w - α * ∇L(w) 
Проблемы: 
• Локальные минимумы — можно "застрять" в локальной впадине, а не найти 
глобальный минимум. 
• Седловые точки — участки, где градиент близок к нулю, но это не минимум. 
11.  Какие виды градиентного спуска ты знаешь? (Batch, Mini-batch, 
Stochastic).  
Критерий 
Batch 
(Пакетный) 
Stochastic 
Mini-batch (Мини
пакетный) 
Размер батча 
Весь датасет 
(n) 
(Стохастический) 
k (32, 64, 128...) 
Скорость / 
Итерация 
Медленная 
Средняя 
1 
Быстрая 
Качество 
градиента 
Плавное, 
точное 
Умеренно шумное 
Сходимость 
Стабильная 
Очень шумное 
Нестабильная 
("метание") 
Баланс скорости и 
стабильности 
Память 
Высокие 
требования 
Умеренные требования 
Векторизация 
на GPU 
Да 
Низкие требования 
Да (эффективно) 
Основное 
применение 
Маленькие 
датасеты 
Нет 
Огромные/потоковы
е данные 
Стандарт для 
нейросетей 
Ключевой вывод: Mini-batch — это оптимальный компромисс между вычислительной 
эффективностью и устойчивостью сходимости, поэтому он используется чаще всего. 
12.  Как работает логистическая регрессия? Почему она называется 
"регрессией", если это классификатор?  
Как работает: 
1. Как и в линейной регрессии, вычисляется линейная комбинация: z = wX + b 
2. Результат пропускается через сигмоиду: σ(z) = 1 / (1 + e^(-z)) 
3. Сигмоида преобразует Score z в вероятность принадлежности к классу: 
P(y=1|X) = σ(z) 
Почему называется "регрессией"? 
• Модель предсказывает не дискретный класс, а непрерывную вероятность (от 
0 до 1). Задача предсказания непрерывной величины — это по определению 
регрессия. 
Особенности: 
• Граница решения линейна в пространстве признаков. 
• Функция потерь — Log Loss (логарифмические потери): L = -[y*log(p) + (1
y)*log(1-p)] 
• Обучение — через градиентный спуск, аналогично линейной регрессии. 
• Многоклассовая классификация — через стратегии One-vs-Rest или 
использование softmax вместо sigmoid. 
13. Что такое функция потерь (Loss function) в логистической 
регрессии? (Log Loss). 
Формула: 
• Для одного объекта: L = -[y * log(p) + (1 - y) * log(1 - p)] 
• Для всей выборки: J(w) = -(1/m) * Σ [y_i * log(p_i) + (1 - y_i) * log(1 - p_i)] 
Как работает (интуиция): Функция жестоко наказывает модель за два типа ошибок: 
1. Уверенность в неверном ответе: Если истинный класс y=1, а модель 
предсказала p≈0 (уверена, что класса нет), то -log(1-p) станет огромным 
(сильный штраф). 
2. Неуверенность в верном ответе: Штрафует даже за правильные, но 
"неуверенные" предсказания. Например, при y=1 и p=0.6 штраф -log(0.6) будет 
существенно больше, чем при p=0.99. 
Почему не MSE? MSE для вероятностей приводит к невыпуклой функции потерь с 
множеством локальных минимумов, что затрудняет обучение. Log Loss — выпуклая 
функция, гарантирующая нахождение глобального минимума. 
Почему выпуклость так важна? Выпуклая функция потерь гарантирует, что у нас 
есть единственный глобальный минимум. Это означает, что методы оптимизации 
(например, градиентный спуск) гарантированно найдут оптимальные параметры 
модели. 
Доказательство выпуклости для Log Loss: 
Рассмотрим функцию для одного примера: L(z) = -[y * log(σ(z)) + (1-y) * log(1-σ(z))] 
где σ(z) = 1 / (1 + e^(-z)) - сигмоида, z = w*x + b 
1. Выпуклость по z: 
a. Можно показать, что вторая производная ∂²L/∂z² всегда 
неотрицательна. 
b. Для сигмоиды выполняется: σ'(z) = σ(z)*(1-σ(z)) 
c. После вычислений получаем: ∂²L/∂z² = σ(z)*(1-σ(z)) ≥ 0 (так как σ(z) ∈ 
[0,1]) 
2. Выпуклость по w: 
a. Поскольку z - линейная функция от w (z = w*x + b), а композиция 
выпуклой и линейной функции остается выпуклой, то L(w) - выпуклая 
функция по w. 
Сравнение с MSE: MSE для логистической регрессии: L = (y - σ(z))² Вторая 
производная MSE по z может принимать отрицательные значения, что делает 
функцию невыпуклой с множеством локальных минимумов. 
Связь с правдоподобием: Минимизация Log Loss эквивалентна максимизации 
правдоподобия (Maximum Likelihood Estimation) для данных при заданных параметрах 
модели. 
Статистический вывод логистической регрессии: 
1. Модель вероятности: Логистическая регрессия моделирует вероятность как: 
P(y=1|x) = σ(w*x + b) = 1/(1 + e^(-(w*x + b))) 
2. Функция правдоподобия: Для набора данных {(x_i, y_i)} функция 
правдоподобия: 
L(w) = Π [P(y_i=1|x_i)]^y_i * [1 - P(y_i=1|x_i)]^(1-y_i) 
= Π [σ(z_i)]^y_i * [1-σ(z_i)]^(1-y_i) 
3. Логарифмическое правдоподобие: Максимизация правдоподобия 
эквивалентна максимизации логарифма правдоподобия: 
log L(w) = Σ [y_i * log(σ(z_i)) + (1-y_i) * log(1-σ(z_i))] 
4. Преобразование в функцию потерь: В машинном обучении мы обычно 
минимизируем функцию потерь, поэтому берем отрицательное 
логарифмическое правдоподобие: 
J(w) = -log L(w) = -Σ [y_i * log(σ(z_i)) + (1-y_i) * log(1-σ(z_i))] 
Что это означает на практике: 
• Минимизируя Log Loss, мы фактически находим параметры w, которые 
максимизируют правдоподобие наблюдаемых данных 
• Это статистически обоснованный подход, который гарантирует, что мы 
находим наиболее вероятные параметры для объяснения наших данных 
• Связь с MLE обеспечивает хорошие статистические свойства модели 
(например, состоятельность оценок) 
Итог по Log Loss: 
1. Выпуклость → Гарантирует нахождение глобального оптимума 
2. Связь с MLE → Обеспечивает статистическую обоснованность 
3. Поведение штрафов → Сильно наказывает за уверенные ошибки 
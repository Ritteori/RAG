Деревья и ансамбли:  
14. Как строится дерево решений?  
Процесс построения (жадный рекурсивный алгоритм): 
1. Начало: Весь датасет — это корневой узел. 
2. Поиск лучшего разбиения: Для каждого узла алгоритм ищет наилучшее 
разбиение, перебирая: 
a. Все признаки 
b. Все возможные пороги (для непрерывных признаков) 
c. Критерий: Выбирается разбиение, которое дает максимальное 
уменьшение неопределенности (максимальный Information Gain). 
3. Создание ветвей: Узел разделяется на два дочерних узла согласно лучшему 
разбиению. 
4. Рекурсия: Шаги 2-3 повторяются для каждого нового узла. 
5. Критерии остановки: 
a. Достигнута максимальная глубина. 
b. В узле осталось меньше минимального допустимого объектов. 
c. Все объекты в узле принадлежат одному классу (неопределенность = 0). 
d. Разбиение не дает значительного улучшения (Gain меньше порога). 
Критерии неопределенности (Impurity): 
• Энтропия (Entropy): E = - Σ (p_i * log₂(p_i)) 
o Максимальна, когда классы равномерно распределены (p = 0.5). 
o Минимальна (равна 0), когда в узле объекты только одного класса (p = 0 
или p = 1). 
• Индекс Джини (Gini Impurity): G = 1 - Σ (p_i)² 
o Максимальна, когда классы равномерно распределены. 
o Минимальна (равна 0), когда в узле объекты только одного класса. 
15.  В чем преимущества и недостатки деревьев решений?  
Преимущества 
Недостатки 
1. Простота интерпретации (White Box 
model) 
1. Склонность к переобучению 
(особенно без ограничений) 
2. Минимальная предобработка данных 
(не требуют масштабирования признаков, 
устойчивы к выбросам) 
2. Неустойчивость (High Variance) 
(малые изменения в данных сильно 
меняют дерево) 
3. Могут моделировать нелинейные 
зависимости 
3. Жадный алгоритм (не 
гарантирует глобальный оптимум) 
4. Работают с категориальными и 
числовыми признаками 
4. Плохо экстраполируют (не 
предсказывают за пределами train 
data) 
5. Инвариантны к масштабу данных 
5. Проблемы с коррелирующими 
признаками (случайный выбор из 
них) 
Главный вывод: Деревья — отличные интерпретируемые модели, но из-за 
неустойчивости и склонности к переобучению их редко используют поодиночке. Их 
главная сила раскрывается в ансамблях (Random Forest, Gradient Boosting). 
16. Как работает случайный лес (Random Forest)? В чем идея бэггинга 
(Bagging)?  
Идея Bagging (Bootstrap Aggregating): 
1. Создание множества параллельных моделей: 
a. Из исходных данных создается N бутстрап-выборок (выборка того же 
размера, что и исходная, но созданная случайным выбором с 
возвращением). 
2. Обучение на разных данных: 
a. На каждой такой выборке обучается своя отдельная модель (в случае RF 
— дерево решений). 
3. Агрегация результатов: 
a. Итоговый прогноз — это усреднение (регрессия) или мажоритарное 
голосование (классификация) всех моделей. 
Зачем это нужно? Bagging позволяет уменьшить variance модели, не жертвуя сильно 
bias. Это особенно эффективно для моделей с высокой дисперсией, таких как глубокие 
деревья. 
Как работает Random Forest (частный случай Bagging): 
1. Дано: Набор данных, количество деревьев n_estimators. 
2. Для каждого дерева: 
a. Создается бутстрап-выборка из исходных данных. 
b. Строится дерево решений, но с важной модификацией: при каждом 
разбиении узла рассматривается не все признаки, а только случайное 
подмножество размера max_features (например, sqrt(n_features)). 
3. Результат: Ансамбль из некоррелированных между собой деревьев (так как они 
обучены на разных данных и с разными признаками). 
4. Прогноз: Усреднение (регрессия) или голосование (классификация) прогнозов 
всех деревьев. 
Почему это работает? 
• Bagging уменьшает variance за счет усреднения множества деревьев. 
• Случайность признаков делает деревья еще более независимыми друг от 
друга и не дает одному сильному признаку доминировать во всех деревьях. 
17.  Почему случайный лес лучше одного дерева? Как он борется с 
переобучением?  
Почему Random Forest лучше одного дерева: 
1. Сила ансамбля (The Wisdom of Crowds): 
a. Одно дерево сильно зависит от конкретного набора тренировочных 
данных и может ошибаться. 
b. Лес усредняет прогнозы сотен деревьев, обученных на разных данных. 
Ошибки отдельных деревьев компенсируются, что приводит к более 
стабильным и точным предсказаниям. 
2. Кардинальное снижение Variance (Разброса): 
a. Одно дерево имеет высокий variance — небольшое изменение в данных 
радикально меняет его структуру. 
b. Random Forest, усредняя множество таких "неустойчивых" моделей, 
превращает их в одну устойчивую (low-variance) модель. Это основная 
идея бэггинга. 
Как Random Forest борется с переобучением (конкретные механизмы): 
1. Бэггинг (Bootstrap Aggregating): 
a. Каждое дерево обучается на случайном подмножестве данных (с 
повторениями). 
b. Это означает, что каждое дерево переобучается по-своему, на своем 
уникальном шуме. 
c. При агрегации (усреднении/голосовании) этот "шум" усредняется и 
взаимно уничтожается, а общие закономерности — усиливаются. 
2. Случайный выбор признаков (Feature Randomness): 
a. При каждом разбиении узла дерево выбирает из случайного 
подмножества признаков (а не из всех). 
b. Это не дает одному-двум очень сильным признакам доминировать во 
всех деревьях и "натягивать" на себя всю модель, что является частой 
причиной переобучения. 
c. Это заставляет модель учитывать более слабые, но потенциально 
полезные признаки, улучшая обобщающую способность. 
3. Отсутствие необходимости в глубокой обрезке (Pruning): 
a. Отдельные деревья в лесу можно обучать до максимальной глубины 
(fully grown), не боясь переобучения. 
b. Ансамблевый эффект бэггинга и случайности признаков сам по себе 
предотвращает переобучение всей модели, даже если ее компоненты 
(деревья) переобучены. Это контринтуитивно, но это ключевая 
особенность. 
Краткий итог: Случайный лес не столько "борется" с переобучением, сколько делает 
его безвредным через механизм усреднения множества переобученных, но различных 
между собой моделей. 
18. Как работает градиентный бустинг (Gradient Boosting)? Объясни 
основную идею.  
Проблема: Мы хотим использовать мощь ансамблей деревьев, но не параллельно, как 
в Random Forest, а последовательно, чтобы каждое новое дерево исправляло ошибки 
предыдущих. Однако деревья — недифференцируемые модели. 
Решение (Идея): Мы используем принцип градиентного спуска не для параметров 
модели, а для самой модели. 
Алгоритм шаг за шагом: 
1. Инициализация: Начинаем с простой начальной модели F₀(x) (например, 
константа, предсказывающая среднее значение y). F₀(x) = argmin_γ Σ L(y_i, γ) 
2. Последовательное добавление деревьев: Для m = 1 до M (число деревьев): a) 
Вычисление "остатков" (псевдо-остатков): Для каждого объекта i вычисляем 
антиградиент функции потерь по текущему прогнозу: r_{im} = - [∂L(y_i, F(x_i)) 
/ ∂F(x_i)]_{F(x)=F_{m-1}(x)} На практике: Для MSE это будет обычная невязка 
(y - ŷ), для других функций потерь — более сложная величина. 
b) Обучение нового дерева: Обучаем новое дерево h_m(x) предсказывать эти самые 
"остатки" r_{im}. Таким образом, дерево учится исправлять ошибки текущего 
ансамбля F_{m-1}(x). 
c) Обновление ансамбля: Добавляем новое дерево к ансамблю с небольшим шагом 
обучения (learning rate, ν): F_m(x) = F_{m-1}(x) + ν * h_m(x) 
3. Итоговая модель: F(x) = F₀(x) + ν * Σ h_m(x) 
Ключевая аналогия: Представьте, что вы стреляете по мишени. 
• Random Forest — это как попросить 100 человек выстрелить по мишени один 
раз каждый, а потом усреднить их результаты. 
• Gradient Boosting — это как один стрелок, который делает 100 выстрелов. 
После каждого выстрела он смотрит, куда попал (вычисляет ошибку), и 
корректирует прицел для следующего выстрела. 
19. (XGBoost, LightGBM, CatBoost). 
XGBoost (eXtreme Gradient Boosting) 
Плюсы и особенности: 
• Регуляризация: Имеет L1 (Lasso) и L2 (Ridge) регуляризацию в функции 
потерь, что помогает бороться с переобучением 
• Пропущенные значения: Умеет автоматически обрабатывать пропуски в 
данных 
• Блочная структура: Данные хранятся в памяти в специальных блоках, что 
ускоряет обучение 
• Кэширование: Кэширует метрики и индексы для ускорения поиска разбиений 
• Распределенные вычисления: Поддержка распределенных вычислений на 
нескольких узлах 
• Широкое сообщество: Самый популярный вариант, отлично документирован 
LightGBM (Light Gradient Boosting Machine) 
Плюсы и особенности: 
• Скорость: Значительно быстрее XGBoost на больших датасетах 
• Гистограммный метод: Объединяет непрерывные значения в бины 
(гистограммы), что ускоряет поиск лучшего разбиения 
• Growth strategy: 
o Leaf-wise: Растет дерево, выбирая лист с максимальным уменьшением 
потерь (более эффективно, но может переобучаться) 
o В отличие от level-wise (по уровням) в XGBoost 
• Потребление памяти: Занимает значительно меньше памяти 
• Оптимизирован для больших данных: Лучше работает когда объектов много 
(>10k) 
CatBoost (Categorical Boosting) 
Плюсы и особенности: 
• Работа с категориальными признаками: Автоматически обрабатывает 
категориальные фичи без one-hot encoding 
• Ordered boosting: Специальный механизм для борьбы с "target leakage" 
(утечкой целевой переменной) 
• Устойчивость к переобучению: Лучше обобщается без тонкой настройки 
параметров 
• Автоматизация: Требует меньше ручной настройки гиперпараметров 
• Качество на табличных данных: Часто показывает state-of-the-art результаты 
на табличных данных с категориальными признаками 
Сравнение: XGBoost vs Random Forest (Boosting vs Bagging) 
Критерий 
Random Forest (Bagging) 
XGBoost (Boosting) 
Тип ансамбля 
Параллельный 
Последовательный 
Зависимость 
моделей 
Деревья независимы 
Каждое дерево зависит от 
предыдущих 
Цель каждого 
дерева 
Быть максимально точным 
и разнообразным 
Исправить ошибки предыдущих 
деревьев 
Борьба с 
переобучением 
Бэггинг + случайные 
признаки 
Регуляризация + learning rate + 
early stopping 
Скорость 
обучения 
Быстрее (можно 
параллелить) 
Медленнее (последовательное 
обучение) 
Чувствительнос
ть к шуму 
Устойчив к шуму и 
выбросам 
Чувствителен к шуму (может на 
нем фокусироваться) 
Интерпретируем
ость 
Feature Importance через 
усреднение 
Более сложная интерпретация 
Типичная 
глубина 
деревьев 
Полная глубина (до 
переобучения) 
Неглубокие деревья (обычно 3-8 
уровней) 
Сходимость 
Быстрое начальное 
улучшение 
Медленнее, но может достичь 
большей точности 
Практические рекомендации по выбору 
• Начинать с: Random Forest — быстрый, устойчивый, хорош для baseline 
• Для максимальной точности: XGBoost/LightGBM с тщательной настройкой 
• Для больших данных: LightGBM (скорость и память) 
• Для категориальных данных: CatBoost (минимум предобработки) 
• Для продакшена: XGBoost (надежность, сообщество, документация) 
Ключевое отличие: Bagging уменьшает variance, Boosting уменьшает bias. Поэтому на 
практике бустинг часто показывает лучшую точность, но требует более аккуратной 
настройки и больше вычислительных ресурсов.
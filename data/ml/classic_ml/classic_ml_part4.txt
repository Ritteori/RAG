Кластеризация:  
20.  Как работает k-Means? Опиши алгоритм шаг за шагом. 
Цель: Разбить данные на k кластеров так, чтобы объекты внутри одного кластера были 
максимально похожи друг на друга, а объекты из разных кластеров — максимально 
отличались. 
Шаги алгоритма: 
1. Инициализация: Случайно выбираем k точек из датасета в качестве начальных 
центроидов (условных "центров" кластеров). 
2. Шаг присваивания (Assignment Step): 
a. Для каждого объекта в датасете вычисляем расстояние до всех k 
центроидов (чаще всего — евклидово расстояние). 
b. Приписываем каждый объект к тому кластеру, чей центроид оказался 
ближе всего к нему. 
c. Теперь все данные разбиты на k групп. 
3. Шаг обновления (Update Step): 
a. Для каждого из k кластеров пересчитываем его центроид. Новый 
центроид — это среднее арифметическое всех точек, приписанных к 
этому кластеру. 
b. Формально: новый центроид = среднее по всем координатам точек 
кластера. 
4. Проверка сходимости: 
a. Проверяем, изменились ли положения центроидов по сравнению с 
предыдущим шагом (или изменилась ли принадлежность точек к 
кластерам). 
b. Если да: Возвращаемся к Шагу 2. 
c. Если нет: Алгоритм завершен. Кластеры стабилизировались. 
Простая аналогия: Представьте, что у вас есть мешок с разноцветными шариками 
(ваши данные), и вы хотите разложить их по k коробкам (кластерам) по цвету. 
1. Вы наугад достаете k шариков и кладете каждый в свою пустую коробку — это 
ваши начальные "центроиды". 
2. Берете каждый шарик из мешка и смотрите, к какому из k шариков в коробках 
он по цвету ближе. Кладете его в ту же коробку. 
3. Теперь в каждой коробке лежит несколько шариков. Вы находите "средний 
цвет" шариков в каждой коробке — это ваш новый центроид. 
4. Повторяете шаги 2 и 3 до тех пор, пока шарики не перестанут "перепрыгивать" 
из коробки в коробку. 
Наглядная визуализация, которая поможет запомнить: (Представь это мысленно) 
1. (Инициализация) Случайные центроиды:       
2. (Присваивание) Все точки поблизости от   
от   
зелеными. 
3. (Обновление)   
стали красными, от   
переехал в центр своих красных точек.   
и   
синими, 
тоже 
переехали. 
4. (Повтор) Точки снова перераспределились по новым центрам. Центроиды 
снова сдвинулись. 
5. (Стоп) Центроиды больше не двигаются. Кластеры готовы. 
21.  Как выбирают число кластеров k в k-Means? (Метод локтя, 
Silhouette Score). 
Так как это задача без учителя, у нас нет готовых ответов, чтобы проверить качество. 
Мы оцениваем его косвенно. 
1. Метод Локтя (Elbow Method) 
Идея: С ростом числа кластеров k объекты внутри кластеров становятся ближе к своим 
центрам. Мы смотрим, когда это "уплотнение" кластеров перестает быть 
существенным. 
Как это работает: 
1. Запускаем k-Means для разного количества кластеров (например, k = 1, 2, 3, ..., 
10). 
2. Для каждого k вычисляем Within-Cluster Sum of Squares (WCSS) — сумму 
квадратов расстояний от каждой точки до центроида ее кластера. WCSS = Σ Σ ||x - c_i||² (чем меньше, тем "плотнее" кластеры) 
3. Строим график: по оси X — k, по оси Y — WCSS. 
4. Ищем на графике "локоть" (elbow) — точку, после которой падение WCSS 
замедляется. Эта точка и есть наш кандидат на оптимальное k. 
Интуиция: 
• Представьте, что вы растягиваете пружину. Сначала она растягивается легко 
(k=1 -> k=2 -> k=3 дает большой выигрыш), а потом усилия растут, а результат 
почти не меняется. Тот момент, когда пружина начинает "сопротивляться", и 
есть "локоть". 
• Недостаток: "Локоть" не всегда явно выражен, решение может быть 
субъективным. 
2. Silhouette Score (Силуэтный коэффициент) 
Идея: Метод оценивает, насколько хорошо каждый объект лег в свой кластер, 
сравнивая расстояние до "своего" кластера с расстояниями до "соседних" кластеров. 
Как это работает (упрощенно): 
1. Для каждой точки вычисляют: 
a. a = среднее расстояние до других точек в своем кластере (насколько я 
близок к своим). 
b. b = среднее расстояние до точек в следующем ближайшем кластере 
(насколько я далек от чужих). 
2. Для каждой точки вычисляют силуэтный коэффициент: s = (b - a) / max(a, b) 
3. Усредняют s по всем точкам датасета. Получается Silhouette Score от -1 до 1. 
Как интерпретировать: 
• Score ≈ 1: Идеально. Объекты находятся в правильных кластерах. 
• Score ≈ 0: Кластеры перекрываются, объекты находятся на границе. 
• Score ≈ -1: Объекты назначены неверно. 
Как выбирать k: Запускаем k-Means для разных k, для каждого считаем усредненный 
Silhouette Score. Выбираем k с наибольшим значением Score. 
Краткий итог: 
• Метод локтя: Смотрим на график WCSS, ищем изгиб. Быстро и интуитивно. 
• Silhouette Score: Вычисляем метрику качества, берем k с лучшим score. Более 
формальный и надежный. 
На практике часто используют оба метода для уверенности. 

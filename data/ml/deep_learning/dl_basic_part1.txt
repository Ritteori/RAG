Раздел 2: Глубокое обучение (Deep Learning) 
Основы нейросетей:  
1. Из чего состоит искусственный нейрон? 
Искусственный нейрон — это основная вычислительная единица нейронной сети, 
математическая модель, вдохновленная биологическим нейроном. 
Он состоит из трех основных компонентов: 
1. Входы (Inputs) и Веса (Weights): 
a. Нейрон получает на вход несколько числовых сигналов x₁, x₂, ..., xₙ. 
b. Каждому входу xᵢ соответствует вес wᵢ, который представляет его 
"важность". Сильный вес усиливает сигнал, слабый — ослабляет. 
2. Сумматор (Summing Function) и Смещение (Bias): 
a. Нейрон вычисляет взвешенную сумму своих входов: z = w₁x₁ + w₂x₂ + ... 
+ wₙxₙ. 
b. К этой сумме добавляется смещение (bias) b — параметр, который 
позволяет сдвигать функцию активации для лучшего обучения. Итог: z = 
Σ(wᵢxᵢ) + b. 
3. Функция активации (Activation Function): 
a. Полученная сумма z пропускается через нелинейную функцию 
активации f(z). 
b. Зачем это нужно? Чтобы нейросеть могла обучаться и представлять 
сложные, нелинейные зависимости в данных. Без нее вся сеть была бы 
просто большой линейной моделью. 
c. Итоговый выход нейрона: y = f(z) = f(Σ(wᵢxᵢ) + b). 
Аналогия: Представьте, что нейрон — это маленький процессор, который: 
• Взвешивает входящие сигналы (умножает на wᵢ). 
• Суммирует все доказательства (вычисляет z). 
• Принимает нелинейное решение (применяет f(z)) — "активироваться" или 
нет, и насколько сильно. 
2. Что такое функция активации? Зачем она нужна? Назови 
популярные (Sigmoid, Tanh, ReLU, Leaky ReLU).  
Зачем нужна? Чтобы придать нейронной сети нелинейность. Без нее, сколько бы 
слоев ни было, сеть оставалась бы просто линейным преобразованием (композиция 
линейных функций = линейная функция) и не могла бы аппроксимировать сложные 
зависимости. 
Популярные функции и их свойства: 
Функци
я 
Формула / 
Описание Плюсы Минусы 
Где 
использует
ся 
Sigmoid 
σ(x) = 1 / (1 + e^{
x}) 
Диапазон: (0, 1) 
Интерпретируе
мый выход 
(вероятность) 
Затухающие 
градиенты, не 
нулевое среднее, 
медленные 
вычисления 
Выходной 
слой 
бинарной 
классифик
ации 
Tanh 
tanh(x) = (e^x - e^{
x}) / (e^x + e^{-x}) 
Диапазон: (-1, 1) 
Нулевое 
среднее 
(сходится 
быстрее 
sigmoid) 
Затухающие 
градиенты 
Скрытые 
слои RNN 
ReLU f(x) = max(0, x) 
Простая и 
быстрая, 
решает 
проблему 
затухания 
градиентов для 
x>0 
"Умирающий 
ReLU": нейроны с 
x<0 "умирают" и не 
обучаются 
Стандарт 
для 
скрытых 
слоев CNN 
и MLP 
Leaky 
ReLU 
f(x) = max(αx, x), 
где α ~ 0.01 
Решает 
проблему 
"умирающего 
ReLU" 
Результаты не 
всегда устойчивы 
Когда есть 
риск 
"мертвых" 
нейронов 
GELU 
x * Φ(x) 
(умножение на 
функцию 
распределения) 
Аппроксимируется
Плавная 
аппроксимация 
ReLU, работает 
лучше в 
Вычислительно 
сложнее 
Transforme
r
архитектур
ы (BERT, 
GPT) 
: 0.5x * (1 + 
tanh[√(2/π)(x + 
0.044715x³])) 
некоторых 
случаях 
SELU 
λ * x (если x>0) 
или λ * α(e^x - 1) 
(если x≤0) 
Специальные λ и α 
Самомасштаби
рующаяся: 
автоматически 
нормализует 
выходы к 
нулевому 
среднему и 
единичной 
дисперсии 
Практический вывод: 
Требует 
специальной 
инициализации 
весов 
(lecun_normal) и 
архитектуры 
(последовательной) 
SNN (Self
Normalizing 
Neural 
Networks) 
• Скрытые слои: Начинай с ReLU. Если есть подозрение на "мертвые нейроны" 
— пробуй Leaky ReLU. 
• Выходной слой: 
o Бинарная классификация: Sigmoid. 
o Многоклассовая классификация: Softmax (не упомянут в вопросе, но 
критически важен). 
o Регрессия: Линейная активация (или нет активации). 
3.  Почему ReLU стала такой популярной? В чем ее проблема 
("умирающий ReLU") и как ее решают?  
Почему ReLU стала такой популярной? 
1. Вычислительная простота: Операция max(0, x) выполняется несоизмеримо 
быстрее, чем вычисление экспоненты в sigmoid/tanh. 
2. Решение проблемы затухающих градиентов: Для положительных значений 
производная ReLU постоянна и равна 1. Это позволяет градиентам 
беспрепятственно течь через множество слоев во время обратного 
распространения, в отличие от sigmoid, чья производная быстро стремится к 
нулю. 
3. Разреженность представлений: ReLU "обнуляет" половину активаций (все 
отрицательные), что делает модель более разреженной, эффективной и менее 
склонной к переобучению. 
В чем проблема "умирающего ReLU"? 
Проблема возникает, когда нейрон "застревает" в состоянии, где для всех входных 
данных его выход равен 0. 
• Как это происходит? 
o Если веса нейрона таковы, что линейная комбинация z = w*x + b для 
всех примеров в батче становится отрицательной, то выход ReLU будет 
всегда 0. 
o Соответственно, градиент по этим весам тоже будет равен 0 (так как 
производная ReLU при x < 0 равна 0). 
o Раз градиент нулевой, веса никогда не обновятся. Нейрон "мертв" и 
больше не участвует в обучении. 
Как решают эту проблему? 
1. Leaky ReLU: Вместо того чтобы возвращать 0 для отрицательных значений, 
возвращает небольшую, ненулевую величину: f(x) = max(α * x, x), где α — 
маленький коэффициент (например, 0.01). Это гарантирует, что градиент для 
отрицательных значений будет ненулевым, и нейрон имеет шанс "ожить". 
2. Parametric ReLU (PReLU): Это Leaky ReLU, где коэффициент α не 
фиксирован, а обучается в процессе. Это позволяет нейросети самой найти 
оптимальный "уклон" для отрицательной области. 
3. Exponential Linear Unit (ELU): f(x) = x при x > 0 и f(x) = α(e^x - 1) при x ≤ 0. 
Эта функция плавно приближается к значению -α при x -> -∞, что помогает 
ускорить обучение и получить более качественные представления. 
4. Инициализация весов и настройка скорости обучения: Правильная 
инициализация (например, He Initialization) и не слишком высокий learning rate 
могут значительно снизить риск возникновения этой проблемы. 
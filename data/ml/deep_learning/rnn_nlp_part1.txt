RNN & NLP:  
12. Зачем нужны рекуррентные сети (RNN)? В чем их проблема 
(исчезающий градиент)? 
Зачем нужны RNN? Обычные нейросети (полносвязные и сверточные) предполагают, 
что все входы и выходы независимы друг от друга. RNN были созданы для работы с 
последовательностями, где порядок и контекст имеют критическое значение. 
• Примеры задач: 
o Машинный перевод: Последовательность слов на одном языке -> 
последовательность слов на другом. 
o Генерация текста: Предсказание следующего слова на основе 
предыдущих. 
o Анализ тональности: Классификация всего предложения, а не 
отдельных слов. 
Как работает RNN (интуиция): У RNN есть "внутренняя память" (hidden state h_t), 
которая хранит информацию о всех предыдущих элементах последовательности. 
• На каждом шаге t сеть: 
o Получает на вход текущий элемент последовательности x_t и 
предыдущее состояние памяти h_{t-1}. 
o Вычисляет новое состояние памяти: h_t = f(W * [h_{t-1}, x_t] + b) 
o Выдает выход y_t (например, вероятность следующего слова). 
Это создает "петлю", позволяя информации передаваться от одного шага к другому. 
В чем проблема RNN (Исчезающий / Взрывающийся Градиент)? 
Проблема возникает при обучении с помощью Backpropagation Through Time 
(BPTT), когда градиенты распространяются назад по всей последовательности. 
1. Причина: Градиент потерь по весам сети вычисляется как сумма градиентов в 
каждый момент времени. Этот градиент включает в себя многократное 
умножение одних и тех же весовых матриц. 
a. Формально: ∂L/∂W = Σ ∂L_t/∂W, и каждый ∂L_t/∂W включает 
произведение ∂h_t/∂h_k, которое раскладывается в произведение матриц 
Якоби. 
2. Исчезающий градиент (Vanishing Gradient): 
a. Если собственные значения матрицы весов |λ| < 1, то при умножении 
градиент будет экспоненциально затухать с каждым шагом. 
b. Последствие: Веса, отвечающие за долгосрочные зависимости, почти не 
обновляются. Сеть "забывает" то, что было в начале длинной 
последовательности и учится учитывать только ближайший контекст. 
3. Взрывающийся градиент (Exploding Gradient): 
a. Если собственные значения матрицы весов |λ| > 1, то градиент будет 
экспоненциально расти. 
b. Последствие: Веса обновляются слишком сильно, что приводит к 
нестабильности обучения и численной неустойчивости (значения 
становятся NaN). 
Борьба с проблемами: 
• Против взрыва: Gradient Clipping — обрезание градиента, если его норма 
превышает порог. 
• Против затухания: Использование более сложных архитектур — LSTM и 
GRU, которые были специально разработаны для решения этой проблемы. 
 
class MyRNN(nn.Module): 
    def __init__(self, input_dim, hidden_dim, output_dim): 
        super().__init__() 
        # Матрицы весов 
        self.Wxh = nn.Parameter(torch.randn(hidden_dim, input_dim))   # вход → скрытое 
        self.Whh = nn.Parameter(torch.randn(hidden_dim, hidden_dim))  # скрытое → скрытое 
        self.Why = nn.Parameter(torch.randn(output_dim, hidden_dim))  # скрытое → выход 
 
        # Смещения 
        self.bh = nn.Parameter(torch.zeros(hidden_dim)) 
        self.by = nn.Parameter(torch.zeros(output_dim)) 
 
    def forward(self, x): 
        """ 
        x: [batch_size, seq_len, input_dim] 
        """ 
        batch_size, seq_len, _ = x.shape 
        h = torch.zeros(batch_size, self.Wxh.shape[0])  # начальное скрытое состояние 
 
        outputs = [] 
        for t in range(seq_len): 
            xt = x[:, t, :]  # вход на шаге t 
 
            # Обновляем скрытое состояние 
            h = torch.tanh(xt @ self.Wxh.T + h @ self.Whh.T + self.bh) 
 
            # Вычисляем выход 
y = h @ self.Why.T + self.by 
outputs.append(y) 
outputs = torch.stack(outputs, dim=1)  # [batch_size, seq_len, output_dim] 
return outputs, h 
13. Что такое LSTM и GRU? Как они решают проблему долгосрочной 
зависимости?  
Проблема простой RNN в том, что ее состояние h — это "бутылочное горлышко". При 
каждом шаге информация из предыдущего состояния перемешивается с новым входом 
и перезаписывается. Важная информация из начала последовательности ("Кот") легко 
"вымывается" к концу. 
LSTM (Long Short-Term Memory) решает это, вводя механизм "вентилей" (gates), 
которые управляют потоком информации. 
У LSTM есть два состояния, передаваемых между шагами: 
1. Скрытое состояние (h_t): Аналогично состоянию в простой RNN, 
используется для вычисления выхода. 
2. Вектор клеточного состояния (C_t): Это "долговременная память". 
Информация в него может записываться, сохраняться или удаляться с 
помощью специальных вентилей. 
Ключевые вентили LSTM на нашем примере: 
• На шаге "на": 
o Вентиль забывания (Forget Gate): Решает, какую информацию из 
долговременной памяти (C_2) стоит забыть. Например: "Уже знаем, что 
'Кот сидит', можно забыть точную форму глагола, но не сам факт". 
o Вентиль входа (Input Gate): Решает, какую новую информацию из 
текущего входа ("на") записать в долговременную память. 
o Выходной вентиль (Output Gate): Решает, какую часть долговременной 
памяти (C_3) использовать для формирования нового скрытого 
состояния (h_3), которое пойдет на предсказание слова. 
GRU (Gated Recurrent Unit) — упрощенный брат LSTM. 
• У него только одно состояние (h_t). 
• Всего два вентиля: 
o Вентиль сброса (Reset Gate): Определяет, какая часть предыдущей 
информации не нужна для вычисления нового состояния. 
o Вентиль обновления (Update Gate): Определяет, какая доля старого 
состояния сохранится, а какая доля будет заменена на новую 
информацию. 
Итог: Вместо того чтобы слепо перезаписывать память на каждом шаге, LSTM и GRU 
избирательно обновляют ее, сохраняя критически важную информацию с самого 
начала последовательности ("Кот" — это субъект, о котором идет речь) на протяжении 
всего предложения. 
14. Что такое механизм внимания (Attention)? Объясни интуицию.  
Интуиция: Attention — это механизм, который позволяет модели при обработке 
каждого элемента последовательности (например, слова) "взвешивать" и 
"фокусироваться" на других, наиболее релевантных элементах этой же (или другой) 
последовательности. 
Решаемая проблема: 
• В RNN/LSTM информация о начале последовательности "размывалась" и 
"забывалась" к концу. 
• В encoder-decoder архитектурах без внимания encoder был вынужден "втиснуть" 
всю информацию исходного предложения в один фиксированный вектор 
контекста, что было узким местом. 
Как работает (на примере Self-Attention в Transformer): 
1. Векторы Query, Key, Value: 
a. Каждое слово (токен) в последовательности преобразуется в три разных 
вектора: 
i. Query (Запрос): "Что я ищу?" 
ii. Key (Ключ): "Какой я могу дать ответ на запрос?" 
iii. Value (Значение): "Какую информацию я несу по сути?" 
b. Получаются матрицы Q, K, V. 
2. Вычисление весов внимания: 
a. Мы хотим понять, насколько каждое слово должно влиять на каждое 
другое. 
b. Для этого Q одного слова скалярно умножается на K всех слов. Чем 
больше скалярное произведение, тем сильнее "связь" между словами. 
c. Формула: Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V 
d. Масштабирование (/ sqrt(d_k)): Необходимо, чтобы перед softmax 
скалярные произведения не уходили в очень большие значения, где 
градиент softmax был бы близок к нулю. 
3. Multi-Head Attention: 
a. Вместо одного набора Q/K/V используют несколько (например, 8). 
Каждая "голова" обучается искать зависимости в своем подпространстве 
(например, одна — синтаксические, другая — семантические). Их 
результаты затем конкатенируются. 
Аналогия: При переводе предложения "Кот сидит на столе" на английский, чтобы 
правильно перевести слово "столе", модель с вниманием будет: 
• Смотреть на "Кот" (чтобы понять субъект). 
• Смотреть на "сидит" (чтобы понять действие). 
• Сильнее всего смотреть на "на" (чтобы правильно выбрать предлог "on"). 
• И, конечно, смотреть на само слово "столе". 
Плюсы: 
• Позволяет работать с очень длинными зависимостями. 
• Повышает интерпретируемость (можно посмотреть, на какие слова модель 
"смотрела"). 
• Вычислительно распараллеливается (в отличие от последовательных RNN). 
Минус: 
• Квадратичная сложность O(N^2), так как нужно посчитать попарные 
взаимодействия всех токенов. Для очень длинных текстов (тысячи токенов) это 
становится вычислительно неприемлемым. 
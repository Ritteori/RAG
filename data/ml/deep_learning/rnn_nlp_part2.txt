15. Что такое Word2Vec, GloVe? Чем они лучше one-hot encoding? 
1. One-Hot Encoding (Проблема, которую решают) 
• Что это? Вектор размерности словаря, где почти все нули, и одна единица на 
позиции, соответствующей слову. 
o Слово "король" = [0, 0, 1, 0, ..., 0] 
o Слово "царица" = [0, 1, 0, 0, ..., 0] 
• Недостатки: 
o Высокая размерность: Размер вектора равен размеру словаря 
(десятки/сотни тысяч). Вычислительно неэффективно. 
o "Проклятие размерности": Данные становятся чрезвычайно 
разреженными. 
o Отсутствие семантики: Все векторы ортогональны. Скалярное 
произведение между любыми двумя разными словами равно 0. Модель 
"не знает", что "король" и "царица" похожи. 
2. Word2Vec (Представление слов вложенными векторами) 
Основная идея: "Слово определяется своим окружением". Значение слова задается 
словами, которые часто встречаются рядом с ним. 
• Как работает: Обучает нейросеть предсказывать слово по его контексту (Skip
gram) или контекст по слову (CBOW). На выходе нас интересует не 
предсказание, а веса скрытого слоя, которые и становятся векторным 
представлением слова. 
• Результат: Каждое слово — это плотный вектор фиксированного размера 
(например, 100-300 измерений). 
• Ключевое свойство: Семантически близкие слова имеют близкие векторы в 
этом пространстве. Более того, в нем возникают линейные закономерности: 
Вектор("король") - Вектор("мужчина") + Вектор("женщина") ≈ 
Вектор("царица"). 
3. GloVe (Global Vectors for Word Representation) 
Основная идея: Комбинирует идеи Word2Vec и глобальной статистики всего корпуса 
текстов. 
• Как работает: GloVe анализирует всю текстовую коллекцию сразу и строит 
глобальную матрицу совместной встречаемости слов (сколько раз слово j 
появлялось в контексте слова i). Затем она обучает векторы так, чтобы их 
скалярное произведение было пропорционально логарифму частоты их 
совместной встречаемости. 
• Проще говоря: Если два слова часто встречаются вместе (как "лед" и "холод"), 
их векторы будут близки. 
Сравнительная таблица 
Критери
й 
One-Hot 
Encoding 
Word2Vec 
GloVe 
Размерн
ость 
Семант
ика 
Огромная 
(размер 
словаря) 
Нет 
Компактная (100-300) 
Есть (учитывает 
локальный контекст) 
Компактная (100-300) 
Есть (учитывает 
глобальную статистику) 
Вычисл
ения 
Неэффективн
ые 
Эффективные 
Эффективные 
Основа Индекс слова Локальные контекстные 
Глобальная статистика 
встреч 
предсказания 
Аналоги
я 
Уникальный 
ID-номер 
Понимание смысла из 
непосредственного 
окружения 
Понимание смысла из 
анализа всех употреблений 
в языке 
Итог: Word2Vec и GloVe — это следующие шаги после one-hot encoding. Они 
превращают слова из "бессмысленных меток" в компактные векторы, несущие 
семантический смысл, что является фундаментом для всех современных моделей 
NLP. 
16. Что такое BERT? В чем его основная идея предобучения? 
BERT (Bidirectional Encoder Representations from Transformers) — это большая 
языковая модель, основанная исключительно на энкодере из архитектуры 
Transformer. Ключевое слово здесь — Bidirectional (двунаправленный). 
В отличие от моделей вроде GPT, которые генерируют текст слева направо (и видят 
только "прошлые" слова), BERT при обработке каждого слова в последовательности 
видит все окружающие его слова — и слева, и справа. Это позволяет ему лучше 
понимать контекст. 
Основная идея предобучения BERT 
Главная инновация BERT — это его задачи для самообучающегося предобучения 
(self-supervised pre-training) на большом объеме неразмеченного текста (например, 
всей Википедии). 
Он обучается на двух задачах одновременно: 
1. Задача Masked Language Model (MLM) — "Замаскированный язык" 
• Что происходит: BERT на вход получает предложение, в котором 15% слов 
были случайным образом заменены на специальный токен [MASK]. 
o Пример: Исходное: "Кот сидит на столе." 
o После маскирования: "Кот [MASK] на столе." 
• Задача модели: Предсказать исходное замаскированное слово ("сидит") на 
основе всего контекста, включая слова, которые идут после маски. 
• Зачем это нужно: Это заставляет модель научиться глубоко понимать контекст 
каждого слова с двух сторон, а не только слева направо. Именно это делает 
BERT таким мощным для задач понимания языка. 
2. Задача Next Sentence Prediction (NSP) — "Предсказание следующего предложения" 
• Что происходит: BERT на вход получает два предложения, разделенных 
токеном [SEP]. В 50% случаев это два соседних предложения из текста, а в 50% 
— второе предложение берется случайно из другой статьи. 
o Пример 1 (IsNext): [CLS] Кот сидит на стле [SEP] Он любит молоко 
[SEP] 
o Пример 2 (NotNext): [CLS] Кот сидит на стле [SEP] Машина едет по 
дороге [SEP] 
• Задача модели: Определить, являются ли эти два предложения соседними в 
исходном тексте (IsNext) или нет (NotNext). Ответ формируется на основе 
специального токена [CLS]. 
• Зачем это нужно: Эта задача учит модель понимать логические связи между 
предложениями, что критически важно для таких задач, как ответы на вопросы 
или логический вывод. 
Почему этот подход революционный? 
1. Универсальность: Предобученная модель BERT содержит в себе мощные 
представления слов и предложений. Её можно дообучить (fine-tune) с помощью 
всего одного дополнительного выходного слоя для широкого спектра задач: 
классификация текста, NER, ответы на вопросы, etc. 
2. Глубокое контекстное понимание: Благодаря MLM, BERT понимает значение 
слова в полном контексте, а не в вакууме. Слово "банка" в контексте 
"рыболовная" и "стеклянная" будет иметь разные векторные представления. 
3. Отказ от авторегрессии: В отличие от GPT, BERT не генерирует текст 
последовательно. Он "осмысляет" сразу весь предоставленный ему текст, что 
делает его идеальным для задач анализа, а не генерации. 
Итог: BERT — это предобученный двунаправленный энкодер, который благодаря 
задачам MLM и NSP научился строить глубокие контекстные представления языка, 
став основой для огромного количества современных NLP-решений.
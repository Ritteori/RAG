CNN (Computer Vision):  
8. Зачем сверточные слои (Convolutional Layers) в изображениях? 
Какую идею они реализуют?  
Какую проблему решают? Полносвязные сети плохо работают с изображениями. Для 
картинки 1000x1000 пикселей было бы 3 миллиона весов в первом же слое! Кроме 
того, такой слой не учитывает: 
1. Пространственную локальность: Важные признаки находятся в соседних 
пикселях. 
2. Трансляционную инвариантность: Объект важен сам по себе, а не его точное 
положение в кадре. 
Какую идею реализуют? 
1. Локальная связность (Local Connectivity): 
a. Нейрон сверточного слоя связан не со всем изображением, а лишь с 
небольшей областью (например, 3x3). Это ядро (filter) скользит по всему 
изображению, выделяя один конкретный низкоуровневый признак 
(например, вертикальный край) в каждой позиции. 
2. Разделение весов (Weight Sharing): 
a. Одно и то же ядро (с одними и теми же весами) используется для 
сканирования всей картинки. Это значит, что сеть учит не признаки для 
конкретного места, а универсальные признаки (вертикальный край 
важен и вверху, и внизу изображения). Это и дает инвариантность к 
сдвигу и радикально сокращает число параметров. 
3. Иерархия признаков (Feature Hierarchy): 
a. Первый сверточный слой учится находить простейшие паттерны: 
градиенты, края, углы. 
b. Второй слой, получая на вход карты признаков первого, комбинирует 
эти простые края в более сложные паттерны: углы, текстуры, контуры. 
c. Последующие слои комбинируют текстуры в части объектов (ушко, 
глазко), а те — в целые объекты (кошка, собака). 
Итог: Сверточные слоя — это умный способ работать с изображениями, который 
учитывает их двумерную природу, радикально сокращает параметры и строит 
иерархию признаков от простых к сложным. 
9. Что такое пулинг (Pooling)? Зачем он нужен? (Max Pooling, Average 
Pooling). 
Что это? Пулинг (объединяющий слой) — операция, которая уменьшает размерность 
карт признаков (feature maps), сохраняя наиболее важную информацию. 
Зачем он нужен? (Основные причины): 
1. Уменьшение вычислительной нагрузки и количества параметров: 
Меньший размер feature map означает, что последующие слои (особенно 
полносвязные) будут иметь намного меньше параметров для обучения. 
2. Контроль переобучения (Regularization): Уменьшая размерность, мы 
заставляем сеть быть более устойчивой к небольшим смещениям и искажениям 
входных данных. 
3. Повышение инвариантности к местоположению (Translation Invariance): 
Пулинг помогает сети сосредоточиться на наличии признака ("здесь есть 
глаз"), а не на его точном местоположении ("глаз находится ровно в 
координатах x=155, y=243"). Это делает модель более робастной. 
4. Выделение наиболее сильных признаков: Особенно в случае Max Pooling, мы 
оставляем только самые активные нейроны, которые скорее всего 
соответствуют настоящим, значимым признакам. 
Основные типы: 
• Max Pooling (Максимальный пулинг): 
o Как работает: Из выбранной области (например, 2x2) выбирается 
максимальное значение. 
o Интуиция: "Если хотя бы один нейрон в этом регионе активировался 
сильно, значит, признак здесь есть". Отлично выделяет самые яркие 
черты. 
o Наиболее популярный метод. 
• Average Pooling (Усредняющий пулинг): 
o Как работает: Из выбранной области вычисляется среднее 
арифметическое всех значений. 
o Интуиция: "Давайте усредним информацию по всей области". Может 
сглаживать и терять резкие, но важные признаки. 
o Применение: Чаще используется в самых глубоких слоях классических 
сетей (например, в конце перед полносвязным слоем). 
Аналогия: Представьте, что вы смотрите на карту города с отмеченными 
достопримечательностями (ваши feature maps). 
• Max Pooling — это как взять каждый район города и оставить на новой, 
уменьшенной карте только главную достопримечательность этого района. 
• Average Pooling — это как вычислить среднюю "интересность" каждого 
района. 
Итог: Пулинг — это не просто "изменение размера", а стратегическая операция для 
уменьшения вычислений, борьбы с переобучением и выделения устойчивых 
признаков. 
10. Назови известные архитектуры CNN (VGG, ResNet, Inception) и их 
ключевые фичи. 
1. VGG (Visual Geometry Group) 
• Ключевая фича: Глубина через однородность. 
• Описание: 
o Состоит из последовательных блоков, где каждый блок содержит 
несколько (2-4) сверточных слоев 3x3 с шагом 1 и дополнением 1 
(сохраняя размер), за которыми следует один слой Max Pooling 2x2 
(уменьшая размер в 2 раза). 
o Идея: Несколько сверток 3x3 подряд имеют рецептивное поле одного 
сверточного слоя 5x5 или 7x7, но при этом содержат больше 
нелинейностей (больше функций активации между слоями) и меньше 
параметров. 
• Вклад: Показала важность глубины сети. VGG-16 (16 весовых слоев) и VGG-19 
стали классическими и очень влиятельными архитектурами. 
2. ResNet (Residual Network) 
• Ключевая фича: Остаточные блоки (Residual Blocks) и Skip-Connections. 
• Описание: 
o Вводит "короткие сквозные связи" (skip-connections), которые 
пропускают один или несколько слоев. 
o Блок учит остаточную функцию (residual function): F(x) = H(x) - x, а 
итоговый выход блока: H(x) = F(x) + x. 
o Решаемая проблема: Проблема исчезающего градиента в очень 
глубоких сетях. Skip-connection позволяет градиентам беспрепятственно 
течь напрямую от более поздних слоев к ранним. 
• Вклад: Позволила обучать сети в сотни и даже тысячи слоев (ResNet-152, 
ResNet-1000) без деградации производительности. 
3. Inception (GoogLeNet) 
• Ключевая фича: Параллельные свертки внутри одного слоя (Inception 
Module). 
• Описание: 
o Основная идея — вместо того чтобы выбирать размер ядра (1x1, 3x3, 
5x5), делать их все параллельно и конкатенировать результаты. 
o Классический Inception-модуль содержит параллельно: 
▪ Свертка 1x1 (для выделения локальных признаков) 
▪ Свертка 3x3 
▪ Свертка 5x5 
▪ Max Pooling 3x3 
o Проблема: Прямая реализация слишком затратна. Решение — 
использование сверток 1x1 (bottleneck layers) перед свертками 3x3 и 
5x5 и после пулинга для уменьшения размерности и числа параметров. 
• Вклад: Эффективное использование вычислительных ресурсов и извлечение 
признаков на разных масштабах в пределах одного слоя. 
Сравнительная таблица: 
Архитект
ура 
Ключевая Идея 
VGG 
Однородные блоки из сверток 
3x3 
Главное Преимущество 
Простота и демонстрация важности 
глубины 
ResNet 
Остаточные связи (F(x) + x) 
Обучение экстремально глубоких 
сетей 
Inception 
Параллельные свертки и 
конкатенация 
Эффективность и анализ в разных 
масштабах 
11. В чем была основная идея ResNet (Residual Networks)? Зачем нужны 
skip-connections? 
Проблема: В очень глубоких сетях (20+ слоев) происходят две вещи: 
1. Исчезающий/взрывающийся градиент — градиенты становятся слишком 
малы/велики при прохождении через множество слоев. 
2. Деградация (Degradation Problem) — добавление слоев приводит не к 
улучшению, а к ухудшению качества как на тренировочной, так и на тестовой 
выборке. Сеть не может даже выучить тождественную функцию. 
Решение ResNet — Остаточные Блоки (Residual Blocks): 
• Формула: H(x) = F(x, {W_i}) + x 
o x — вход в блок 
o F(x, {W_i}) — обучаемое остаточное отображение (2-3 сверточных слоя) 
o H(x) — итоговое отображение, которое мы хотим получить 
• Философия: Вместо того чтобы учить прямое отображение H(x) (например, из 
10 в 12), сеть учит разницу (residual) между входом и выходом: F(x) = H(x) - x 
(например, 12 - 10 = +2). 
• Роль Skip-Connection (+ x): 
o Супер-магистраль для градиентов: Позволяет градиентам течь 
напрямую от выходных слоев к ранним, минуя нелинейные 
преобразования. Это решает проблему затухания градиентов. 
o Упрощение задачи: Сети проще обучаться небольшим отклонениям 
F(x) от тождественной функции, чем учить полное преобразование "с 
нуля". По сути, по умолчанию каждый блок начинает как тождественная 
функция (F(x) = 0), а потом учится вносить полезные поправки. 
o "Безопасная" глубина: Даже если остаточное отображение F(x) в 
каком-то блоке станет нулевым, блок все равно будет пропускать сигнал 
(H(x) = x). Это позволяет строить сети экстремальной глубины (ResNet
152, ResNet-1000) без деградации производительности. 
Аналогия: Представьте, что вы учитесь играть на пианино. 
• Без ResNet: Каждый раз вам нужно играть всю песню с начала. 
• С ResNet: Вы уже умеете играть базовый вариант песни (это x). Ваша задача — 
выучить лишь небольшие украшения и вариации (F(x)), чтобы сделать ее лучше. 
Это гораздо проще. 
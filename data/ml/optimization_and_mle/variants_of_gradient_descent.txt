3. Варианты градиентного спуска 
21.Что такое Stochastic Gradient Descent (SGD)? 
SGD — вариант градиентного спуска, где параметры модели обновляются часто и 
с шумом. Изначально под SGD понимали обновление весов после каждого 
отдельного примера из набора данных. 
В реальной практике чаще используется mini-batch SGD — обновление весов 
происходит после обработки небольшого батча примеров (например, 32, 64, 
128). 
Например, если на вход подаётся батч с размерностью [64, 3, 32, 32] (64 
изображения), то модель сделает один forward pass по этому батчу, посчитает 
средний loss по всем 64 примерам, затем один backward pass и один шаг 
обновления весов. 
Такой подход даёт компромисс между точностью оценки градиента и скоростью 
обучения. 
22.Чем отличается Batch Gradient Descent от SGD? 
Batch Gradient Descent (BGD): 
• Обновляет веса после прохождения всего тренировочного набора (всех 
примеров) — одна итерация = вся эпоха. 
• Вычисляет точный градиент по всему датасету, что может быть медленно и 
дорого при больших данных. 
Stochastic Gradient Descent (SGD): 
• Обновляет веса после каждого отдельного примера (в классическом 
понимании). 
• Шумные, но быстрые обновления, которые помогают быстрее сходиться и 
выходить из локальных минимумов. 
23.Что такое Mini-Batch Gradient Descent? 
Mini-batch gradient descent — это способ обучения, когда данные разбиваются на 
небольшие подмножества — батчи (например, 32, 64, 128 примеров). 
Обновление весов происходит после обработки каждого такого батча. 
Это компромисс между: 
• Batch Gradient Descent — обновление после всего набора данных 
(медленно, но точно). 
• Stochastic Gradient Descent — обновление после каждого примера 
(шумно, но быстро). 
Mini-batch уменьшает шум в градиенте по сравнению с SGD, при этом быстрее, 
чем полный BGD. 
24.Какой плюс у Mini-Batch подхода? 
• Mini-batch позволяет экономить память по сравнению с Batch Gradient 
Descent, потому что не нужно загружать весь датасет целиком. 
• По сравнению с SGD, mini-batch даёт более стабильную и менее шумную 
оценку градиента, что улучшает сходимость. 
• Обновления параметров происходят чаще, чем в Batch GD (после каждого 
батча, а не всей эпохи), что позволяет быстрее видеть улучшения лосса. 
• Мини-батчи хорошо подходят для параллельных вычислений на GPU, что 
ускоряет обучение. 
25.Что такое Momentum? 
Momentum (моментум) — это техника оптимизации, которая помогает ускорить 
обучение и сгладить обновления весов при градиентном спуске. 
Идея Momentum 
• В классическом градиентном спуске обновления весов зависят только от 
текущего градиента. 
• Momentum добавляет инерцию — учитывает не только текущий градиент, 
но и предыдущие обновления. 
• Это похоже на движение шарика по холмистой поверхности: шарик 
набирает скорость и «пробивает» небольшие холмы и плато, вместо того 
чтобы застрять. 
Зачем нужен Momentum? 
• Ускоряет движение по направлениям, где градиенты согласованы 
(одинаковы по знаку). 
• Сглаживает колебания в «крутых» направлениях, помогая избежать 
дерганий. 
• Помогает преодолевать плато и локальные минимумы. 
• Обычно приводит к более быстрой и стабильной сходимости. 
Пример 
Если градиенты постоянно направлены в одну сторону, скорость vtv_tvt  растёт, и 
параметры движутся быстрее в этом направлении. Если градиенты меняют знак 
(колебания), то скорость уменьшается, и движение становится плавнее. 
26.Как Momentum помогает в оптимизации? 
Momentum помогает оптимизации за счёт: 
1. Ускорения в устойчивых направлениях — если градиенты несколько 
шагов подряд указывают в одну сторону, накопленный импульс разгоняет 
модель, сокращая время сходимости. 
2. Сглаживания колебаний — в узких долинах или при резких изменениях 
градиента метод уменьшает «дёрганье» весов. 
3. Выхода из плато и мелких локальных минимумов — накопленный 
импульс даёт возможность «перепрыгнуть» небольшие ямки. 
27.Что такое Nesterov Accelerated Gradient? 
Nesterov Accelerated Gradient (NAG) — это модификация Momentum, которая 
позволяет делать более «осознанные» шаги в оптимизации. 
Как работает обычный Momentum: 
1. Мы двигаемся в направлении накопленного импульса (среднего градиента 
за прошлые шаги). 
2. Этот импульс прибавляется к текущим весам, и после этого мы делаем шаг 
по градиенту. 
Как работает Nesterov: 
1. Сначала мы делаем предварительный шаг вперёд по направлению 
импульса — «заглядываем в будущее» (look ahead). 
2. Считаем градиент не в текущей точке, а в точке после этого 
предварительного шага. 
3. Обновляем веса с учётом этого уточнённого градиента. 
Плюс: 
NAG даёт более точную оценку, куда мы движемся, и позволяет скорректировать 
траекторию заранее. 
Это уменьшает риск проскочить минимум и ускоряет сходимость. 
28.Как работает RMSProp (Root Mean Square Propagation)? 
• Это метод адаптивного подбора шага обучения (learning rate) для каждого 
параметра. 
• В отличие от обычного градиентного спуска, RMSProp хранит скользящее 
среднее квадратов градиентов для каждого веса и делит текущий 
градиент на корень из этого среднего. 
• Таким образом, если по какому-то направлению градиенты часто большие 
— шаг по нему уменьшается, а если маленькие — шаг увеличивается. 
• Это помогает бороться с затухающими или взрывающимися 
градиентами и ускоряет сходимость. 
Плюсы: 
• Хорошо работает на нестационарных задачах (RNN, online-learning). 
• Автоматически подбирает эффективный размер шага для каждого 
параметра. 
Минусы: 
• Есть гиперпараметры (β, η), которые всё же надо настраивать. 
29.Что такое алгоритм Adam? 
Adam (Adaptive Moment Estimation) — это оптимизационный алгоритм, который 
сочетает идеи Momentum и RMSProp. 
Принцип работы: 
1. Сохраняет экспоненциальное скользящее среднее градиентов (первый 
момент) — это как Momentum, чтобы учитывать направление движения. 
2. Сохраняет экспоненциальное скользящее среднее квадратов 
градиентов (второй момент) — как RMSProp, чтобы адаптировать 
скорость обучения по каждому параметру. 
3. Корректирует смещения (bias correction), чтобы на первых шагах оценка 
моментов была более точной. 
Плюсы Adam: 
• Быстрая сходимость. 
• Хорошо работает с шумными градиентами. 
• Не требует тщательной подгонки learning rate (по умолчанию часто 
берут 1e-3). 
• Эффективен при больших данных и высокоразмерных параметрах. 
Минусы: 
• Иногда может застревать в плохих локальных минимумах. 
• Может переобучаться при слишком большом lr. 
30.Какие параметры есть у Adam, и что они означают? 
Параметры Adam: 
1. lr (learning rate) — шаг обучения. 
a. По умолчанию: 0.001 
b. Определяет, насколько сильно изменяются веса после каждого 
шага. 
2. beta1 — коэффициент для первого момента (среднее градиентов). 
a. По умолчанию: 0.9 
b. Чем ближе к 1, тем медленнее обновляется направление движения 
(Momentum-эффект). 
3. beta2 — коэффициент для второго момента (среднее квадратов 
градиентов). 
a. По умолчанию: 0.999 
b. Чем ближе к 1, тем сильнее сглаживаются изменения масштаба 
шага. 
4. eps (epsilon) — маленькое число для предотвращения деления на ноль. 
a. По умолчанию: 1e-8. 
5. weight_decay — L2-регуляризация (сокращает переобучение). 
a. По умолчанию: 0. 
6. amsgrad — вариант Adam, который хранит максимальное значение второго 
момента для улучшенной стабильности. 
a. По умолчанию: False. 
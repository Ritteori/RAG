4. Проблемы при оптимизации 
31.Что такое vanishing gradients? 
Что такое vanishing gradients? 
Vanishing gradients — это явление, при котором градиенты, распространяющиеся 
назад через слои нейросети, постепенно стремятся к нулю. В результате веса 
ранних слоев обновляются крайне медленно или вообще не меняются, что 
затрудняет обучение глубоких моделей. 
Почему это происходит? 
При обратном проходе градиенты умножаются на производные функций 
активации и веса слоев. Если эти значения меньше 1, градиенты уменьшаются 
экспоненциально с ростом глубины. 
Если на каком-то слое активация ReLU выдала нули (например, отрицательные 
входы), то при обратном проходе производная будет 0, и градиенты обнулятся для 
этих нейронов. 
Даже если активации положительные, при многократном умножении на веса 
меньше 1 градиент становится очень маленьким. 
Пример на практике 
В глубоких сетях (например, AlexNet) без residual connections градиенты с каждой 
итерацией умножаются на маленькие числа, постепенно уменьшаясь. В итоге 
первые слои обновляются медленно или совсем не обновляются. 
32.Что такое exploding gradients? 
Exploding gradients — это проблема, когда значения градиентов становятся 
чрезмерно большими во время обратного распространения ошибки. В результате 
обновления параметров получаются слишком большими, что делает обучение 
нестабильным и может привести к «вылету» из оптимума. 
Чаще всего эта проблема возникает в очень глубоких нейронных сетях или 
рекуррентных сетях, где градиенты многократно умножаются на веса, что 
вызывает экспоненциальный рост их величины. 
Для борьбы с exploding gradients применяют несколько методов. Один из самых 
эффективных — это градиентный клиппинг, при котором градиенты обрезаются по 
величине до заранее заданного максимума, что предотвращает слишком 
большие шаги обновления. Кроме того, помогают нормализация слоёв (например, 
Batch Normalization), правильная инициализация весов и использование более 
устойчивых архитектур, таких как LSTM в рекуррентных сетях или residual 
connections в глубоких сверточных сетях. 
33.Как можно бороться с исчезающими градиентами? 
Исчезающие градиенты — проблема, когда градиенты становятся очень малыми, 
и веса на ранних слоях почти не обновляются. Чтобы с ней справиться, применяют 
следующие методы: 
1. Использование менее «агрессивных» функций активации: 
a. Вместо сигмоидных или tanh часто применяют ReLU и его варианты 
(LeakyReLU, ELU), которые не имеют проблемы с затуханием 
градиента на положительной части. 
2. Skip connections / Residual connections: 
a. Добавление прямых путей («обходов») в архитектуре (например, в 
ResNet), которые позволяют градиенту обходить несколько слоев и 
уменьшать затухание. 
3. Нормализация слоёв (Batch Normalization): 
a. Помогает стабилизировать распределение входов в каждый слой, 
уменьшая эффект исчезающих градиентов. 
4. Инициализация весов: 
a. Правильный выбор метода инициализации (например, Xavier или He 
инициализация) помогает сохранить масштаб градиентов. 
5. Умеренная глубина модели: 
a. Для простых задач не стоит использовать чрезмерно глубокие 
модели. 
34.Что такое переобучение в контексте оптимизации? 
В контексте оптимизации переобучение возникает, когда процесс обучения 
слишком сильно минимизирует функцию потерь на тренировочных данных, 
стремясь достигнуть очень низких значений loss. Это может привести к тому, что 
модель начинает подстраиваться под шум и случайные особенности 
тренировочного набора, теряя способность хорошо обобщать на новых данных. 
То есть, с точки зрения оптимизации, переобучение — это ситуация, когда 
оптимизатор слишком эффективно снижает тренировочный лосс, но без учёта 
качества обобщения, что вызывает ухудшение результатов на тесте. 
35.Что такое регуляризация и как она помогает? 
Регуляризация — это набор методов, которые уменьшают переобучение 
(overfitting) путём ограничения сложности модели или введения дополнительных 
ограничений на веса. 
Она помогает оптимизации тем, что: 
• Не даёт модели чрезмерно подстраиваться под шум в данных. 
• Улучшает способность обобщать на новых данных. 
Примеры: 
• L1/L2-регуляризация (штраф за большие веса). 
• Dropout (отключение случайных нейронов во время обучения). 
• Data augmentation (искусственное расширение обучающей выборки). 
• Early stopping (остановка обучения, когда ошибка на валидации перестаёт 
снижаться). 
Интуиция: регуляризация как будто слегка «связывает руки» модели, заставляя её 
искать более простые и общие зависимости вместо запоминания частных 
случаев. 
36.Что делает L2-регуляризация? 
L2-регуляризация — это метод уменьшения переобучения, при котором в 
функцию потерь добавляется штраф за большие значения весов. 
Формула модифицированного лосса: 
Эффект: 
• L2 стремится сделать веса маленькими, но не обнуляет их полностью. 
• Это помогает модели быть менее чувствительной к шуму в данных и 
предотвращает переобучение. 
• Работает как «сглаживающий» фактор: чем больше вес, тем сильнее он 
штрафуется. 
37.Что делает L1-регуляризация? 
L1-регуляризация (манхэттенская) — метод регуляризации, при котором в 
функцию потерь добавляется сумма модулей весов модели, умноженная на 
коэффициент λ: 
Особенности и влияние: 
• Принцип работы: создаёт одинаковое давление (градиент = ±λ) на все 
веса, что приводит к их «подтягиванию» к нулю. 
• Разреженность: в отличие от L2, L1 может полностью занулять веса, тем 
самым отбрасывая неинформативные признаки и делая модель проще и 
быстрее. 
• Интерпретируемость: занулённые веса помогают понять, какие признаки 
действительно важны. 
• Недостаток: одинаковый градиент для больших и малых весов может быть 
проблемой на шумных данных, возможна потеря редких, но полезных 
признаков. 
• Геометрия: линии уровня функции штрафа L1 имеют форму ромба, что из
за острых углов часто приводит решение оптимизации к точкам на осях 
(нулевые веса). 
• Elastic Net: для смягчения недостатков часто комбинируют с L2
регуляризацией, получая Elastic Net: 
38.Что такое dropout и влияет ли он на оптимизацию? 
Dropout — это техника регуляризации, при которой на этапе обучения случайным 
образом «выключается» (зануляется) часть нейронов с вероятностью ppp (обычно 
0.2–0.5). 
Как это помогает: 
• Разрывает слишком сильные связи между нейронами (co-adaptation). 
• Заставляет каждый нейрон учиться полезным признакам независимо от 
других. 
• Модель становится более устойчивой к шуму и переобучению. 
Влияние на оптимизацию: 
• Dropout увеличивает шум в градиентах из-за случайного отключения 
нейронов, что в некотором смысле помогает модели не «застревать» в 
локальных минимумах. 
• Иногда это замедляет сходимость, но даёт лучшие обобщающие 
способности. 
• На инференсе dropout отключается, а выходы масштабируются, чтобы 
компенсировать отключение во время обучения. 
Плюсы: 
• Простая и эффективная регуляризация. 
• Уменьшение переобучения. 
• Улучшение обобщения модели. 
Минусы: 
• Замедление обучения. 
• Нужно подбирать вероятность ppp. 
• Не всегда полезен для всех архитектур и задач. 
39.Как влияет выбор функции активации на процесс оптимизации? 
Как функции активации помогают оптимизации 
• Преодоление нелинейности: Без активаций сеть — просто линейная 
комбинация, ограниченно полезная. Активации вводят нелинейность, 
позволяя моделям учить сложные зависимости. 
• Проблема затухающих градиентов: Sigmoid и Tanh насыщаются, 
градиенты становятся близкими к нулю. ReLU и её производные помогают 
избежать этой проблемы. 
• Мертвые нейроны: ReLU зануляет отрицательные значения, что иногда 
приводит к тому, что нейроны перестают обновляться. LeakyReLU, PReLU и 
ELU уменьшают эту проблему. 
• Гладкость функции: Swish и GELU имеют гладкие производные, создавая 
более мягкий и стабильный ландшафт функции потерь, что улучшает 
сходимость и обучение. 
• Выбор зависит от задачи: Например, для последнего слоя классификации 
лучше Softmax (мультикласс) или Sigmoid (бинарная). Для внутренних слоев 
часто ReLU или его улучшения. 
40.Почему важна нормализация признаков перед обучением? 
Нормализация признаков — это предварительная обработка данных, 
направленная на приведение значений входных признаков к единому масштабу и 
распределению. Это важный шаг, который помогает улучшить и стабилизировать 
обучение моделей, особенно глубоких нейронных сетей. 
Основные причины важности нормализации: 
• Ускоряет и стабилизирует обучение. Когда данные нормализованы, 
градиенты становятся более равномерными, что способствует более 
плавной и быстрой сходимости алгоритмов оптимизации (например, 
градиентного спуска). 
• Предотвращает проблемы с градиентами. Нормализация помогает 
избежать затухающих и взрывающихся градиентов, так как входные данные 
не имеют слишком больших или слишком маленьких значений. 
• Согласование с функцией активации. В задачах компьютерного зрения и 
генеративных моделях (например, при использовании tanh) часто 
нормализуют данные к диапазону, подходящему под выход функции 
активации (например, [-1,1]), чтобы модель лучше обучалась и выход 
соответствовал нужному формату. 
• Улучшает обобщающую способность. Правильная нормализация 
помогает модели лучше работать на новых данных, снижая риск 
переобучения. 
Дополнительные нюансы: 
1. Использование статистик тренировочного датасета. Параметры 
нормализации (среднее, стандартное отклонение) вычисляются на 
тренировочных данных и должны использоваться одинаково при обработке 
валидационных и тестовых данных, чтобы избежать утечки информации. 
2. Разница между нормализацией данных и нормализацией внутри сети. 
Нормализация признаков — это препроцессинг входных данных, тогда как 
внутри сети применяются методы, такие как BatchNorm, LayerNorm, 
которые дополнительно помогают стабилизировать обучение. 
3. Почему именно среднее 0 и дисперсия 1. Это снижает корреляцию между 
признаками и приводит к более равномерному распределению градиентов, 
что предотвращает доминирование отдельных признаков и ускоряет 
обучение. 
4. Влияние на оптимизаторы. Хотя некоторые оптимизаторы (например, 
Adam) более устойчивы к масштабу данных, нормализация всё равно 
улучшает стабильность и эффективность обучения. 
5. Роль в регуляризации. Нормализация данных может выступать как 
неявный метод регуляризации, снижая вероятность переобучения
6. Связь с ML и вероятностями(доделать) 
51.Как связаны кросс-энтропия и MLE? 
52.Почему log loss — это MLE для логистической регрессии? 
53.Как MLE относится к Байесовскому выводу? 
54.Что такое MAP-оценка и как она расширяет MLE? 
55.Что означает "обучение модели = оптимизация функции потерь"? 
56.Почему важно уметь выводить градиенты вручную? 
57.Как можно вычислить градиент логарифмической функции правдоподобия? 
58.Что значит "максимизировать логарифм правдоподобия = минимизировать 
loss"? 
59.Почему оптимизация — это ядро всего обучения в ML? 
60.В каких ещё моделях используется MLE кроме регрессии?
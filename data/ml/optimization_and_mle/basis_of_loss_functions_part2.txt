9. Чем MAE отличается от MSE по устойчивости к выбросам? 
MAE (Mean Absolute Error) менее чувствительна к выбросам, так как использует 
абсолютное значение ошибки, а не квадрат. Это значит, что большие ошибки не 
"наказываются" так сильно, как в MSE. Из-за этого MAE может быть более 
устойчивой при наличии шумных данных. Минус — MAE сложнее 
дифференцировать в точке 0, что может затруднять оптимизацию, и она менее 
чувствительна к большим ошибкам, что иногда нежелательно. 
10.Binary Cross-Entropy(BCE) 
Определение: 
Используется при бинарной классификации (или при многоклассовой 
классификации с one-vs-all). Сравнивает вероятность предсказания с истинной 
меткой (0 или 1). 
Плюсы: 
• Подходит для задач бинарной классификации и многоклассовой (в 
сочетании с сигмоидой / softmax). 
• Дифференцируема, хорошо работает с градиентным спуском. 
• Математически интерпретируема: минимизация BCE эквивалентна 
максимизации логарифма правдоподобия. 
Минусы: 
• Очень чувствительна к ошибкам на примерах, где предсказанная 
вероятность близка к 0 или 1. 
• При дисбалансе классов может быть неэффективна (см. Focal Loss). 
Нюансы: 
• Требует, чтобы выход модели был в диапазоне (0, 1), т.е. перед BCE 
обязательно должна быть сигмоида. 
• Если используешь BCEWithLogitsLoss в PyTorch — она сама включает 
сигмоиду внутри и работает с «сырыми» логитами. 
11. Cross-Entropy Loss (CEL) 
�
� Назначение: 
Применяется для оценки качества классификаторов, особенно при задаче 
мультиклассовой классификации. 
�
� Идея: 
Функция сравнивает два распределения: 
• истинное распределение метки (one-hot или вероятности), 
• предсказанное распределение вероятностей от модели (обычно после 
softmax). 
Если модель уверенно ошибается (предсказывает 0.99 для неверного класса), то 
Cross-Entropy Loss будет очень большой, "наказывая" за чрезмерную 
уверенность в неправильном ответе. 
�
� Формула: 
Для одного примера с C классами: 
Плюсы: 
• Отлично работает в мультиклассовой классификации 
• Учитывает уверенность модели — чем выше уверенность в ошибке, тем 
больше штраф 
• Совместима с softmax, что делает её естественным выбором для 
последнего слоя классификатора 
❌ Минусы: 
• Неустойчива к переобучению, если модель становится слишком 
уверенной 
• Плохо работает с несбалансированными классами (один класс чаще 
других) 
• Может давать бесконечный loss, если логарифм берётся от 0 → нужно 
использовать epsilon или log(softmax + 1e-7) 
�
� Частные случаи: 
• Binary Cross-Entropy (BCE) — применяется для бинарной классификации 
• Categorical Cross-Entropy — для мультиклассов 
• Sparse Categorical Cross-Entropy — аналог, где y — не one-hot, а просто 
индекс класса 
12. Focal Loss 
Focal Loss — это модификация стандартной Cross-Entropy Loss, разработанная 
специально для задач с сильным дисбалансом классов, например, когда один 
класс встречается значительно чаще других. 
�
� Формула: 
Для бинарной классификации: 
Идея: 
• Когда модель уверена в ответе (например, pt ≈1), то множитель (1−pt )γ≈0 → 
наказание становится маленьким. 
• Когда модель ошибается (например, pt ≈0.2), то (1−pt )γ≈0.8γ → наказание 
становится большим. 
• То есть Focal Loss смещает фокус на сложные для классификации 
примеры, игнорируя лёгкие. 
�
� Применение: 
• Задачи с дисбалансом классов, например: 
o Обнаружение объектов (например, редкие классы на фоне), 
o Медицинская диагностика (редкие заболевания), 
o Классификация с огромным перекосом выборки. 
Плюсы: 
• Отлично работает с дисбалансированными данными. 
• Смещает фокус на трудные примеры, снижая влияние лёгких. 
❌ Минусы: 
• Нужно тюнинговать параметры γ, α. 
• Сложнее в интерпретации, чем обычная Cross-Entropy. 
• Может замедлять обучение, так как "подавляет" лёгкие примеры. 
13. Dice Loss 
�
� Что это такое: 
Dice Loss (или Dice Coefficient Loss) — функция потерь, основанная на 
метрике схожести между двумя бинарными масками (предсказанием и ground 
truth). Она напрямую оптимизирует Dice коэффициент (индекс Жаккара), 
который измеряет степень перекрытия между предсказанной маской и истинной. 
Преимущества Dice Loss: 
1. Инвариантность к дисбалансу классов 
Даже если в выборке мало положительных пикселей (например, опухоль 
занимает 1% изображения), Dice всё равно может давать хороший сигнал 
об ошибке. 
2. Оптимизирует метрику, по которой часто оценивают качество 
В задачах сегментации IoU или Dice часто используются как финальные 
метрики — Dice Loss оптимизирует то, что реально важно. 
3. Чувствительность к форме и перекрытию 
Особенно важно в медицинских изображениях, где важна точная 
локализация объекта. 
❌ Недостатки: 
1. Плохо масштабируется на многоклассовые задачи напрямую 
Требуется отдельный Dice по каждому классу с усреднением. 
2. Не всегда стабильно обучается 
Особенно в начале обучения, когда пересечение может быть нулевым → 
очень низкий градиент. 
3. Не работает хорошо в задачах, где важна интенсивность ошибки, а не 
просто перекрытие 
Например, в регрессии или классификации — использовать не стоит. 
14. IoU Loss (Intersection over Union Loss) 
IoU Loss (также известный как Jaccard Loss) используется в задачах, где 
необходимо оценить качество пересечения двух областей: предсказанной и 
истинной. Это может быть маска, bounding box или другая пространственная 
форма. 
Идея: 
IoU измеряет, насколько хорошо предсказанная область совпадает с реальной: 
• Если они идеально совпадают → IoU = 1 → Loss = 0 
• Если они не пересекаются → IoU = 0 → Loss = 1 
Таким образом, минимизация IoU Loss напрямую увеличивает точность 
предсказанных масок или боксов. 
Применение: 
• Семантическая и instance-сегментация 
• Обнаружение объектов (bounding boxes) 
• Медицинская сегментация, где важно учитывать форму и площадь 
• Архитектуры типа U-Net, Mask R-CNN и др. 
Плюсы: 
• Лучше согласуется с метриками оценки, чем BCE или Dice. 
• Наказывает за неправильные формы, даже если площадь правильная. 
• Устойчив к дисбалансу фона и объекта. 
Минусы: 
• Градиенты могут быть нестабильны при отсутствии пересечения. 
• Может быть трудно оптимизировать (особенно в начале обучения). 
• Плохо работает, если предсказания пустые — IoU = 0. 
Примечание: 
• Можно расширить на мультиклассовую сегментацию, посчитав IoU для 
каждого класса. 
• Иногда используется Soft IoU — с непрерывными значениями (после 
сигмоиды), без жёсткого округления. 
15. Combo Loss (BCE + Dice) 
Combo Loss — это комбинированная функция потерь, которая объединяет Binary 
Cross-Entropy Loss и Dice Loss. 
Идея в том, что BCE хорошо работает как пиксельный классификатор, а Dice — 
как метрика совпадения областей. Вместе они обеспечивают устойчивое и 
точное обучение. 
Идея: 
• BCE даёт локальную оценку ошибки для каждого пикселя. 
• Dice Loss учитывает глобальное совпадение формы. 
• Вместе они уменьшают недостатки друг друга. 
Применение: 
• Медицинская сегментация (особенно при дисбалансе фона/объекта). 
• Сегментация мелких объектов. 
• Архитектуры типа U-Net, DeepLab, SegNet. 
Плюсы: 
• Более стабильное обучение, чем у чистого Dice Loss. 
• Сохраняет способность Dice работать с дисбалансом. 
• Гибко настраивается через α. 
Минусы: 
• Нужно подбирать α\alphaα под задачу. 
• Увеличивает время вычислений (считаем два лосса). 
Примечание: 
• Иногда берут веса α=0.7 или α=0.8, чтобы BCE доминировала. 
• Можно расширить для мультиклассовой сегментации, посчитав BCE и Dice 
для каждого класса. 
5. Метод максимального правдоподобия (MLE) 
41.Что такое функция правдоподобия? 
Определение: 
Функция правдоподобия — это функция параметров модели, которая 
показывает, насколько вероятны наблюдаемые данные при этих параметрах. 
• Пусть у нас есть данные X={x1 ,x2 ,...,xn } и вероятностная модель с 
параметрами θ. 
• Вероятность наблюдать данные при фиксированных параметрах: P(X∣θ). 
• Функция правдоподобия записывается как: 
То есть теперь мы считаем X фиксированными данными, а переменными 
являются параметры θ. 
Для независимых наблюдений: 
Если данные независимы, общая вероятность наблюдать всю выборку — это 
произведение вероятностей каждого наблюдения: 
Примеры: 
1. Нормальное распределение (N(μ,σ2)) 
2. Биномиальное распределение (например, монетка) 
• Монета подбрасывается n раз, количество орлов = k, вероятность орла = p 
Особенности функции правдоподобия: 
1. Она не нормализована как обычная вероятность по θ — нельзя сказать 
«вероятность параметров = L(θ)», она показывает правдоподобие данных 
при разных параметрах. 
2. Используется для оценки параметров: 
a. MLE: выбираем θ^, которое максимизирует L(θ∣X). 
3. Логарифмическая функция правдоподобия: 
a. Берут логарифм для удобства вычислений и дифференцирования: 
Идея на пальцах: 
• Представь, что твой датасет — это фиксированные наблюдения. 
• Функция правдоподобия говорит: «если бы параметры были такими, как 
здесь, насколько вероятно было бы увидеть эти данные?» 
• MLE ищет параметры, которые делают наблюдаемые данные 
максимально вероятными. 
42.В чём разница между вероятностью и правдоподобием? 
1. Вероятность (Probability): 
a. Рассматриваем параметры модели фиксированными, а данные — 
случайной величиной. 
b. Формально: P(X∣θ). 
c. Пример: вероятность выпадения 6 орлов в 10 бросках монеты при 
p=0.5. 
2. Правдоподобие (Likelihood): 
a. Рассматриваем данные фиксированными, а параметры модели — 
переменными. 
b. Функция правдоподобия L(θ∣X)=P(X∣θ). 
c. Используется для оценки параметров: выбираем такие θ, при 
которых наблюдаемые данные наиболее вероятны. 
d. Пример: по 6 орлам в 10 бросках оцениваем вероятность p, которая 
сделала бы такие данные максимально вероятными  MLE. 
Идея на пальцах: 
• Вероятность: «при известных параметрах, насколько вероятны данные?» 
• Правдоподобие: «при данных, насколько правдоподобны разные 
параметры?» 
43.Что такое логарифмическая функция правдоподобия? 
Определение: 
• Логарифмическая функция правдоподобия — это логарифм функции 
правдоподобия: 
• Мы берём логарифм не ради теории, а для удобства вычислений и 
оптимизации. 
Почему берём логарифм: 
1. Превращаем произведения в суммы: 
• Для независимых наблюдений функция правдоподобия — это 
произведение вероятностей: 
• Логарифм превращает это в сумму: 
• Это намного удобнее для дифференцирования и численной оптимизации. 
2. Стабильность вычислений: 
• Произведение очень маленьких вероятностей может быть численно близко 
к нулю → потеря точности. 
• Логарифм предотвращает переполнение или «зануление». 
3. Сохраняется максимум: 
Идея на пальцах: 
• Логарифмическая функция правдоподобия — это просто удобная форма 
функции правдоподобия, которая облегчает математическую работу с 
произведениями вероятностей и поиском максимума. 
44.Почему берут логарифм функции правдоподобия? 
Причины: 
1. Стабильность вычислений: 
a. Произведение вероятностей для большого числа наблюдений может 
быть очень маленьким, близким к нулю → потеря точности. 
b. Логарифм превращает их в более удобные числа. 
2. Упрощение вычислений: 
a. Произведение превращается в сумму, что удобно для 
дифференцирования: 
3. Смысл остается тем же: 
• Максимум функции правдоподобия L(θ) совпадает с максимумом 
логарифма ℓ(θ): 
4. Гибкость: 
a. В принципе можно использовать другие монотонные 
преобразования, но логарифм — стандартный и удобный выбор. 
45.Как найти максимум логарифма правдоподобия? 
Максимум логарифма правдоподобия ℓ(θ∣X) ищут так: 
1. Записываем логарифм функции правдоподобия 
Здесь θ — параметры модели (например, для нормального распределения это μ и 
σ). 
2. Вычисляем частные производные по каждому параметру 
3. Приравниваем производные к нулю (условие экстремума) 
Решаем полученную систему уравнений. 
4. Проверяем, что это максимум 
a. Для этого смотрим на знак второй производной (или на форму 
функции). 
5. Если аналитическое решение невозможно 
a. Используем численные методы оптимизации: 
i. Градиентный спуск (Gradient Descent) 
ii. Метод Ньютона (Newton-Raphson) 
iii. L-BFGS и др. 
Пример для нормального распределения: 
Кратко: 
• Для простых случаев — берём производную, приравниваем к нулю, 
решаем. 
• Для сложных — идём к максимуму с помощью численных методов. 
46.Пример: как выглядит логарифмическая функция правдоподобия 
для нормального распределения? 
Пусть у нас есть выборка X={x1 ,x2 ,…,xn }, и мы предполагаем, что она 
распределена нормально с параметрами: 
• μ — среднее значение (мат. ожидание) 
• σ — стандартное отклонение 
Функция плотности вероятности (PDF) 
Для одного наблюдения xi  нормальное распределение: 
Функция правдоподобия (Likelihood) 
Так как наблюдения независимы: 
Логарифм правдоподобия (Log-Likelihood) 
Берём логарифм, чтобы превратить произведение в сумму: 
•  
47.Как используется MLE в линейной регрессии? 
• Идея: 
В линейной регрессии мы предполагаем, что наблюдаемые значения yi  
связаны с признаками xi  через линейную модель: 
где εi  — случайная ошибка. 
• Предположение о распределении ошибок: 
Мы предполагаем, что ошибки εi  независимы и нормально распределены: 
• Функция правдоподобия: 
Вероятность получить все наблюдения y1 ,…,yn  при данных xi  и 
параметрах w равна: 
• Максимизация правдоподобия: 
Берём логарифм функции правдоподобия (лог-правдоподобие) и 
максимизируем по w: 
Максимизация logL(w) по w эквивалентна минимизации суммы квадратов ошибок: 
• Вывод: 
• MLE позволяет получить оценки параметров линейной регрессии. 
• Минимизация MSE — это частный случай MLE при предположении 
нормальности ошибок. 
• Таким образом, MSE и MLE тесно связаны: MSE — это просто конкретная 
форма правдоподобия для нормальных ошибок. 
48.Как используется MLE в логистической регрессии? 
• Модель: 
Логистическая регрессия моделирует вероятность принадлежности 
объекта к классу 1: 
где σ — сигмоидальная функция. 
• Предположение о распределении: 
Так как Y∈{0,1}, мы предполагаем, что Y при фиксированном X подчиняется 
бернуллиевскому распределению: 
• Функция правдоподобия: 
Для наблюдений (xi ,yi ) правдоподобие: 
• Лог-правдоподобие: 
• Максимизация по w: 
• Максимизация logL(w) эквивалентна минимизации бинарной кросс
энтропии (BCE): 
• Именно поэтому в PyTorch при логистической регрессии используют 
BCEWithLogitsLoss — это реализация MLE для бернуллиевских данных. 
49.Почему логистическая регрессия не имеет аналитического 
решения? 
1. Форма функции правдоподобия 
Для логистической регрессии лог-правдоподобие: 
2. Почему нет аналитического решения 
a. Для нахождения MLE нужно решить уравнение: 
b. Производная даёт выражение с сигмоидой внутри логарифмов, что 
приводит к системе нелинейных уравнений. 
c. Эти уравнения нельзя решить алгебраически (нет закрытой формулы 
через обычные операции над числами). 
3. Что делают на практике 
a. Используют численные методы оптимизации, например: 
градиентный спуск, Newton-Raphson, L-BFGS. 
b. Поэтому в PyTorch и других библиотеках BCE/BCEWithLogitsLoss 
минимизируют через итеративный градиентный спуск.
6. AE 
1. Мотивация использования AE 
1. Основная идея: 
Autoencoder — это нейросеть, которая учится кодировать данные в 
компактное представление (latent vector) и затем восстанавливать 
исходные данные из этого представления. 
Иными словами, она учится сжатию без потери ключевой 
информации. 
�
� Почему это нужно (мотивация): 
1. Обучение представлений (representation learning): 
AE позволяет без меток (unsupervised) научиться осмысленным 
признакам, которые можно потом использовать в других 
задачах — классификации, кластеризации, anomaly detection. 
Пример: обученный AE на изображениях может выделить латентные 
признаки типа «форма», «текстура», «цвет». 
2. Снижение размерности (nonlinear PCA): 
AE можно рассматривать как нелинейное обобщение PCA — он 
способен захватывать сложные, нелинейные зависимости в 
данных. 
В отличие от PCA, который делает линейную проекцию, AE способен 
аппроксимировать любое преобразование через нелинейные слои 
(ReLU, Conv и т.д.). 
3. Удаление шума и восстановление данных: 
Вариант Denoising AE учится игнорировать шум во входных 
данных и восстанавливать «чистую» версию. 
Пример: очистка изображений, сигналов, текстов с ошибками. 
4. Аномалия детекция: 
Если AE обучен восстанавливать «нормальные» данные, то 
аномальные образцы (на которых сеть не обучалась) будут 
восстанавливаться с высокой ошибкой. 
→ Reconstruction error становится метрикой аномальности. 
5. Предобучение (pretraining): 
AE раньше часто использовали для предобучения весов 
encoder’а, особенно до эпохи больших датасетов и мощных 
GPU. 
→ Encoder потом использовался как feature extractor для других 
моделей. 
6. Генерация признаков / эмбеддингов: 
Латентное представление (z) можно считать эмбеддингом 
данных, который часто: 
a. более информативен; 
b. имеет меньшую размерность; 
c. устойчив к шуму. 
7. Сжатие данных: 
AE можно использовать как нейросетевой метод сжатия, 
особенно для изображений или временных рядов, где обычные 
кодеки не оптимальны под конкретный тип данных. 
�
� Формулировка в терминах оптимизации 
AE минимизирует разность между входом и выходом: 
2. Архитектура и основные компоненты (encoder, decoder) 
⚙
️ Архитектура и основные компоненты (Encoder, Decoder) 
1
️
⃣ Общая структура 
Autoencoder состоит из двух симметричных частей: 
• ( x ) — входные данные (например, изображение, сигнал, текст). 
• ( z ) — латентное (скрытое) представление, сжатая форма 
данных. 
• ( x^) — реконструкция (восстановленная версия входа). 
• (fθ ,gϕ   ) — параметризованные функции (обычно нейросети). 
Цель обучения: 
где ( L ) — функция потерь (например, MSE, BCE). 
2
️
⃣ Encoder (кодировщик) 
Задача: 
Преобразовать вход ( x  ∈  R^n ) в более компактное скрытое 
представление ( z  ∈  R^M ), где ( m < n ). 
Типовая структура: 
• Для изображений — Conv + BatchNorm + ReLU блоки + Flatten + 
Linear. 
• Для табличных данных — MLP (несколько Linear + ReLU). 
• Для последовательностей — LSTM / GRU (sequence → embedding). 
Математически: 
z=fθ (x)=σ(Wx+b) 
или более сложная композиция нелинейных преобразований. 
Интуиция: 
Encoder сжимает данные, «выбрасывая» несущественные детали и 
сохраняя наиболее информативные признаки. 
3
️
⃣ Decoder (декодировщик) 
Задача: 
Научиться из сжатого кода ( z ) восстанавливать исходные данные 
(x^), максимально приближённые к ( x ). 
Типовая структура: 
• Для изображений — ConvTranspose (или Upsample + Conv). 
• Для табличных — MLP, обратный по размерностям encoder’у. 
• Для последовательностей — LSTM/GRU, декодирующие из 
эмбеддинга. 
Математически: 
x^=gϕ (z)=σ(W′z+b′) 
или аналогичная последовательность нелинейностей. 
4
️
⃣ Latent space (средняя точка) 
Латентное пространство ( z ) — это «бутылочное горлышко» 
(bottleneck): 
Input → encode → [z] → decode → output 
• Оно заставляет сеть выделять только существенные 
признаки, необходимые для восстановления. 
• Размерность ( z ) определяет баланс: 
o слишком маленькая — недовосстановление (underfitting); 
o слишком большая — переобучение (identity mapping). 
5
️
⃣ Инженерные аспекты 
• Симметричность: decoder часто зеркалирует encoder по слоям 
(например, Conv → ConvTranspose). 
• Нормализация: BatchNorm / LayerNorm помогают 
стабилизировать обучение. 
• Активации: ReLU в encoder, Sigmoid/Tanh в выходе decoder 
(чтобы ограничить диапазон [0,1] или [-1,1]). 
• Bottle-neck placement: latent vector обычно — перед последним 
flatten или linear-слоем encoder’а. 
6
️
⃣ Концептуальная суть: 
Encoder — выделяет суть данных. 
Decoder — учится восстанавливать их из этой сути. 
Архитектура AE — это обучаемое сжатие информации, где bottleneck 
играет роль регуляризатора, заставляющего сеть находить 
компактные и обобщающие признаки. 
Хочешь, следующей разберём «Латентное пространство и его роль» 
— там важно понять, как AE формирует embedding, почему оно 
неструктурированное, и чем это потом отличается от VAE. 
3. Латентное пространство и его роль 
1. Определение: 
Латентное пространство (latent space) — это сжатое представление 
данных z=fθ (x), формируемое encoder’ом в bottleneck. 
Оно содержит только те признаки, которые нужны для 
восстановления входа через decoder. 
2. Роль: 
• Сжатие информации: уменьшает размерность, отбрасывая 
лишнее. 
• Выделение признаков: служит вектором эмбеддингов, который 
можно использовать для downstream задач (классификация, 
кластеризация и т.д.). 
• Регуляризация: ограниченная размерность заставляет сеть 
фокусироваться на наиболее информативных паттернах. 
3. Особенности: 
• В обычном AE распределение latent-векторов не 
контролируется — оно не обязательно нормальное, не 
обязательно непрерывное. 
• Из-за этого AE не может уверенно сэмплировать новые точки 
— он знает только конкретные области, где были данные 
обучения. 
• Размерность latent space критически влияет на баланс между 
восстановлением и обобщением. 
4. Практически: 
• Хорошее latent space → компактно, информативно, устойчиво к 
шуму. 
• Плохое latent space → переобучение (просто копирует вход), или 
теряет важные признаки. 
4. Функция потерь (reconstruction loss) 
1
️
⃣ Суть 
Цель AE — научиться восстанавливать входной пример xxx через его 
реконструкцию x^: 
L=Loss(x,x^)  
где 
x^=gϕ (fθ (x))  
Эта функция потерь измеряет насколько близко восстановленные 
данные к оригиналу, то есть качество реконструкции. 
2
️
⃣ Основные типы reconstruction loss 
�
� MSE (Mean Squared Error) 
LMSE =1/N * ∑i(xi −x^i )**2 
Используется для небинарных, непрерывных данных (например, 
нормализованных изображений, сигналов, признаков). 
Интерпретация: AE минимизирует евклидово расстояние между 
входом и выходом. 
→ Работает как приближение к максимальному правдоподобию при 
предположении, что шум на выходе ~ N(0, σ²). 
Плюсы: простая, стабильная, интерпретируемая. 
Минусы: чувствительна к масштабу, не учитывает восприятие. 
�
� BCE (Binary Cross-Entropy) 
Используется для бинарных данных (например, пиксели в диапазоне 
[0,1], one-hot признаки). 
Интерпретация: оценивает вероятность восстановления каждого 
бита/пикселя. 
→ Эквивалентно максимизации правдоподобия при Bernoulli
допущении для данных. 
Важно: выход decoder’а должен быть ограничен в [0,1] → обычно через 
Sigmoid. 
�
� L1 Loss (MAE) 
Менее чувствителен к выбросам, чем MSE, и даёт более чёткие 
визуальные результаты (особенно при изображениях). 
→ Часто комбинируют с MSE: 
L=λ1 LMSE +λ2 LMAE   
3
️
⃣ Почему reconstruction loss — не просто “ошибка” 
• Он определяет какие признаки модель считает важными: 
Если loss измеряется на уровне пикселей → AE фокусируется на 
локальных деталях. 
Если loss задан в feature-пространстве (например, с помощью 
feature extractor из CNN) → AE учится перцептивно релевантным 
признакам. 
Это называется Perceptual Loss (часто применяют в image AE / 
GAN). 
4
️
⃣ Обобщённая формулировка 
где d(⋅,⋅) — любая метрика сходства (MSE, BCE, Cosine, Perceptual и 
т.д.). 
5
️
⃣ Практические аспекты 
• Нормировка данных: диапазон [0,1] или [-1,1] должен 
соответствовать последней активации decoder’а (Sigmoid или 
Tanh). 
• Loss зависит от задачи: 
o для изображений — MSE/BCE/Perceptual, 
o для текстов — cross-entropy по токенам, 
o для звука — L1 или спектральный loss. 
• Overfitting признак: если reconstruction loss → 0, но латентное 
пространство бессмысленное → AE просто “скопировал вход”. 

5. Ограничения и проблемы (копирование входа, переобучение) 
1
️
⃣ Копирование входа (Trivial identity mapping) 
Проблема: 
AE может просто научиться копировать вход, не извлекая никаких 
полезных признаков. 
То есть вместо того, чтобы сжать информацию в осмысленное 
латентное пространство, сеть просто запоминает пиксели. 
Почему так происходит: 
• Если размер латентного вектора zzz слишком большой (близок 
к размеру входа), 
модель не вынуждена терять лишнюю информацию — она 
просто «передаёт» всё напрямую. 
• Если архитектура слишком мощная (большая ёмкость сети), 
она способна выучить тождественное преобразование x^=x. 
Следствие: 
Такой AE не учится представлению данных, а просто «зеркалит» 
вход — бесполезно для задач вроде кластеризации, генерации, 
сжатия или аномалий. 
Как решается: 
• Ограничение размерности латентного пространства (bottleneck). 
• Dropout, weight decay, sparsity. 
• Добавление шума (→ Denoising AE). 
• Регуляризация архитектуры (например, Sparse AE, Contractive 
AE). 
2
️
⃣ Переобучение (Overfitting) 
Проблема: 
AE может слишком хорошо подстроиться под обучающую выборку, 
теряя способность обобщать на новых данных. 
Причины: 
• Недостаток разнообразных данных. 
• Слишком высокая мощность модели. 
• Отсутствие регуляризации. 
Признаки: 
• Низкий reconstruction loss на train. 
• Высокий reconstruction loss на test. 
Решения: 
• Dropout / L2 регуляризация. 
• Data augmentation. 
• Early stopping. 
• Denoising AE (принудительно делает модель устойчивее к шуму). 
• Использование вариационных AE (VAE) с вероятностной 
регуляризацией. 
3
️
⃣ Плохая интерпретируемость латентного пространства 
AE без регуляризации часто формирует хаотичное, несвязное 
пространство признаков, где соседние точки не соответствуют 
похожим объектам. 
(Эту проблему решает VAE, вводя вероятностное распределение в 
латентном пространстве.) 
4
️
⃣ Ограниченная способность к генерации 
Обычный AE не умеет генерировать новые данные — он просто 
восстанавливает то, что видел. 
Чтобы порождать осмысленные новые примеры, нужно иметь хорошо 
структурированное латентное пространство → отсюда возникает 
мотивация к VAE и GAN. 
6. Отличие AE от PCA 
1
️
⃣ Общая цель 
И PCA, и AE решают похожую задачу — найти компактное 
представление данных, чтобы можно было восстановить оригинал. 
Но делают они это по-разному: 
Свойст
во 
Тип 
Форма 
отобра
жения 
PCA 
Линейный метод 
Ортогональное линейное 
преобразование 
Задача Найти подпространство 
Обрати
мость 
максимальной дисперсии 
Простая (через 
транспонирование матрицы 
весов) 
Autoencoder 
Нелинейный метод 
Нелинейное отображение через 
нейросеть 
Научиться восстанавливать вход через 
узкое латентное представление 
Обратный путь учится самостоятельно 
Интерп
ретиру
емость 
Компоненты ортогональны и 
упорядочены по вкладу в 
дисперсию 
Признаки в латентном пространстве не 
обязаны быть ортогональны или 
интерпретируемы 
2
️
⃣ Формулировка с точки зрения математики 
PCA: 
Где W — матрица главных компонент. 
PCA ищет линейное подпространство, которое минимизирует 
ошибку восстановления. 
Autoencoder: 
Где Eθ  — encoder, Dϕ  — decoder, и они могут быть нелинейными 
функциями (MLP, CNN). 
AE не требует ортогональности и может аппроксимировать 
нелинейные многообразия. 
3
️
⃣ Линейный AE ≈ PCA 
Если: 
• Encoder и decoder состоят из одного линейного слоя без 
активаций, 
• Функция потерь — MSE, 
тогда Autoencoder эквивалентен PCA — он найдёт то же 
подпространство главных компонент. 
4
️
⃣ Ключевое преимущество AE 
AE способен аппроксимировать нелинейные зависимости, 
в то время как PCA ограничен линейными. 
Поэтому AE лучше работает, когда данные лежат на нелинейных 
многообразиях (например, изображения, где пиксели 
коррелированы по форме и цвету). 
5
️
⃣ Недостатки AE относительно PCA 
• Требует настройки гиперпараметров, оптимизации и большого 
количества данных. 
• Может переобучаться. 
• Нельзя гарантировать ортогональность и интерпретируемость 
компонент. 
• Обучение занимает больше времени. 
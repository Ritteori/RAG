7. Почему AE не является генеративной моделью 
1
️
⃣ Что значит “генеративная модель” 
Генеративная модель должна уметь порождать новые данные, 
которые выглядят как реальные, из некоего распределения в 
латентном пространстве. 
То есть она моделирует распределение данных p(x) 
или, как минимум, условное распределение p(x∣z), 
где z — это скрытая переменная (латентный код). 
Пример: 
GAN, VAE, Diffusion моделируют p(x) и могут сэмплировать новые 
объекты. 
2
️
⃣ Что делает Autoencoder на самом деле 
Обычный AE не моделирует распределение данных. 
Он просто учится функции восстановления: 
Где: 
• E(x) — сжатие входа до латентного вектора z; 
• D(z) — восстановление обратно в x; 
• Лосс: L=∥x−x^∥2. 
То есть AE решает задачу минимизации reconstruction error, 
а не задачу моделирования вероятности данных. 
3
️
⃣ Проблема с латентным пространством 
Латентное пространство AE неструктурировано: 
• Нет явного распределения p(z); 
• Кодировка точек из train набора может быть разреженной и 
хаотичной; 
• Между “обученными” латентными точками — пустые зоны, 
где decoder не знает, что генерировать. 
Поэтому если просто взять случайный z (например, из Normal(0, 1)) 
и подать его в decoder: 
на выходе получится мусор, потому что decoder никогда не видел 
таких zzz. 
4
️
⃣ Генеративная способность появляется только при 
регуляризации латентного пространства 
VAE добавляет: 
• распределение q(z∣x) и аппроксимацию p(z), 
• KL-дивергенцию, чтобы сделать z похожим на N(0,I), 
• sampling из этого распределения. 
Именно поэтому VAE = AE + вероятностная структура латентного 
пространства, 
что позволяет сэмплировать осмысленные новые примеры. 
8. Типы AE (denoising, sparse, contractive) 
1
️
⃣ Denoising Autoencoder (DAE) 
Идея: обучается восстанавливать “чистый” вход из зашумлённого, 
чтобы модель научилась извлекать устойчивые, информативные 
признаки, а не просто запоминать пиксели. 
• На вход подаётся x~=x+noise 
• Цель: D(E(x~))≈x 
• Потеря: L=∥x−x^∥2 
Зачем нужно: 
• Повышает устойчивость модели к шуму. 
• Помогает бороться с переобучением. 
• Используется как unsupervised pretraining (например, в NLP или 
speech). 
2
️
⃣ Sparse Autoencoder (SAE) 
Идея: принудительно делает некоторые нейроны латентного слоя 
“неактивными”, 
то есть заставляет модель кодировать информацию в небольшой 
части признаков. 
• Добавляется sparsity penalty: 
где: 
o ρj ^  — средняя активация j-го нейрона, 
o ρ — желаемая средняя активность (например, 0.05). 
Зачем нужно: 
• Получаем более интерпретируемые признаки. 
• Модель учится разделять важные факторы (разреженные 
репрезентации полезны для кластеризации и feature extraction). 
3
️
⃣ Contractive Autoencoder (CAE) 
Идея: сделать представления инвариантными к малым изменениям 
входа. 
То есть если немного изменить x, латентный вектор z=E(x) должен 
меняться минимально. 
• Добавляется регуляризация на якобиан: 
(штраф за сильную чувствительность латентного представления к 
изменению входа) 
Зачем нужно: 
• Модель извлекает гладкие и устойчивые признаки, 
• Лучше работает на данных с шумом и аугментациями, 
• Латентное пространство становится более структурированным 
(идея, близкая к VAE). 

9. Основные применения AE (аномалии, pretraining, снижение 
размерности) 
�
� 1. Обнаружение аномалий (Anomaly Detection) 
• Суть: AE учится восстанавливать типичные (нормальные) 
примеры, потому что видит их на обучении. 
• Если потом на вход подать аномальный пример, его 
reconstruction error (ошибка восстановления) будет высокой → 
это сигнал, что пример выбивается из распределения. 
• Примеры: 
o Мошеннические транзакции в финансах. 
o Дефекты на производстве (изображения изделий). 
o Медицинские данные (аномальные ЭКГ, КТ, МРТ). 
• Почему это работает: AE не видел "аномалий" и не может их 
воспроизвести → высокий MSE или BCE. 
�
� 2. Снижение размерности (Dimensionality Reduction) 
• Аналог PCA, но нелинейный. 
• Encoder → учит нелинейное отображение X→Z (latent space), где Z 
— сжатое представление. 
• Decoder → восстанавливает исходное. 
• Можно использовать latent-вектор z вместо исходных 
признаков: 
o для кластеризации, 
o визуализации (t-SNE, UMAP), 
o последующих моделей (например, классификаторов). 
• Преимущество перед PCA: AE может моделировать сложные, 
нелинейные зависимости. 
�
� 3. Pretraining (предобучение) 
• AE можно использовать, чтобы предобучить Encoder — он 
научится извлекать хорошие, информативные фичи. 
• Потом Encoder можно использовать: 
o в классификаторе (добавить Linear слой и дообучить 
supervised), 
o в других моделях (например, CNN для изображений). 
• Это особенно полезно, когда мало размеченных данных, но 
много неразмеченных (self-supervised обучение). 
�
� 4. Сжатие данных (Compression) 
• Encoder сжимает изображение в латентный вектор → 
хранить/передавать можно меньше данных. 
• Decoder восстанавливает картинку. 
• Это может использоваться в: 
o видео-компрессии, 
o системах передачи данных, 
o embedded устройствах. 
�
� 5. Шумоподавление (Denoising AE) 
• Отдельный тип, но и применение важное. 
• AE учится восстанавливать чистый сигнал из зашумлённого 
входа. 
• Пример: 
o вводим картинку с шумом, 
o на выходе — чистое изображение. 
• Аналог фильтра, но обучаемый и более гибкий.

10. Как оценивать качество восстановления 
Метрики качества реконструкции (вне обучения) 
Чтобы понять качество “на глаз” и количественно, используют 
дополнительные метрики: 
Метрика 
PSNR (Peak Signal-to-Noise 
Ratio) 
SSIM (Structural Similarity 
Index) 
LPIPS (Learned Perceptual 
Image Patch Similarity) 
Что измеряет 
Отношение мощности 
сигнала к шуму 
восстановления 
Сравнение по структуре, 
контрасту и яркости 
Перцептуальное качество 
(на фичах предобученной 
сети) 
Формула / 
Интерпретация 
чем выше, тем лучше; 
30–40 dB — хорошее 
восстановление 
0 – 1, ближе к 1 → 
лучше 
чем ниже, тем лучше, 
учитывает восприятие 
человека
11. Backpropagation и особенности градиентов в ViT 
Обратное распространение в ViT работает как в обычных нейросетях: 
градиенты проходят от loss через все слои. Каждый токен (bs, n+1, 
hidden_dim) участвует в расчёте градиента, включая [CLS] для 
классификации. 
Self-Attention делает градиенты глобальными: каждый патч влияет на 
все остальные через веса внимания (attention_weight_ij * 
value_j), поэтому один токен получает градиенты от всех остальных. 
Это отличается от CNN, где градиенты распространяются локально 
через receptive field. 
Residual connections (x = x + f(x)) дают прямой путь для 
градиентов, предотвращая их исчезновение при глубине L > 10, а 
LayerNorm стабилизирует масштаб градиентов по признакам, делая 
обучение более устойчивым. 
MLP-блок обрабатывает каждый токен отдельно (bs, hidden_dim), 
но residual позволяет градиенту “обойти” MLP, если он слишком 
сильно меняет сигнал. 
Главные отличия от CNN: глобальная область восприятия через 
attention, плотные связи между токенами, чувствительность к 
масштабу градиентов. Без LayerNorm и residual градиенты легко могут 
взорваться или затухнуть. 
На практике важно следить за инициализацией весов и масштабами, 
иначе ViT может быть нестабильным даже на относительно небольшой 
глубине. 
12. Регуляризация и устойчивость ViT 
ViT очень мощный, но чувствителен к переобучению из-за отсутствия 
индуктивных свойств CNN (нет локальных свёрток, нет иерархии). Для 
стабильного обучения и лучшей генерализации используют несколько 
техник. 
Dropout применяется в MLP и attention, чтобы случайно “отключать” 
части признаков и предотвращать зависимость от отдельных 
нейронов. 
Stochastic Depth / DropPath — случайное пропускание целых блоков 
энкодера на этапе тренировки, что помогает учить более устойчивые 
представления и облегчает градиентный поток. 
Data augmentation критически важна: MixUp, CutMix, RandAugment 
позволяют модели видеть больше вариаций изображений, 
компенсируя отсутствие свёрточной индукции. 
Weight decay / L2 регуляризация помогает контролировать величину 
весов и предотвращает переобучение, особенно в MLP и линейных 
слоях attention. 
Pre-LayerNorm + Residual сами по себе дают устойчивость 
градиентов, предотвращая взрыв или затухание при большой глубине. 
Для задач сегментации/детекции часто используют трансформерные 
декодеры с skip-connections или линейные проекции патчей, чтобы 
стабилизировать обучение на высоких разрешениях. 
В сумме, комбинация этих техник делает ViT достаточно устойчивым, 
но он всё ещё чувствителен к размеру датасета и качеству 
аугментаций, поэтому на малых данных часто уступает CNN. 
13. Оптимизация ViT 
ViT обучается так же, как обычные трансформеры, но есть 
особенности, связанные с вниманием и глобальными зависимостями 
между патчами. 
Оптимизаторы: чаще используют AdamW, так как он сочетает 
адаптивные шаги и decoupled weight decay — это особенно важно для 
MLP и линейных слоёв attention. SGD здесь обычно хуже, из-за 
глобальных связей и чувствительности к масштабу градиентов. 
Learning rate schedule: часто применяют cosine decay или warmup. 
Warmup нужен, чтобы градиенты не взорвались на старте, особенно 
при глубоком стеке энкодеров. 
Weight decay: регулирует величину весов и предотвращает 
переобучение. Важно особенно для MLP и линейных слоёв attention. 
Gradient clipping: иногда используют, если batch небольшой или 
learning rate велик — это предотвращает взрыв градиентов в attention. 
Mixed Precision / AMP: ускоряет обучение и уменьшает память, но 
нужно следить за стабильностью градиентов (иногда требует small eps 
в LayerNorm). 
Batch size: ViT чувствителен к размеру батча — маленький batch может 
ухудшить обучение из-за шумных градиентов. Используют 
accumulation или large batch с корректной нормализацией. 
В сумме оптимизация ViT требует комбинации адаптивного 
оптимизатора, нормализации, регулировки LR и регуляризации, 
иначе обучение может быть нестабильным или медленным. 
14. Pretraining и Transfer Learning 
ViT требует больших датасетов для эффективного обучения с нуля, 
потому что у него нет свёрточной индукции и локальных приоритетов. 
На маленьких датасетах сеть быстро переобучается. 
Pretraining: обычно используют крупные наборы вроде ImageNet-21k 
или JFT-300M. Модель учится глобальным зависимостям между 
патчами и мощным feature-эмбеддингам. После этого можно 
применять Transfer Learning. 
Transfer Learning: pretrained ViT легко адаптируется к новым задачам: 
• Для классификации: заменяют финальный Linear head и 
дообучают на новой выборке. 
• Для сегментации/детекции: используют pretrained патч
эмбеддинги и трансформер-энкодер как backbone, а сверху 
добавляют decoder (например, U-Net или DETR-подобную 
голову). 
Fine-tuning стратегии: 
• Замораживание части энкодеров на старте → предотвращает 
разрушение pretrained признаков. 
• Full fine-tuning на малых датасетах с малым learning rate. 
• Использование агрессивных аугментаций и регуляризации для 
адаптации. 
Плюс: pretrained ViT обычно быстрее сходится и даёт более 
качественные глобальные представления, чем обучение с нуля на 
маленьких датасетах. 
15. Разные размеры ViT (ViT-Base, ViT-Large, ViT-Huge) 
ViT бывает разных масштабов, чтобы балансировать вычисления, 
память и качество. Различия идут по количеству слоёв, 
размерности скрытого пространства и количеству голов attention. 
• ViT-Base (B): 
o hidden_dim ≈ 768 
o num_heads ≈ 12 
o num_layers ≈ 12 
→ подходит для большинства задач и стандартных 
датасетов вроде ImageNet. 
• ViT-Large (L): 
o hidden_dim ≈ 1024 
o num_heads ≈ 16 
o num_layers ≈ 24 
→ больше capacity, лучше на крупных датасетах, но требует 
больше GPU-памяти. 
• ViT-Huge (H): 
o hidden_dim ≈ 1280 
o num_heads ≈ 16–32 
o num_layers ≈ 32 
→ супермощная модель, только для очень больших 
датасетов (JFT-300M, ImageNet-21k) и сильных GPU
кластеров. 
Инженерная идея: 
• Увеличение hidden_dim и числа слоёв повышает 
выразительность модели, позволяет учить более сложные 
зависимости между патчами. 
• Но растёт потребление памяти и вычислительные затраты, а 
малые датасеты быстро вызывают переобучение. 
Вывод: 
Выбор размера ViT зависит от размеров датасета, доступной памяти 
и цели: Base для большинства практических задач, Large/Huge для 
research и масштабных наборов данных. 
16. Сравнение с CNN 
Область восприятия: 
• CNN: локальная, через свёртки и receptive field. 
• ViT: глобальная, каждый патч через self-attention видит все 
остальные патчи сразу. 
Индуктивные свойства: 
• CNN: встроенная инвариантность к сдвигу, локальная структура 
и иерархия признаков. 
• ViT: нет встроенной инвариантности, полагается на данные и 
аугментации. 
Гибкость: 
• CNN: хорошо работает на малых датасетах, легко обучается. 
• ViT: требует больших датасетов или pretraining, иначе быстро 
переобучается. 
Выражательная способность: 
• CNN: ограничена локальными фильтрами, глобальный контекст 
появляется только на верхних слоях. 
• ViT: глобальные зависимости с первого слоя, более гибкое 
комбинирование признаков. 
Обучение и оптимизация: 
• CNN: проще оптимизировать, устойчив к небольшим batch и 
learning rate. 
• ViT: чувствителен к масштабу градиентов, требует LayerNorm, 
residual, warmup, AdamW, агрессивных аугментаций. 
Гибкость для задач: 
• CNN: хорошо подходит для классификации и сегментации с 
архитектурой U-Net, FPN. 
• ViT: легко расширяется на детекцию, сегментацию и 
мультимодальные задачи через адаптацию энкодера и 
добавление decoder. 
Итого: 
ViT сильнее в глобальных зависимостях и expressiveness, но CNN 
устойчивее на малых данных и проще в оптимизации. Выбор зависит 
от размера датасета, вычислительных ресурсов и задачи. 
5. CLS токен 
[CLS] токен (Classification token) — это специальный обучаемый 
вектор, добавляемый к началу последовательности патчей перед 
подачей в Transformer Encoder. 
Идея: 
Введён по аналогии с языковыми моделями (BERT), где [CLS] 
используется как агрегатор информации о всём предложении. 
В ViT он выполняет ту же роль — представление всего изображения в 
одном векторе. 
Как работает: 
• После Patch Embedding у нас есть последовательность [B, N, 
D]. 
• Добавляем CLS токен → [B, N+1, D]. 
• CLS участвует в self-attention наравне с другими токенами и 
“видит” их все. 
• После нескольких слоёв attention его финальное состояние 
несёт информацию о всём изображении. 
Финальный шаг: 
CLS токен подаётся в классификатор (MLP Head) для получения 
предсказания класса. 
Важно: 
• Необязателен: альтернативой является mean pooling или 
attention pooling по всем патчам. 
• CLS токен обучаемый (имеет свои веса), и участвует в 
градиентном обновлении. 
6. Transformer Encoder 
Transformer Encoder в ViT — это стек одинаковых блоков (обычно 8–12), каждый 
из которых обрабатывает последовательность токенов [B, N, D] (включая CLS). 
Он сохраняет размерность токенов, но обогащает их представления за счёт 
механизма самовнимания. 
�
� 1. Общая структура блока: 
Каждый encoder-блок состоит из: 
1. LayerNorm → Multi-Head Self-Attention (MHSA) → Residual connection 
2. LayerNorm → MLP (Feed-Forward Network, FFN) → Residual connection 
Схематично: 
x = x + MHSA(LayerNorm(x)) 
x = x + MLP(LayerNorm(x)) 
�
� 2. Multi-Head Self-Attention (MHSA): 
• Делает “взаимопонимание” между токенами: каждый патч может учитывать 
контекст других патчей (учимся понимать, кто с кем связан). 
• Для каждого токена формируются три матрицы: Query, Key, Value. 
• Attention вычисляется как softmax(QKᵀ / √d_k) V. 
• Multi-Head означает, что таких механизмов несколько параллельно → 
модель учится видеть разные “аспекты” связи между патчами. 
�
� 3. MLP / Feed-Forward Network: 
• После внимания идёт двухслойный перцептрон: 
Linear(D → 4D) → GELU → Linear(4D → D) 
• Он обрабатывает каждый токен независимо, но нелинейно, добавляя 
модели выразительности (учимся понимать, что это за объект и какие у 
него свойства). 
�
� 4. Residual connections и LayerNorm: 
• Нормализация стабилизирует обучение. 
• Skip-connections помогают избежать деградации градиентов при глубокой 
сети (до десятков блоков). 
�
� 5. Особенности в ViT по сравнению с NLP Transformer: 
• Отсутствует positional shift (т.е. порядок неважен, кроме position 
embeddings). 
• Нет декодера — только encoder. 
• Все токены равноправны, кроме [CLS]. 
• Обработка происходит в 2D-контексте (патчи вместо слов). 
�
� 6. Результат: 
• Выход Encoder имеет ту же форму [B, N, D]. 
• CLS-токен после последнего блока содержит глобальное представление 
изображения и идёт в классификатор. 
7. Self-Attention для изображений 
Self-Attention для изображений — это механизм, который позволяет 
каждому патчу изображения взаимодействовать со всеми 
остальными, оценивая, насколько каждый из них важен для 
интерпретации текущего. 
Для каждого токена (патча) формируются три вектора — Query (Q), Key 
(K) и Value (V) — через обучаемые линейные проекции. 
Затем вычисляется attention map: 
которая показывает, какие патчи нужно "усилить" или "ослабить" при 
формировании нового представления. 
Затем это внимание применяется к Value: 
Результат — новое представление каждого токена, включающее контекст всех 
остальных. 
В отличие от CNN, где свёртки локальны и работают с ограниченным полем 
восприятия, self-attention в ViT обеспечивает глобальное восприятие 
изображения уже с первого слоя. 
8. Multi-Head Self-Attention (MHSA) 
Multi-Head Self-Attention (MHSA) — это расширение обычного self
attention, где скрытое пространство размерности D делится на 
несколько “голов” (num_heads). 
Каждая голова работает со своей подпространственной частью 
признаков (head_dim = D / num_heads), независимо вычисляя 
внимание. 
Несколько голов позволяют модели параллельно фокусироваться на 
разных типах связей между патчами — пространственных, цветовых, 
текстурных и т.д. 
Это делает ViT гораздо более выразительным, чем вариант с одной 
головой внимания. 
9. Residual connections и LayerNorm 
1. Зачем нужна LayerNorm 
• ViT — это глубокая архитектура, где слои внимания и MLP 
чередуются. Без нормализации значения активаций (особенно 
после attention) могут «взрываться» или «замирать». 
• LayerNorm стабилизирует распределение признаков внутри 
одного патча (т.е. нормализует по размерности hidden_dim), 
чтобы каждая позиция имела нулевое среднее и единичное 
стандартное отклонение. 
• В отличие от BatchNorm, LayerNorm не зависит от размера батча 
— что особенно удобно при обучении на малых батчах, 
характерных для ViT (т.к. память быстро заканчивается из-за 
внимания). 
2. Где именно применяется LayerNorm 
В классическом ViT (Dosovitskiy et al., 2020) LayerNorm используется: 
• Перед каждым саб-блоком (Pre-Norm вариант, в отличие от 
ResNet’ов, где часто Post-Norm). 
o Т.е. схема такая: 
x = x + MSA(LayerNorm(x)) 
x = x + MLP(LayerNorm(x)) 
• Таким образом, каждый residual path получает на вход уже 
нормализированные данные → это помогает градиентам 
распространяться без взрывов. 
3. Почему именно Pre-LN (а не Post-LN, как раньше) 
• В ранних Transformer’ах (NLP) нормализация шла после residual, 
но это ухудшало стабильность при глубоком стеке. 
• В ViT и современных архитектурах (DeiT, Swin, T5, GPT и др.) 
применяется Pre-LN, что: 
o делает обучение более устойчивым; 
o позволяет использовать больше слоёв без взрывов 
градиента; 
o уменьшает потребность в warmup LR. 
4. Итого 
• LayerNorm → нормализует по feature-измерению каждого патча; 
• Используется до внимания и до MLP; 
• В комбинации с residual connection предотвращает деградацию 
сигналов при передаче через десятки блоков. 
10. Forward-pass ViT 
Этап 
Вход 
Размерность 
(bs, c, h, w) 
Смысл 
Изображение 
Патчи 
Линейная 
проекция 
(bs, n, p²·c) 
(bs, n, d) 
Разбиение на фрагменты 
Эмбеддинг патчей 
+ CLS + Pos 
(bs, n+1, d) 
Добавление контекста и 
позиции 
Трансформер
блоки 
Классификация 
(bs, n+1, d) 
(bs, d) → (bs, 
num_classes) 
Обработка внимания и MLP 
Финальный предикт 
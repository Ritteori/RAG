17. Модификации ViT 
1. DeiT (Data-efficient Image Transformer) 
• Основная проблема ViT: требуется огромный датасет для 
pretraining. 
• Решение: знания с больших датасетов имитируют через 
distillation. 
• Используется distillation token, аналог CLS, который собирает 
знания от учителя (обычно CNN). 
• Позволяет обучать ViT на ImageNet без ImageNet-21k, а модели 
всё равно хорошо обобщают. 
• Вводятся агрессивные аугментации (MixUp, CutMix, 
RandAugment) и stochastic depth. 
2. Swin Transformer (Shifted Window) 
• Проблема стандартного ViT: O(N²) внимание, где N = число 
патчей → вычислительно дорого для высоких разрешений. 
• Идея: локальные окна внимания, сдвигаемые на каждом слое: 
(bs, n, d) → attention только внутри окон 
→ уменьшает сложность до линейной от числа патчей. 
• Сдвигаемые окна дают связь между окнами, сохраняя 
глобальный контекст постепенно. 
• Используется как backbone для детекции и сегментации, 
заменяет CNN. 
3. PVT (Pyramid Vision Transformer) 
• Проблема стандартного ViT: фиксированные патчи и одинаковый 
hidden_dim → плохая иерархия признаков для 
детекции/сегментации. 
• Решение: создаём пирамидальную структуру, как в CNN: 
o первые слои: маленький hidden_dim, много патчей (high 
resolution) 
o верхние слои: большой hidden_dim, меньше патчей 
• Пирамидальная структура позволяет эффективно использовать 
ViT для dense prediction tasks. 
4. CrossViT / Multi-scale ViT 
• Идея: использовать патчи разного размера одновременно → 
комбинировать локальные и глобальные признаки. 
• Позволяет модели видеть детали и контекст одновременно. 
5. Efficient / Lite ViT 
• Для мобильных устройств или небольших GPU: 
o уменьшают количество слоёв, heads, hidden_dim 
o используют linear attention или patch merging 
• Сохраняется глобальная внимательность, но падают требования 
к памяти. 
6. Ключевые инженерные трюки 
• Stochastic Depth / DropPath — стабилизация градиентов. 
• Pre-LayerNorm — улучшает стабильность обучения глубоких 
стэков. 
• Distillation token (DeiT) — перенос знаний с CNN. 
• Windowed attention (Swin) — уменьшение вычислительной 
сложности. 
• Pyramidal feature maps (PVT) — иерархия для dense tasks. 
• Multi-scale patch embedding — комбинирование локального и 
глобального контекста. 
18. Подводные камни и ограничения ViT 
1. Требования к данным 
• ViT чувствителен к размеру датасета: без большого объёма 
данных быстро переобучается, потому что у него нет встроенной 
локальной индукции (в отличие от CNN). 
• На маленьких датасетах без pretraining результаты хуже, чем у 
обычных CNN. 
• Решения: pretraining на больших датасетах (ImageNet-21k, JFT
300M), использование DeiT с distillation token, aggressive data 
augmentation (MixUp, CutMix, RandAugment). 
2. Вычислительная сложность 
• Self-Attention имеет O(N²) сложность по числу патчей N. 
o Например, для 224×224 с patch_size=16 → N=196 патчей → 
attention матрица 196×196. 
• Высокое разрешение и большое количество патчей → память и 
вычисления растут квадратично. 
• Решения: 
o Swin Transformer: внимание по локальным окнам → 
сложность O(N) 
o Пирамидальные ViT (PVT): уменьшаем количество патчей в 
верхних слоях 
o Linear attention или sparse attention для мобайл/low
resource задач 
3. Нет индуктивных приоритетов 
• CNN строят иерархию признаков локально → инвариантность к 
сдвигу, устойчивость к шуму. 
• ViT не имеет встроенной локальной иерархии, поэтому: 
o Требуется больше данных и аугментаций 
o Модель чувствительна к смещению, шуму и малым 
объектам 
• Решения: 
o Multi-scale patch embedding 
o Swin Transformer с окнами и перекрытием 
o Пирамидальные структуры для dense prediction 
4. Чувствительность к гиперпараметрам 
• Learning rate, weight decay, batch size критичны: 
o Малый batch → шумные градиенты 
o Большой learning rate → взрыв градиентов 
• Warmup и cosine decay почти обязательны для стабильного 
обучения. 
5. Градиенты и обучение 
• Self-Attention создаёт плотные связи между всеми патчами, 
что может вести к: 
o Взрыву градиентов, если веса Q/K слишком большие → 
softmax становится очень острым 
o Затуханию градиентов в глубоких трансформерных стэках, 
если нет Pre-LayerNorm или residual 
• Решения: Pre-LayerNorm + residual connections + gradient clipping 
6. Малая эффективность на малых датасетах 
• ViT не “учит” локальные шаблоны автоматически 
• Без pretraining и аугментаций на CIFAR-10 или маленьких 
медицинских датасетах часто хуже CNN 
• Часто приходится использовать transfer learning или DeiT-like 
стратегии 
7. Ограничения для dense prediction 
• ViT → патчи фиксированного размера, нет inherent downsampling 
→ сложнее сегментация и детекция 
• CLS токен полезен для классификации, но для 
сегментации/детекции приходится: 
o Игнорировать CLS токен 
o Использовать decoder или линейную проекцию патчей 
o Применять пирамидальные или multi-scale структуры 
8. Ограничения по ресурсам 
• Большие модели (ViT-Large, ViT-Huge) требуют: 
o Много GPU-памяти 
o Mixed precision и оптимизированные attention слои для 
ускорения 
• Малые ресурсы → приходится уменьшать hidden_dim, heads, 
depth → теряется expressiveness 
9. Подводные камни инженерной практики 
• Инициализация весов важна: плохая init → нестабильное 
обучение 
• LayerNorm чувствителен к eps, может вызвать NaN на small batch 
• При fine-tuning: малый LR и замораживание части слоёв часто 
обязательны 
• Для high-resolution изображений стандартный ViT может не 
помещаться в память → нужны patch merging, window attention 
или pyramid ViT 
10. Итог 
• ViT мощен для глобальных зависимостей, но: 
o Требует больших датасетов или pretraining 
o Вычислительно дороже CNN 
o Чувствителен к гиперпараметрам и нормализации 
o Для dense tasks часто нужно адаптировать архитектуру 
(decoder, Swin, PVT) 
• Инженерная практика включает: attention optimization, stochastic 
depth, augmentation, transfer learning, careful LR/weight decay, 
mixed precision.
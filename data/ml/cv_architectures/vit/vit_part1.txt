3. ViT 
1. Мотивация и контекст ViT 
Vision Transformer (ViT) появился как попытка перенести успех 
трансформеров из NLP в компьютерное зрение. 
Главная мотивация — ограничения CNN: 
• Свёртки имеют локальное восприятие (малый receptive field) и 
строят глобальный контекст постепенно, через глубину сети. 
• В архитектуре CNN зашито сильное индуктивное смещение — 
локальность и трансляционная инвариантность, что помогает на 
малых данных, но ограничивает способность модели к 
обобщению на сложных сценах. 
ViT заменяет свёртки на self-attention, который позволяет каждой 
части изображения (патчу) взаимодействовать со всеми остальными 
напрямую. 
Это даёт глобальный контекст уже на ранних слоях, упрощает 
архитектуру и делает модель более универсальной между задачами 
CV и NLP. 
При достаточном количестве данных и вычислительных ресурсов ViT 
демонстрирует лучшую масштабируемость и качество, чем CNN, что 
стало основой для последующих архитектур вроде DeiT, Swin и др. 
2. Основная идея ViT 
Основная идея ViT — рассматривать изображение как 
последовательность токенов, аналогично словам в NLP. 
Изображение размером (H, W, C) делится на непересекающиеся 
патчи размером (P, P). 
Каждый патч разворачивается в вектор длиной P*P*C и затем 
линейно проецируется в пространство размерности D (embedding 
dimension). 
В результате получаем матрицу токенов формы [B, N, D], где N = 
(H*W)/(P²) — количество патчей. 
К этим токенам добавляется: 
• Positional embedding, чтобы сохранить пространственные 
отношения между патчами. 
• CLS-токен (если классификация), который агрегирует 
глобальную информацию после прохода через encoder. 
Такой подход решает проблему высокой вычислительной сложности 
self-attention на уровне пикселей (O((HW)²)) и делает возможным 
применение трансформеров к изображениям. 
3. Patch embedding 
Patch embedding — это первый шаг ViT, где изображение разбивается 
на патчи и каждый патч преобразуется в вектор фиксированной 
размерности. 
Изображение [B, C, H, W] делится на непересекающиеся блоки 
(P, P). Каждый патч разворачивается в вектор длиной P×P×C, и к 
нему применяется линейная проекция (обычно nn.Linear) в 
пространство размерности D. 
На практике это часто реализуют через свёртку с параметрами: 
Conv2d(in_channels=C, out_channels=D, kernel_size=P, 
stride=P) 
— такая операция одновременно «режет» изображение на патчи и 
делает линейную проекцию. 
Результат — последовательность патчей [B, N, D], где N = 
(H×W)/(P²). 
Эти токены далее поступают в encoder трансформера. 
4. Position embedding 
Трансформер сам по себе не знает порядок токенов. Для изображений 
(как и для текста) нужно донести пространственную информацию — 
какие патчи рядом, как они расположены. Для этого к patch
эмбеддингам добавляют позиционные эмбеддинги (positional 
embeddings), которые «вкладывают» информацию о координатах патча 
в вектор токена. 
1) Почему это важно (технически) 
• Self-attention — permutation-invariant: без позиционных 
кодировок перестановка токенов не изменяет выход. Для 
изображений это катастрофа: потеряется геометрия. 
• Position embedding восстанавливает индуктивный приёмник о 
пространственной структуре: относительное/абсолютное 
положение патча влияет на предсказание. 
• Позволяет модели учитывать локальные паттерны (например, 
что полосы рядом друг с другом — часть края) и глобальное 
расположение (например, небо обычно сверху). 
2) Основные типы позиционных эмбеддингов 
A. Learnable absolute positional embeddings (learned) 
• Простая реализация: параметр pos_embed формы [1, N+1, D] 
(если есть CLS — +1), где N = num_patches. 
• Инициализируются случайно (обычно норм/ксатав), обучаются 
вместе с моделью. 
• Преимущества: гибкие, хорошо работают при достаточных 
данных (DeiT, оригинальный ViT использовали их). 
• Минусы: плохо экстраполируют при изменении входного 
разрешения (нужна интерполяция или разная размерность). 
B. Fixed (deterministic) sinusoidal embeddings (sine/cos) 
• Как в оригинальном Transformer (Vaswani): функции синуса и 
косинуса разных частот. 
• Плюсы: не параметрические, хорошо экстраполируют на другие 
длины, фиксированы (меньше параметров). 
• Минусы: иногда уступают learnable в итоговом качестве на 
больших датасетах. 
C. Relative positional embeddings (relative) 
• Кодируют относительное смещение между парами токенов, а не 
абсолютную позицию. 
• Часто эффективнее для задач, где важны отношения между 
локальными элементами (NLP, некоторые варианты ViT). Могут 
быть реализованы как добавочные bias в attention score 
(DeiT/PVT/Swin использовали варианты относительных 
смещений). 
• Плюсы: лучше масштабируются и обобщают по позициям. Могут 
уменьшать потребность в большом абсолютном знании 
позиции. 
D. 2D-aware positional encodings 
• Для изображений удобно формировать pos-emb из двух 
компонент: по высоте и по ширине, объединённых (concat или 
sum). 
• Технически: две матрицы [H_positions, D/2] и 
[W_positions, D/2] → суммируются/конкатенируются, дают 
[H*W, D]. 
• Это более естественно отражает 2D-топологию. 
E. Rotary positional embeddings (RoPE) 
• Внедряют позицию внутрь проекций запросов/ключей (через 
вращения), могут давать хорошую относительную индукцию и 
избавляют от явного хранения pos-векторов. 
• Используются в современных NLP/мультимодальных 
архитектурах, иногда адаптируются и для CV. 
3) Практические детали реализации 
Формат и форма 
• Если нет CLS: pos_embed.shape = (1, N, D). 
• Если есть CLS: pos_embed.shape = (1, N+1, D); обычно 
pos_embed[0,0] — позиция CLS (иногда нулевая или обучаемая 
отдельно). 
• При использовании Conv2d patch embedding: N = (H / P) * (W 
/ P). 
Как добавлять 
• Обычно x = patch_embeddings + pos_embed (поэлементное 
сложение). Альтернативы: конкатенация + линейная проекция 
(реже). 
• Если использован LayerNorm до сложения — порядок может 
влиять; чаще — add → then transformer blocks. 
Проблема изменения разрешения входа 
• Learnable pos emb не совпадают для другого H×W. Решения: 
o Интерполяция (bicubic/linear) pos embeddings из исходной 
сетки к целевой сетке (DeiT/ViT-impl используют это). Код: 
reshape (1, H0, W0, D) → interpolate → reshape (1, N, 
D). 
o Переобучение/finetune pos-emb на новом размере. 
o Использование относительных эмбеддингов или RoPE, 
которые не требуют точной сетки. 
Инициализация 
• Learnable: нормальное распределение с маленьким σ или Xavier. 
• Sin/Cos: по формуле из Vaswani. 
4) Аблации и эмпирические наблюдения 
• На малых датасетах fixed sin/cos иногда даёт более стабильные 
результаты (меньше переобучения), но чаще learnable 
выигрывает при достаточных данных. 
• Relative positional bias часто улучшает локальные задачи 
(детекция/сегментация), потому что внимание получает явную 
информацию о смещении. 
• RoPE/relative часто лучше экстраполируют на разные 
разрешения.
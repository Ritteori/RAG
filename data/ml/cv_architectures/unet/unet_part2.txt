5. Bottleneck и его назначение 
1. Определение 
Bottleneck (или “bridge”) — это центральная часть U-Net между 
encoder и decoder. 
Она работает с feature maps с наименьшим пространственным 
разрешением и наибольшим числом каналов. 
Пример (для 224×224 входа): 
Encoder → 224 → 112 → 56 → 28 → 14 (bottleneck) → 28 → 56 → 112 → 224 
(Decoder) 
2. Главная функция — информационная компрессия 
Bottleneck действует как информационный сжатый слой: 
hbottleneck=fenc(x)h_{bottleneck} = f_{enc}(x)hbottleneck =fenc (x)  
Он должен сохранить всю важную семантическую информацию о 
сцене, при этом отбросив шум и локальные избыточные детали. 
То есть: 
• encoder превращает вход в компактное латентное 
представление, 
• bottleneck удерживает только “что где находится”, без точных 
границ, 
• decoder восстанавливает spatial структуру, используя этот 
контекст + skip connections. 
Это аналог информационного узкого горлышка (Information 
Bottleneck Principle): 
I(h;x) максимально информативно о y, но минимально о x  
(где I — взаимная информация, h — bottleneck-признаки) 
3. Почему нельзя просто убрать bottleneck 
• Без него encoder и decoder не разделяются по функциям: 
сеть теряет чёткую границу между “пониманием” и 
“восстановлением”. 
• У encoder нет зоны, где он агрегирует глобальный контекст 
сцены — receptive field не доходит до всей картинки. 
• Decoder начинает работать с “полусырыми” признаками, 
которые ещё содержат шум, поэтому сегментация становится 
менее стабильной. 
4. Семантический эффект 
• Bottleneck расширяет receptive field до предела (все пиксели 
видят друг друга). 
• Именно здесь формируется “понимание” того, какие объекты 
вообще есть на изображении, а не просто где текстура. 
• Его активации — это semantic map уровня сцены, без 
локальной точности, но с высоким уровнем абстракции. 
5. Инженерные детали 
Обычно bottleneck состоит из: 
• 2 Conv(3×3) слоёв с большим числом каналов (512–1024), 
• иногда Dropout (0.3–0.5) для регуляризации — особенно в мед. 
сегментации, 
• ReLU / LeakyReLU, 
• иногда residual блоки, если глубина encoder большая (для 
лучшего градиентного потока). 
6. Аналогии 
• В автоэнкодерах — это “latent vector”. 
• В VAE / Diffusion / GAN — аналог z-пространства. 
• В U-Net — “compressed semantic map”, используемая decoder’ом 
как контекст для точной реконструкции. 
�
� TL;DR 
• Bottleneck = точка максимальной компрессии и максимального 
смысла. 
• Удерживает глобальный контекст, теряя локальные детали. 
• Без него encoder и decoder сливаются и теряют функциональное 
разделение. 
• Его выход — “смысловое описание” сцены. 
6. Forward-pass и соответствие размерностей 
Forward-pass в U-Net проходит в три фазы: 
1. Encoder (downsampling): 
На каждом уровне выполняется: 
a. Conv → ReLU → Conv → ReLU → MaxPool(2×2). 
Каждый MaxPool уменьшает spatial размер в 2 раза, а количество 
каналов обычно удваивается. 
Например: 
(B, 3, 224, 224) 
(B, 64, 112, 112) 
(B, 128, 56, 56) 
(B, 256, 28, 28) 
(B, 512, 14, 14) 
Эти feature maps сохраняются для skip-коннектов. 
2. Bottleneck: 
Это самая "глубокая" часть сети: 
(B, 1024, 7, 7) 
Здесь фильтры улавливают высокоуровневые (семантические) признаки, но 
почти теряется spatial информация. 
3. Decoder (upsampling): 
На каждом шаге выполняется: 
a. Upsample (через ConvTranspose2d или bilinear + Conv) → 
b. Конкатенация с соответствующим feature map из encoder → 
c. Двойная свёртка. 
Пример: 
(B, 1024, 7, 7) 
→ Upsample → (B, 512, 14, 14) 
+ Skip (B, 512, 14, 14) 
→ (B, 512*2, 14, 14) 
→ ConvConv → (B, 512, 14, 14) 
⚙
️ Почему важно совпадение spatial размеров 
Skip-конкатенация выполняется по измерению каналов (dim=1), 
то есть тензоры должны иметь одинаковые H × W. 
Если хотя бы на 1 пиксель не совпадут, PyTorch выдаст ошибку Sizes of tensors 
must match except in dimension 1. 
Несовпадение возникает, если: 
• при downsample/upsample не хватает padding, 
• исходное изображение имеет нечётные размеры, 
• stride или kernel size не идеально делят spatial размер. 
�
� Как решают на практике 
• Используют padding="same" при свёртках, 
• При необходимости — torch.nn.functional.pad перед конкатенацией, 
• Или просто центр-кроп feature map из encoder перед skip-соединением, как 
в оригинальной статье U-Net. 
7. Backpropagation и влияние skip connections на обучение 
1) Что происходит с градиентами в U-Net — механика 
Рассмотрим простую часть сети, где на уровне i у нас есть: 
• выход энкодера Ei  (размер Ce ×H×W), 
• апсемпл декодера Di+1↑  (размер Cd ×H×W), 
• конкатенация: Ci =concat(Di+1↑ ,Ei ), 
• после неё идут свёртки и далее — следующий слой декодера. 
При backprop производная лосса L по отношению к конкатенации 
распадается по каналам и распределяется на оба входа: 
То есть градиент напрямую попадает в Ei  (и далее по энкодеру) без 
прохождения через все следующие более глубокие декодер-слои. Это 
сокращает длину пути, по которому ошибка должна 
распространяться, по сравнению с вариантом без skip-связей, где 
сигнал шел бы только через весь bottleneck и обратный проход 
декодера. 
Ключевая мысль: concat даёт параллельный путь для градиента в 
сторону энкодера — он не «складывает» сигнал (как в ResNet), но 
обеспечивает короткий маршрут, уменьшая эффект затухания и 
облегчая обучение ранних слоёв. 
2) Влияние на стабильность и скорость обучения 
• Уменьшение vanishing gradient: благодаря более коротким 
путям для градиента ранние слои энкодера получают более 
сильные градиентные сигналы — точность обучения 
повышается, сходимость ускоряется. 
• Более точная локальная информация: skip-связи передают 
высокочастотные, пространственно точные признаки (границы, 
контуры). Когда градиент напрямую доходит до этих признаков, 
модель лучше корректирует параметры, ответственные за 
локальную детализацию. 
• Меньше “перетирания” деталей: декодеру легче научиться 
комбинировать глобальный контекст (от bottleneck) и локальные 
признаки (от skip), т.к. энкодер получает стабильные градиенты 
для формирования этих локальных карт. 
3) Чем concat отличается от суммирования (residual) в плане 
градиентов 
• Residual (сумма): есть идентичный путь (identity), который 
пропускает сигнал без изменений и позволяет градиенту течь 
через него напрямую. Это даёт очень сильный эффект 
стабилизации градиентов. 
• Concat (U-Net): градиент тоже течёт напрямую в энкодер-фичи, 
но через операцию разреза каналов — нет «чистого» identity
пута для входа декодера. 
→ В результате concat улучшает поток градиентов, но не так 
мощно как суммирование для внутриблоковой стабильности. 
Зато сохраняет обе представления (декодерные и энкодерные) 
полностью, а не смешивает их. 
4) Подводные камни и тонкие места (что может пойти не так) 
1. Semantic gap (разрыв семантики): фичи из энкодера и 
декодера могут иметь разный статистический масштаб и 
уровень абстракции. Прямая конкатенация без приведения 
может затруднить обучение (декодеру тяжелее научиться 
комбинировать несовместимые представления). 
a. Решения: 1×1 Conv для приведения каналов/масштаба, 
нормализация (BatchNorm/LayerNorm) перед конкатом, 
attention-веса (Attention U-Net). 
2. Разная амплитуда градиентов: если одна ветка (энкодер) даёт 
большие по масштабу градиенты, это может «перетянуть» 
обучение; нуждаются в балансе (scale/normalization). 
3. Глубина и слабый эффект по сравнению с residual: concat 
помогает, но если сеть очень глубокая, добавление residual 
внутри блоков (residual U-Net) часто необходимо для стабильной 
тренировки. 
4. Апсемпл-вариации: если апсемплинг делается 
параметризованно (ConvTranspose2d), градиенты проходят и 
через веса апсемплинга; если апсемплинг — интерполяция (без 
параметров), то через эту операцию градиент просто 
распространяется без обучения. Это меняет динамику 
корректировки декодера. 
5) Практические инженерные рекомендации (чтобы обучение было 
стабильным и быстрым) 
• Нормализация перед конкатом: BatchNorm / InstanceNorm / 
GroupNorm на обеих ветках до конкатенации — выравнивает 
масштабы. 
• 1×1 conv для выравнивания каналов / масштаба: сокращает 
semantic gap и даёт контролируемую смесь признаков. 
• Attention / gating на skip: позволяет подавлять нерелевантные 
локальные признаки, улучшая обобщение. 
• Residual внутри ConvBlock: добавляет ещё один сильный канал 
для устойчивого градиентного потока. 
• Deep supervision: подключение дополнительных потерь на 
промежуточных уровнях (aux losses) — помогает распространять 
цель напрямую в ранние слои. 
• Мониторинг градиентов: логируй нормы градиента в ключевых 
слоях; при взрывных градиентах — gradient clipping. 
• LR schedule и веса инициализации: аккуратная инициализация 
и learning rate warmup помогают избежать ранних проблем с 
неустойчивыми градиентами. 
6) Математическая/интуитивная картина влияния на ландшафт loss 
Skip-связи добавляют дополнительные каналы прямого влияния 
параметров энкодера на итоговый loss, что: 
• делает ландшафт loss «менее крутым» в направлениях ранних 
параметров (т.е. градиенты более информативны), 
• уменьшает количество локальных плато, где градиенты могут 
почти исчезнуть, 
• повышает эффективность стахастического градиентного поиска 
(лучше сходится). 
7) Связанные продвинутые темы (коротко) 
• U-Net++ / dense skip: увеличивают число путей для градиента → 
ещё лучшая тренируемость, но больше параметров и риск 
переобучения. 
• Deep supervision: даёт дополнительные градиентные пути к 
ранним слоям (ещё одна форма «ускорения» обучения). 
• Gated/Attention skips: контролируют поток информации и 
градиентов через маски. 
8) Короткие ответы на возможные собеседовательские подковырки 
• Q: «А дают ли skip-связи полностью те же преимущества, 
что и residual?» 
A: Нет. Обе улучшают поток градиента, но residual даёт identity
путь внутри блока (сумма) — более сильный «короткий» маршрут; 
concat даёт параллельный маршрут для признаков и частично 
для градиента, сохраняя обе репрезентации. 
• Q: «Почему иногда добавляют residual внутри U-Net?» 
A: Чтобы совместить преимущества обоих подходов: сохранить 
локальные признаки (skip) и обеспечить сильную внутриблочную 
стабилизацию (residual). 
TL;DR (быстро) 
• Skip-связи в U-Net через concat дают короткий путь для 
градиента к ранним слоям — это улучшает сходимость и 
сохранение деталей. 
• Это не полностью то же, что residual-summation, но даёт важный 
практический выигрыш для сегментации. 
• Внимание к semantic gap, нормализациям, 1×1 conv и (по 
необходимости) residual-блокам делает обучение более 
стабильным и точным. 

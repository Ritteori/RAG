2. U-Net 
 
1. Мотивация и контекст U-Net 
До U-Net существовали Fully Convolutional Networks (FCN, 2014), которые 
первыми позволили сегментировать изображения пиксель-в-пиксель, заменив 
dense-слои на свёртки. Но FCN имели серьёзный недостаток: 
• при downsampling (pooling) терялась пространственная точность, 
• итоговые карты были грубые и размытые, особенно для мелких объектов, 
• и восстановление разрешения через простой upsampling не 
компенсировало потерю деталей. 
U-Net (2015) была создана в ответ на эти проблемы, особенно для медицинских 
изображений, где: 
• датасетов мало, 
• объекты мелкие, 
• критична точная граница сегментации. 
Главная идея: 
• добавить симметричный decoder, который поэтапно восстанавливает 
spatial resolution; 
• ввести skip connections между encoder и decoder, чтобы напрямую 
передавать локальные признаки и градиенты. 
Благодаря этому: 
• сеть сочетает глобальный контекст (из encoder) и локальную 
детализацию (из skip connections); 
• обучается устойчиво даже на малом количестве данных; 
• даёт резкие, детализированные маски без “размытия”. 
Ключевые тезисы для конспекта: 
• FCN → потеря деталей при upsampling. 
• U-Net → симметричный encoder-decoder + skip connections. 
• Цель: сегментация мелких структур при малых данных. 
• Применение: медицина, спутниковые снимки, биология. 
2. Основная идея: U-образная архитектура 
U-Net построена по принципу симметричной encoder-decoder 
архитектуры с U-образной формой, где левая часть сети сжимает 
изображение (encoder), а правая — восстанавливает его размерность 
(decoder). 
Encoder извлекает глобальные семантические признаки и контекст, 
постепенно уменьшая spatial resolution. 
Decoder восстанавливает spatial resolution, используя эти признаки 
для построения точной маски. 
Ключевая идея: 
Сегментация требует одновременно понимания контекста (что это за 
объект) и точного знания положения (где он находится). 
U-Net объединяет эти аспекты через skip connections, которые: 
• передают локальные (высокочастотные) признаки с верхних 
уровней encoder, 
• совмещают их с глобальными (семантическими) признаками 
из нижних уровней decoder. 
Эта комбинация устраняет проблему размытых контуров и потери 
мелких структур, характерную для FCN. 
Форма “U” отражает симметричную структуру downsampling → 
bottleneck → upsampling. 
Технические нюансы (часто забывают): 
• Структура encoder/decoder не обязательно строго зеркальна, но 
глубина слоёв должна совпадать, чтобы skip connections 
соединялись корректно. 
• Upsampling может быть необучаемым (bilinear) или обучаемым 
(ConvTranspose2d) — второе даёт чуть лучше результат, но 
дороже по памяти. 
• Существует semantic gap: признаки encoder и decoder могут 
иметь разный уровень абстракции, из-за чего concat не всегда 
идеально — это привело к появлению U-Net++. 
3. Skip connections — их смысл и роль 
1. Смысл skip connections 
В U-Net skip connections (или concatenation links) связывают энкодер 
и декодер на одинаковых уровнях разрешения. 
То есть, после каждого downsampling (Conv+Pool) мы сохраняем 
feature map и передаём его через skip в соответствующий upsampling
блок. 
Зачем: 
• Сохранить пространственную информацию, которая теряется 
при downsampling. 
• Компенсировать потерю мелких деталей (тонкие структуры, 
границы объектов). 
• Упростить восстановление исходной геометрии сцены — 
декодер получает не только апсемпленные “глобальные” 
признаки, но и “локальные” контуры от энкодера. 
Формально: 
2. Отличие от residual connections 
Характеристи
ка 
Цель 
U-Net skip connection 
Сохранить 
пространственные 
признаки 
Направление Между энкодером и 
декодером 
Операция 
ResNet residual connection 
Улучшить обратное 
распространение градиента 
Внутри одного блока 
Concatenation (по каналам) Summation (поэлементно) 
Содержимое Feature maps из разных 
стадий сети 
Эффект 
Восстановление деталей, 
повышение точности 
сегментации 
Один и тот же уровень 
абстракции 
Устранение затухания 
градиентов, стабилизация 
обучения 
3. Почему без skip-связей падает качество 
Без skip’ов декодер должен восстанавливать пространственную структуру только 
из сжатых латентных признаков. 
Из-за потери локальной информации при последовательных пулингах он теряет 
точность на границах и мелких объектах. 
В результате сегментация становится: 
• размытой, 
• “плавающей”, 
• с разрывами контуров. 
4. Глубокий инсайт (уровень senior) 
Skip-связи не просто добавляют информацию — они разделяют два типа 
признаков: 
• глобальные (semantic, контекстные) — через энкодер-декодер путь, 
• локальные (spatial, boundary-aware) — через skip. 
U-Net, по сути, реализует feature fusion с разными уровнями инвариантности: 
нижние слои → точные координаты, 
верхние → смысловые объекты. 
Их конкатенация создаёт “мультиуровневое” представление (multi-scale fusion), 
аналогичное FPN, но проще и раньше по времени. 
�
� TL;DR 
Skip connections в U-Net: 
• соединяют encoder и decoder по уровням, 
• восстанавливают spatial detail и чёткие границы, 
• делают сегментацию детальной и устойчивой, 
• работают по конкатенации, а не сложению, 
• отличаются от residual-связей как “передача признаков” vs “передача 
сигнала”. 
4. Структура encoder и decoder блоков 
1. Encoder — нисходящая часть (“contracting path”) 
• Каждая стадия содержит 2 последовательные свёртки (обычно 
3×33\times33×3) 
o BatchNorm 
o ReLU (раньше использовали просто ReLU, сейчас часто 
LeakyReLU / GELU). 
• После блока идёт downsampling — чаще MaxPooling(2×2). 
Почему MaxPooling, а не Conv(stride=2)? 
• MaxPooling сохраняет сильные активные признаки, не 
смешивая их с соседними пикселями. 
Это полезно для выделения границ — важное свойство в 
медицинских и мелкодетальных задачах. 
• Strided Conv "перемешивает" информацию и может создавать 
aliasing (смещение пространственных частот). 
• MaxPooling — немодульный и стабильный при обратном 
распространении, а веса свёртки — нет. 
Однако в современных реализациях (U-Net++, U-Net 3+) часто 
заменяют MaxPool на Conv(stride=2) — для совместимости с pretrained 
backbone (ResNet, EfficientNet). 
2. Bottleneck (“bridge”) 
• Самая глубокая часть сети с наибольшим числом каналов. 
• Задача — агрегировать глобальный контекст (semantics), 
сохранив инвариантность к положению. 
• Здесь receptive field максимально велик — модель “понимает 
всю сцену”. 
3. Decoder — восходящая часть (“expansive path”) 
• На каждом уровне делается Upsampling (обычно ×2), после чего 
— конкатенация с feature map из encoder. 
• После конкатенации идут 2 Conv(3×3) + ReLU (аналогично 
encoder-блокам). 
Почему UpSampling, а не Interpolation-only или TransposedConv? 
• TransposedConv (ConvTranspose2d) обучаемая — позволяет 
адаптивно восстанавливать spatial details, но может давать 
checkerboard artifacts. 
Checkerboard artifacts — это тип искажений, которые проявляются 
как мелкая шахматная сетка или полосатый узор на выходных 
изображениях, особенно при генерации или апсемплинге. 
�
� Почему они появляются: 
Главная причина — неравномерное покрытие пикселей при 
Transposed Convolution (ConvTranspose2d). 
Когда свёртка “растягивает” feature map, некоторые выходные 
пиксели получают вклад от большего числа входных пикселей, чем 
другие. 
В результате — часть пикселей оказывается “ярче” или “темнее” → 
возникает регулярный узор в виде сетки. 
• UpSampling (bilinear / nearest) не имеет весов, но проще и 
стабильнее — часто предпочтительнее для задач, где важны 
формы, а не текстуры (например, медицинские снимки). 
• Иногда комбинируют: UpSampling → Conv(3×3) → ReLU, чтобы 
убрать артефакты и сделать реконструкцию более гладкой. 
4. Архитектурная логика “симметрии” 
U-Net симметричен не ради эстетики: 
• каждая стадия encoder ↔ decoder имеет одинаковое 
пространственное разрешение; 
• skip connections обеспечивают точное “соответствие координат” 
между feature maps; 
• за счёт этого границы сегментов реконструируются точно, без 
сдвига на пиксель (spatial alignment). 
С математической точки зрения, это поддерживает изоморфизм 
feature space’ов между encoder и decoder на каждом уровне. 
5. Современные варианты 
• Residual U-Net: добавляют skip внутри ConvBlock (улучшает 
градиентный поток). 
• U-Net++: dense skip-связи между всеми уровнями encoder и 
decoder. 
• Attention U-Net: взвешивает skip-связи (передаёт только 
релевантные признаки). 
�
� TL;DR 
• Encoder = ConvBlock + MaxPool → увеличивает семантику, теряет 
детали. 
• Decoder = UpSample + ConvBlock → восстанавливает spatial 
структуру. 
• Симметрия и skip connections позволяют совместить локальные 
и глобальные признаки. 
• MaxPool и UpSampling выбраны ради стабильности и spatial
согласованности, а не из-за вычислительной простоты. 
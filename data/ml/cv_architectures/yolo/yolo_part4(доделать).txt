14. 
Что такое ignore mask и зачем она нужна? 
Ignore mask в YOLOv3 используется в части лосса для objectness 
и нужна для того, чтобы модель не наказывалась за предсказания, 
которые хорошо совпадают с реальным объектом, но формально не 
были назначены ответственными anchor’ами. При формировании 
таргетов каждому GT-боксу назначается ровно один anchor — тот, у 
которого максимальный IoU по ширине и высоте. Этот anchor получает 
objectness = 1 и участвует во всех частях лосса. Однако остальные 
anchor’ы в той же ячейке или на том же scale могут иметь достаточно 
высокий IoU с GT-боксом. Если такие anchor’ы считать 
отрицательными примерами и принудительно обучать objectness → 0, 
модель будет получать конфликтующие градиенты: с одной стороны, 
она правильно предсказывает объект, с другой — за это же её 
штрафуют. Ignore mask решает эту проблему: для anchor’ов, у которых 
IoU с любым GT превышает заданный порог (обычно 0.5), objectness 
loss просто не считается. Формально это означает, что для них ни 
target=1, ни target=0 не задаётся — они исключаются из BCE. В 
результате объект «обслуживается» одним anchor’ом, остальные не 
мешают обучению, и модель не учится подавлять хорошие, но не 
ответственные предсказания. Это особенно критично при плотных 
сценах и при использовании нескольких anchors на одну ячейку, 
иначе objectness loss начинает доминировать и ухудшает сходимость. 
15. 
Почему YOLOv3 использует BCE вместо softmax и MSE? 
YOLOv3 использует Binary Cross Entropy вместо softmax и MSE потому, 
что objectness и классы в этой модели формулируются как 
независимые бинарные задачи, а не как одна многоклассовая 
классификация. Softmax предполагает взаимоисключающие классы и 
нормализацию вероятностей по всем классам, что плохо сочетается с 
детекцией, где в одном боксе может быть только один объект, но в 
общем случае модель должна быть устойчива к много-меточным 
сценариям и шумным аннотациям. Используя BCE, YOLOv3 
предсказывает вероятность каждого класса независимо, без жёсткого 
соревнования между ними, что упрощает обучение и делает модель 
более гибкой, особенно при большом числе классов или частично 
размеченных данных. 
MSE для objectness и классов в ранних версиях YOLO приводил к 
проблеме дисбаланса: подавляющее большинство anchor’ов — это 
фон, и MSE сильно штрафует крупные ошибки, из-за чего objectness 
loss начинает доминировать и обучение становится нестабильным. 
BCE лучше подходит для вероятностных выходов, даёт более 
информативные градиенты в области около 0 и 1 и естественно 
сочетается с sigmoid-активацией, которую YOLOv3 использует для 
objectness и классов. В итоге BCE делает оптимизацию устойчивее и 
логически согласованной с тем, что именно предсказывает модель — 
вероятности наличия объекта и принадлежности к классам, а не 
абстрактные регрессионные значения. 
16. 
Какие основные проблемы при обучении YOLO (дисбаланс, 
нестабильность и т. д.)? 
17. 
18. 
Как YOLOv3 справляется с мелкими объектами (роль FPN)? 
Что добавили YOLOv4–v8 (CSP, PANet, Mosaic, DFL, anchor
free подход)? 
19. 
20. 
Как реализуется inference: сигмоиды, пороги, NMS. 
Почему YOLOv3 всё ещё используется в реальном времени 
и медицине?
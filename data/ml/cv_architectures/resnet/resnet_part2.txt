6. Архитектуры ResNet-18/34/50/101/152 — различия 
�
� 1. Основная идея 
Где 
используе
тся 
неглубокие 
сети 
глубокие 
сети 
Все ResNet используют residual-блоки, но структура блоков и их количество 
различается. 
Главные различия: 
• Тип блока (Basic vs Bottleneck) 
• Количество блоков на каждой стадии (stage) 
• Глубина сети (число слоёв) 
�
� 2. ResNet-18 и ResNet-34 
• Используют Basic Block (2×3×3 Conv) 
• Структура стадий (каждая стадия = несколько блоков, одинаковое число 
каналов): 
Стадия Кол-во блоков Каналы 
Conv2_x 2 / 3 
64 
Conv3_x 2 / 4 
Conv4_x 2 / 6 
Conv5_x 2 / 3 
128 
256 
512 
• Пример: ResNet-18 → блоки: [2,2,2,2] 
�
� 3. ResNet-50, ResNet-101, ResNet-152 
• Используют Bottleneck Block (1×1 → 3×3 → 1×1) 
• Количество блоков больше, чтобы увеличить глубину, но без резкого роста 
параметров: 
Сеть 
ResNet-50 
ResNet-101 
ResNet-152 
Блоки по 
стадиям 
[3,4,6,3] 
[3,4,23,3] 
[3,8,36,3] 
Глубина 
50 
Каналы 
256,512,1024,2048 
101 
152 
256,512,1024,2048 
256,512,1024,2048 
• Каждая стадия использует projection shortcut для изменения числа 
каналов и spatial size. 
�
� 4. Ключевые отличия между архитектурами 
1. Глубина сети → больше блоков = больше слоёв 
2. Тип блока → Basic (меньше вычислений, неглубокие сети) / Bottleneck 
(вычислительно эффективно, глубокие сети) 
3. Количество каналов на стадиях → Bottleneck использует 1×1 для сжатия и 
восстановления, поэтому выходные каналы больше 
�
� 5. Идея в одном предложении 
ResNet-18/34 — неглубокие, Basic Block; 
ResNet-50/101/152 — глубокие, Bottleneck Block, projection shortcut для 
согласования размеров; 
всё остальное — одна и та же концепция residual learning. 
7. Forward-pass и shape consistency 
�
� 1. Forward-pass в ResNet 
• Сеть получает вход xxx (например, изображение 224×224×3) 
• Проходит через начальный слой: Conv7x7 (stride=2) → BN → ReLU → MaxPool 
• Далее идёт несколько стадий residual-блоков: 
o Каждая стадия состоит из нескольких блоков (Basic или Bottleneck) 
o Внутри блока: Conv → BN → ReLU → Conv → BN → сложение с shortcut → 
ReLU 
• На выходе последней стадии: Global Average Pooling → Fully Connected → 
Softmax (для классификации) 
�
� 2. Shape consistency (согласованность размеров) 
Чтобы можно было выполнять сложение F(x)+x, необходимо, чтобы: 
1. Число каналов совпадало 
a. Если нет → используется 1×1 Conv в shortcut 
2. Spatial size совпадало (H×W) 
a. Если stride ≠ 1 → 1×1 Conv в shortcut с тем же stride 
• Внутри блока, где размеры не меняются, identity shortcut просто передаёт x 
напрямую 
�
� 3. Схема размеров на примере ResNet-50 
Стадия Размер карты признаков (H×W) 
Conv1 
Conv2_x 
Conv3_x 
Conv4_x 
Conv5_x 
112×112 
56×56 
28×28 
14×14 
7×7 
GAP + FC 1×1 
Каналы 
64 
256 
512 
1024 
2048 
1000 (классы) 
• Каждое downsampling изменение размеров делается только в начале 
стадии, все остальные блоки стадии сохраняют форму. 
�
� 4. Ключевая идея 
• Forward-pass = простое сложение трансформации F(x) и оригинального x 
• Shape consistency = обязательное условие для residual connection 
• Identity vs Convolutional shortcut решает проблему несовпадения 
размеров 
8. Initialization и BatchNorm в ResNet 
�
� 1. Инициализация весов 
• Инициализация важна для стабильного обучения глубоких сетей. 
• В ResNet обычно используют He initialization (Kaiming initialization): 
W∼N(0,2/fan_in) 
где fan_in — число входов в слой (kernel_size * kernel_size * channels). 
• Причина: 
o ReLU используется как активация → variance нужно масштабировать 
на 2 (чтобы сигнал и градиенты не исчезали). 
�
� 2. Batch Normalization (BN) 
• После каждой свёртки и до ReLU ставят BN: 
Conv → BN → ReLU 
• Цели BN: 
o Стабилизировать распределение активаций на каждом слое 
o Позволяет использовать большие learning rate 
o Снижает риск затухания/взрыва градиентов 
o Делает обучение глубокой сети более предсказуемым 
• Формула BN: 
где μB ,σB2  — среднее и дисперсия по батчу, γ,β — обучаемые параметры 
масштабирования и смещения. 
�
� 3. Почему инициализация + BN критичны в ResNet 
• Residual блоки делают сети очень глубокими (50–152 слоёв) 
• Без BN и корректной инициализации: 
o градиенты могут затухать или взрываться 
o обучение будет нестабильным 
• BN + He init + residual connection → сеть учится глубоко и стабильно 
�
� 4. Идея в одном предложении 
He initialization + BatchNorm + residual connections → глубокие сети обучаются 
стабильно, без проблем затухающих/взрывающихся градиентов. 
9. Почему ResNet легче оптимизируется 
• Каждый блок учит только “дельту” от входного сигнала, а не полностью 
новое представление. 
• Shortcut connection даёт прямой путь для градиента → нет проблемы 
затухания. 
• В сочетании с BatchNorm и правильной инициализацией обучение 
стабильно даже для очень глубоких сетей. 
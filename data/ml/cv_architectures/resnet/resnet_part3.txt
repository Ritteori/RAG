10. Вариации: Pre-activation, WideResNet, ResNeXt, SE-ResNet 
�
� 1. Pre-activation ResNet 
• Структура: BN → ReLU → Conv вместо Conv → BN → ReLU. 
• Мотивация: 
o В оригинальном ResNet BN и ReLU после свёртки → часть градиента 
может “теряться” в глубоких сетях. 
o Перенос нормализации и активации перед свёрткой создаёт 
прямой путь для сигнала и градиента через identity shortcut. 
• Эффект: 
o Обучение становится ещё более стабильным для очень глубоких 
сетей (100+ слоёв). 
o Дает небольшое улучшение accuracy на ImageNet. 
�
� 2. WideResNet 
• Идея: уменьшить глубину и увеличить ширину блоков (количество каналов). 
• Структура: тот же residual-блок, но каждый слой шире, например, вместо 
64 каналов → 128, 256 и т.д. 
• Мотивация: 
o Глубокие сети труднее оптимизировать → широкие, но менее 
глубокие блоки обучаются легче. 
o GPU эффективнее на широких тензорах, чем на суперглубоких. 
• Эффект: 
o Более быстрая сходимость. 
o Высокая accuracy при меньшей глубине. 
o Иногда требует большего регуляризатора (Dropout) из-за 
увеличенного числа параметров. 
�
� 3. ResNeXt 
• Идея: в каждом residual-блоке разделяем свёртки на несколько 
параллельных “cardinality” веток, затем суммируем. 
• Структура: 
o Вместо одного 3×3 Conv → несколько 3×3 Conv параллельно, каждая 
ветка с меньшим числом каналов. 
o На выходе блок объединяет ветки через суммирование (aggregation). 
• Мотивация: 
o Увеличение cardinality (количество веток) эффективнее, чем просто 
делать сеть глубже или шире. 
o Позволяет лучше извлекать разные признаки одновременно. 
• Эффект: 
o Меньше параметров при той же representational power. 
o Лучшая обобщающая способность на ImageNet. 
o Сохраняет простоту оптимизации благодаря residual connections. 
�
� 4. SE-ResNet (Squeeze-and-Excitation) 
• Идея: добавить внимание на каналы внутри residual-блока. 
• Структура блока: 
o Residual-блок вычисляет F(x) 
o Squeeze: глобальный average pooling по spatial-измерениям → вектор 
размером C 
o Excitation: небольшая MLP с двумя слоями → получает веса для 
каждого канала 
o Scale: умножение исходного F(x) на веса каналов 
o Сложение с x → ReLU 
• Мотивация: 
o Не все каналы одинаково важны для конкретного изображения 
o Позволяет сети “фокусироваться” на ключевых признаках 
• Эффект: 
o Существенное повышение accuracy на ImageNet 
o Увеличение параметров минимально 
o Можно совмещать с другими вариациями, например ResNeXt + SE 
�
� 5. Общие выводы по вариациям 
• Все варианты сохраняют концепцию residual learning. 
• Основные направления улучшений: 
o Pre-activation → стабильность градиента для очень глубоких сетей 
o WideResNet → обучение быстрее, более широкие блоки 
o ResNeXt → эффективное увеличение representational power через 
ветки 
o SE-ResNet → динамическое внимание на каналы, улучшение 
accuracy 
11. Эмпирические эффекты (тренируемость, обобщение, глубина) 
�
� 1. Тренируемость 
• ResNet позволяет обучать очень глубокие сети (50–152+ слоёв) без 
значительных проблем с затухающими градиентами. 
• Residual connections обеспечивают прямой путь для градиента, поэтому 
даже первые слои получают сигнал и обучаются эффективно. 
• Pre-activation ResNet ещё больше улучшает тренируемость сверхглубоких 
сетей (>1000 слоёв). 
�
� 2. Обобщение (generalization) 
• Глубокие ResNet показывают лучшее качество на тестовых данных по 
сравнению с VGG или обычными глубокими ConvNet, несмотря на большее 
количество слоёв. 
• Residual learning упрощает обучение “мелких изменений” от входного 
сигнала, что помогает сохранять информативные признаки и избегать 
переобучения. 
• SE-ResNet и ResNeXt добавляют больше representational power, улучшая 
способность модели извлекать полезные признаки без чрезмерного роста 
параметров. 
�
� 3. Эффект глубины 
• До появления ResNet попытки строить сети глубже 20–30 слоёв часто 
приводили к деградации accuracy. 
• ResNet позволяет постепенно увеличивать глубину до 100–152 слоёв без 
падения производительности. 
• Эмпирические наблюдения: 
o ResNet-50 > ResNet-34 > ResNet-18 по accuracy на ImageNet 
o Слишком глубокие сети без модификаций (например, >200 слоёв) 
могут слегка страдать от оптимизации, но pre-activation или 
WideResNet это решает. 
12. Сравнение с VGG и DenseNet 
�
� 1. VGG 
• Структура: просто цепочка 3×3 свёрток и pooling, без residual connections. 
• Глубина: до 19 слоёв. 
• Проблемы: 
o При попытке увеличить глубину (>20 слоёв) → затухание градиентов, 
трудности с оптимизацией. 
o Большое количество параметров (все 3×3 свёртки → много весов). 
• ResNet vs VGG: 
o ResNet позволяет строить гораздо более глубокие сети с меньшими 
проблемами оптимизации. 
o Несмотря на большее количество слоёв, ResNet имеет сравнимое 
или меньшее число параметров, особенно с Bottleneck-блоками. 
o Accuracy ResNet выше на ImageNet при схожих ресурсах. 
�
� 2. DenseNet 
• Идея: каждый слой получает на вход все предыдущие feature maps через 
concatenation, а не сложение. 
• Плюсы: 
o Максимальное повторное использование признаков 
o Хорошее обобщение и экономия параметров 
• Минусы: 
o Требует больше памяти для хранения всех feature maps 
o Computationally heavier при большой глубине 
• ResNet vs DenseNet: 
o ResNet проще оптимизировать: shortcut через сложение легче для 
градиентов, чем concatenation 
o DenseNet сильнее в feature reuse, но рост памяти может 
ограничивать глубину 
o ResNet легче масштабировать до 100+ слоёв 
�
� 3. Краткая таблица сравнения 
Модель Глубин
а 
Shortcut / 
Connectiv
ity 
Парамет
ры 
Плюсы 
Минусы 
плохо 
VGG 
ResNet 
16–19 
нет 
18–152+ addition 
(residual) 
DenseNet 121–201 concatena
tion 
�
� 4. Идея 
много 
умеренно 
меньше 
простота 
легко 
оптимизируется, 
глубокая 
feature reuse, 
обобщение 
оптимизируетс
я глубоко 
меньше reuse 
фич 
память, 
вычисления 
ResNet занимает золотую середину: глубокая сеть, легко обучаемая, с хорошим 
trade-off между памятью, скоростью и accuracy. 
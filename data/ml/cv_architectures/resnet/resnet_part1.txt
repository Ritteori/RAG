1. ResNet18/34/50/101/152 
1. Мотивация и контекст 
До появления ResNet (He et al., 2015, “Deep Residual Learning for Image 
Recognition”) увеличение глубины сверточных сетей приводило не только к росту 
вычислительной сложности, но и к ухудшению обучаемости. Это явление 
получило название degradation problem — при одинаковых условиях 
(инициализация, оптимизатор, BatchNorm) более глубокие сети показывали 
большую ошибку на обучающем наборе, чем мелкие. 
Основные причины: 
1. Затухание и взрыв градиентов. 
При прохождении градиента через десятки слоёв, особенно с 
нелинейностями, его значение стремилось к нулю (или к бесконечности), 
из-за чего ранние слои переставали обновляться. 
2. Трудность оптимизации глубокой композиции функций. 
Сеть должна аппроксимировать сложное отображение H(x), и по мере 
роста глубины пространства возможных решений становятся всё менее 
гладкими для градиентных методов. 
До ResNet пробовали решения вроде: 
• улучшенной инициализации (Xavier, He init), 
• Batch Normalization, 
• skip-connections на уровне фич (Highway Networks, 2015). 
Однако полностью деградацию они не устраняли. 
Именно это и привело исследователей из Microsoft Research к идее Residual 
Learning, где сеть должна не учить напрямую H(x), а только разницу (остаток) 
относительно входа x: 
Такое простое переопределение задачи резко упростило оптимизацию глубоких 
моделей (до 152+ слоёв) и позволило строить сети, которые легче обучаются, 
лучше обобщают и не теряют точность при увеличении глубины. 
2. Основная идея: Residual Learning 
�
� Проблема 
При увеличении глубины сети классического типа (без пропусков сигналов) 
задача обучения становится труднее: 
сеть должна аппроксимировать сложное нелинейное отображение 
H(x)  
где x — вход, H(x) — желаемая функция. 
На практике оказалось, что более глубокие сети не просто переобучаются, а хуже 
обучаются, даже на трейне. 
�
� Идея He et al. (2015) 
Вместо того чтобы напрямую учить H(x), предлагается представить его в виде: 
H(x)=F(x)+x  
где F(x)=H(x)−x называется residual function (остаточная функция). 
То есть сеть должна учить не полное отображение, а только поправку (остаток) 
относительно тождества. 
�
� Интуитивный смысл 
• Если оптимальная функция близка к тождественной (identity mapping), 
то намного проще обучить F(x)≈0, чем H(x)≈x. 
• Слой может просто "пропустить" вход через себя, если дальнейшее 
обучение не улучшает представление. 
• Таким образом, глубина перестаёт быть проблемой — сеть может добавлять 
новые слои без риска деградации. 
�
� Ключевая идея 
Residual connection обеспечивает прямой поток информации и градиента. 
Это делает возможным обучение сетей с сотнями слоёв, 
так как сигнал не затухает при обратном распространении. 
3. Формула и смысл shortcut connection 
�
� 1. Формула 
Ключевая формула residual-блока: 
где: 
• x — вход в блок, 
y=F(x,{Wi })+x  
• F(x,{Wi }) — преобразование внутри блока (обычно 2–3 слоя Conv+BN+ReLU), 
• y — выход блока, 
• знак “+” — shortcut connection (т.е. простое сложение поэлементно). 
Если размерности отличаются (например, при stride=2 или увеличении числа 
каналов): 
y=F(x,{Wi })+Ws x  
где Ws  — параметры свёртки 1×1, которая подгоняет форму shortcut-а к форме 
F(x). 
�
� 2. Смысл (интуитивно) 
Shortcut connection — это “байпас”, позволяющий сигналу проходить напрямую, 
минуя сложные преобразования. 
То есть: 
• часть информации (x) идёт напрямую, без искажений; 
• часть — трансформируется внутри блока и добавляется к исходной. 
Это: 
• сохраняет информацию, 
• упрощает оптимизацию, 
• стабилизирует градиенты, 
• даёт сети возможность “ничего не менять” (если F(x)≈0 → y≈x). 
�
� 3. Математическая интерпретация (через производные) 
Если рассмотреть градиент через shortcut: 
∂x∂L =∂y∂L ⋅(1+∂x∂F(x) )  
Благодаря “+1” градиент всегда может проходить к предыдущим слоям — даже 
если ∂x∂F(x) →0. 
→ Именно это решает проблему затухающих градиентов 
(в классических сетях без skip-путей, если F(x) — глубокая последовательность 
ReLU и Conv, градиент мог стремиться к нулю). 
�
� 4. Сравнение с обычной сетью 
Обычная сеть 
y=F(x) 
сеть должна выучить полную 
функцию 
градиенты проходят только через 
F 
�
� 5. Ключевая идея 
Residual сеть 
y=x+F(x) 
сеть учит только "дельту" — что нужно 
добавить/убрать 
часть градиента идёт напрямую через "+ x" 
Residual connection не даёт информации “забыться” и не даёт градиенту 
исчезнуть. 
4. Структура Residual Block 
�
� 1. Базовый блок (Basic / Identity Block) 
Используется в ResNet-18 и ResNet-34. 
Структура: 
x ──► [Conv3x3 → BN → ReLU → Conv3x3 → BN] ──► + ──► ReLU ──► y 
↑_______________________________│ 
• Conv3x3 — извлекает признаки, сохраняет размер (stride=1) 
• BN — нормализует выход для стабильного обучения 
• ReLU — нелинейность 
• Shortcut — просто x, если размеры совпадают 
• ReLU после сложения — финальная активация блока 
Особенности: 
• Количество каналов не меняется 
• Spatial size не меняется 
�
� 2. Bottleneck-блок 
Используется в ResNet-50/101/152 для уменьшения вычислительной нагрузки. 
Структура: 
x ──► [Conv1x1 ↓C_reduced → BN → ReLU 
Conv3x3 → BN → ReLU 
Conv1x1 ↑C_out → BN] ──► + ──► ReLU ──► y 
↑_____________________________│ 
• Conv1x1 (сжатие) — уменьшает количество каналов (уменьшение 
параметров) 
• Conv3x3 — основная свёртка на уменьшенном числе каналов 
• Conv1x1 (восстановление) — возвращает нужное количество каналов 
• Shortcut — identity или Conv1x1, если меняется число каналов или размер 
Плюсы: 
• Меньше вычислений, чем три 3x3 Conv на полном размере 
• Легче строить глубокие сети (ResNet-50 и глубже) 
�
� 3. Convolutional Block (Projection) 
Используется при изменении размеров: 
• Shortcut заменяется на 1x1 Conv, чтобы совпадали размеры каналов и H×W. 
• Обычно stride=2 для downsampling. 
�
� 4. Идея в одном предложении 
Residual Block = путь для оригинального сигнала + путь для трансформации 
(F(x)), где трансформация может быть простым блоком Conv3x3 или bottleneck
цепочкой с Conv1x1. 
5. Bottleneck-блок и его причины 
�
� 1. Причины использования 
1. Снижение вычислительной нагрузки 
a. Без bottleneck три 3×3 Conv на 256 каналов требуют огромного 
количества параметров и операций. 
b. Сжатие через 1×1 Conv снижает число каналов для 3×3 Conv, что 
экономит вычисления. 
2. Возможность строить очень глубокие сети 
a. ResNet-50, ResNet-101, ResNet-152 используют bottleneck, чтобы 
увеличить глубину без взрывного роста параметров. 
3. Сохраняет representational power 
a. 1×1 Conv позволяет линейно комбинировать каналы, 3×3 Conv 
извлекает spatial-признаки, второй 1×1 Conv восстанавливает число 
каналов. 
b. Таким образом блок остаётся мощным и эффективным. 
�
� 2. Сравнение с Basic Block 
Блок 
Basic (ResNet
18/34) 
Bottleneck 
(ResNet-50+) 
Кол-во Conv 
2 Conv3x3 
1x1+3x3+1x1 
Основные 
каналы 
как вход 
сначала 
меньше → потом 
восстановление 
Вычисления 
больше при 
больших каналах 
меньше, чем 3×3 
на полных 
каналах
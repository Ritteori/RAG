6. Функция потерь VAE (reconstruction + KL divergence) 
VAE обучается двум вещам одновременно: 
1. Хорошо восстанавливать вход (как обычный AE). 
2. Делать латентное пространство регулярным — т.е. чтобы 
латентные векторы z распределялись плавно и осмысленно, а 
не хаотично. 
�
� Формула лосса 
Общий вид: 
Более строго: 
L=Eq(z∣x) [−logp(x∣z)]+DKL (q(z∣x)∥p(z))  
�
� 1. Reconstruction Loss 
Это — наказание за плохое восстановление. 
Он измеряет, насколько сильно реконструированное изображение x^ 
отличается от исходного x. 
Типичные варианты: 
• Для изображений: 
E[(x−x^)2](MSE)  
• Или если decoder даёт распределение Бернулли (для пикселей в 
[0,1]): 
−∑i [xi logx^i +(1−xi )log(1−x^i )]  
(бинарная кросс-энтропия) 
Интуиция: 
Этот член заставляет decoder выдавать картинки, похожие на 
оригинал. 
�
� 2. KL Divergence 
DKL (q(z∣x)∥p(z))  
Это регуляризатор, который заставляет распределение скрытых 
переменных q(z∣x) быть похожим на приор p(z), 
обычно p(z)=N(0,I). 
Если бы мы оставили только reconstruction loss — VAE превратился бы 
в обычный AE, и z стали бы “разбросанными”, хаотичными. 
KL заставляет модель “прижимать” распределения к нормальному, 
чтобы пространство было: 
• плавным (smooth), 
• континуальным (можно интерполировать между точками), 
• заполняющим (decoder понимает даже новые, не виденные 
ранее z). 
7. Что такое ELBO и почему VAE его оптимизирует 
�
� 1. Начнем с цели 
VAE хочет максимизировать вероятность наблюдаемых данных — 
то есть научиться порождать x, похожие на реальные. 
Формально: 
Но! 
Это интеграл по всем возможным z, который невозможно 
вычислить напрямую — потому что p(x∣z) сложная нелинейная 
функция (decoder), и интеграл по всему z не берётся аналитически. 
Поэтому умножаем и делим под знаком интеграла на q(z∣x): 
Неравенство Йенсена говорит, что: 
�
� 2. Вводим аппроксимацию (вариационный вывод) 
Чтобы обойти это, мы вводим аппроксимацию: 
q(z∣x)  
— это распределение, которое приближает настоящий апостериор 
p(z∣x). 
То есть: 
• Истинное p(z∣x) мы не можем посчитать. 
• Поэтому encoder учится аппроксимировать его своим q(z∣x). 
�
� 3. Разложение log p(x) 
Берём логарифм правдоподобия: 
где: 
�
� 4. Ключевая идея 
•  
• Значит: 
ELBO≤logp(x)  
То есть ELBO — это нижняя оценка (lower bound) на логарифм 
правдоподобия. 
А значит, максимизация ELBO примерно эквивалентна 
максимизации лог-правдоподобия данных. 
�
� 5. Что внутри ELBO 
Именно это и есть знакомая нам функция потерь VAE (с минусом, если 
мы минимизируем): 
LVAE =−ELBO  
�
� 6. Интуитивно 
• ELBO = "насколько хорошо модель объясняет данные + 
насколько она не отклоняется от нормального prior’а". 
• VAE не просто восстанавливает данные, 
а максимизирует нижнюю границу правдоподобия — то есть 
делает вероятностную аппроксимацию. 
�
� 7. Кратко как на собесе: 
ELBO (Evidence Lower BOund) — это нижняя граница логарифма 
правдоподобия данных. 
Так как напрямую вычислить logp(x) невозможно, VAE оптимизирует 
ELBO, 
которая включает два члена: 
Максимизируя ELBO, мы фактически обучаем модель, которая и 
хорошо восстанавливает данные, и умеет порождать новые, 
осмысленные примеры. 
8. Роль prior распределения p(z) 
В VAE приор p(z) — это то, каким мы хотим видеть распределение 
латентных переменных. 
Обычно его берут простым: 
p(z)=N(0,I)  
(многомерное нормальное распределение с нулевым средним и 
единичной дисперсией). 
�
� Зачем он нужен 
1. Регуляризация латентного пространства — 
мы заставляем все латентные векторы “прижиматься” к 
нормальному распределению, 
чтобы z не расползались как попало. 
2. Генеративность — 
при генерации мы можем просто сэмплировать z ∼ N(0, I), 
и декодер выдаст осмысленные новые примеры. 
3. Контроль и интерполяция — 
из-за нормального prior’а пространство получается “плотным” и 
гладким: 
можно двигаться между точками в z-пространстве, и переходы 
будут плавные. 
9. Проблема posterior collapse 
Ситуация, когда encoder полностью игнорируется, и латентные 
переменные z не несут информации о x. 
То есть decoder просто учится восстанавливать данные без 
использования z — модель фактически превращается в обычный AE, 
а KL-дивергенция “съедает” весь сигнал от encoder’а. 
�
� Почему возникает 
1. KL term слишком большой → encoder “тянется к prior” слишком 
сильно. 
2. Decoder мощный (например, глубокая сеть) → может 
восстанавливать x сам, без z. 
�
� Как проявляется 
• z ≈ N(0, I) для всех x 
• Реконструкция работает, но латентное пространство пустое 
• Генерация новых данных теряет смысл 
�
� Как с этим бороться 
1. KL annealing / warm-up — постепенно увеличиваем вес KL в 
лоссе. 
2. Слабее decoder — чтобы модель была вынуждена использовать 
z. 
3. β-VAE с β<1 на старте — даёт свободу encoder’у. 
�
� Кратко как на собесе: 
Posterior collapse — это когда латентные переменные z перестают 
нести информацию о x. 
Возникает из-за сильной KL-регуляризации или слишком мощного 
decoder’а. 
Решают через KL-annealing, ослабление decoder или контроль веса β. 
10. Почему latent space у VAE “smooth” 
Потому что VAE обучается делать распределение скрытых 
переменных z непрерывным и близким к нормальному 
распределению N(0,I).За счёт этого: 
• Похожие входы x кодируются в близкие точки z, 
• Decoder учится восстанавливать похожие объекты из соседних 
точек, 
• А KL-дивергенция не даёт распределению «расползаться» и 
оставлять дыры в латентном пространстве. 
Поэтому, если мы возьмём две точки в latent space и будем плавно 
интерполировать между ними, decoder будет порождать 
осмысленные и постепенно меняющиеся примеры — это и есть 
“smoothness”.
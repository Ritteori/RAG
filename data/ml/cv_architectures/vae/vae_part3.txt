11. β-VAE и влияние коэффициента β 
β-VAE — это модификация VAE, где функция потерь принимает вид 
L=Reconstruction Loss+β⋅KL(q(z∣x)∥p(z))  
Коэффициент β регулирует баланс между точностью восстановления и 
степенью «дисциплинированности» латентного пространства: 
•   β > 1 — усиливаем регуляризацию, латентное пространство 
становится более disentangled, но качество реконструкции 
падает; 
•   β < 1 — ослабляем влияние KL, модель ведёт себя ближе к 
обычному AE (лучше восстанавливает, но теряет структуру 
пространства). 
Проще говоря: β управляет компромиссом между “понятным” 
латентным пространством и качеством восстановления. 
12. Практические приёмы стабилизации обучения (KL annealing, 
warm-up) 
�
� Проблема 
При старте обучения VAE KL-дивергенция начинает доминировать, и 
энкодер быстро «схлопывается» в среднее распределение 
→ модель не передаёт информацию через z → posterior collapse. 
�
� 1. KL Annealing 
Идея простая: 
не включать KL сразу на полную мощность, а постепенно 
увеличивать её вес β от 0 до 1 (или другого значения). 
Формула: 
L=Reconstruction Loss+βt ⋅KL(q(z∣x)∥p(z))  
где βt  плавно растёт с эпохами (например, линейно). 
Интуиция: 
• Сначала модель учится восстанавливать (как AE). 
• Потом постепенно начинает учитывать регуляризацию по 
распределению. 
Результат: 
• Энкодер не схлопывается. 
• Реконструкция стабильнее. 
�
� 2. KL Warm-up 
Это частный случай KL annealing: 
• Несколько первых эпох (например, 10–30) KL-член вообще не 
учитывается (β=0). 
• Потом он включается и растёт. 
Пример: 
beta = min(1.0, epoch / warmup_epochs) 
�
� 3. Free Bits / Threshold Trick 
• Вводится минимальный порог на KL: 
• Это позволяет некоторым латентным переменным не «умирать», 
пока другие сильно наказываются KL. 
�
� 4. Reconstruction loss tuning 
• Вместо MSE можно использовать BCE или perceptual loss 
(например, через VGG features). 
• Это помогает сохранить детали и избежать “мыла”. 
�
� 5. BatchNorm / LayerNorm / SpectralNorm 
• Нормализация в энкодере и декодере стабилизирует 
распределения активаций, 
что уменьшает разброс σ и делает обучение более устойчивым.
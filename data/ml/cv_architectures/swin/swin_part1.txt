4. SWIN 
1. Мотивация Swin: зачем нужны локальные окна внимания и shift 
Мотивация Swin Transformer связана с ключевым ограничением ViT — 
квадратичной сложностью O(N²) по числу патчей, где каждый патч 
взаимодействует со всеми остальными. Это делает ViT плохо 
масштабируемым на высоких разрешениях (например, 1024×1024). 
Swin решает это двумя идеями: 
1. Локальные окна внимания (Window-based Attention) — 
внимание вычисляется не по всем патчам, а только внутри 
небольших локальных окон (например, 7×7). Это снижает 
сложность с O(N²) до O(M²), где M — размер окна. 
2. Shifted Windows (сдвинутые окна) — окна на соседних слоях 
сдвигаются (обычно на половину размера окна), чтобы 
позволить информации перетекать между окнами и захватывать 
глобальный контекст без роста вычислительной сложности. 
В итоге Swin: 
• сохраняет локальность и иерархичность как у CNN; 
• имеет линейную масштабируемость по размеру изображения; 
• остаётся совместим с downstream задачами (детекция, 
сегментация). 
2. Window-based Multi-Head Self-Attention (W-MHSA): принцип 
работы 
В Window-based Multi-Head Self-Attention (W-MHSA) внимание 
считается локально внутри окон фиксированного размера 
(например, 7×7 патчей), а не по всем патчам изображения, как в ViT. 
Принцип: 
• Входное изображение (bs, H, W, C) разбивается на 
непересекающиеся окна. 
• Каждый блок внимания обрабатывает только патчи внутри 
своего окна — то есть Q, K, V формируются локально, и матрица 
внимания имеет размер (M², M²), где M — размер окна 
(например, 7×7 → 49 позиций). 
• Это резко снижает вычислительную сложность: вместо O((HW)²) 
получаем O((M²) × (HW / M²)) = O(HW), то есть линейную по числу 
патчей. 
• Механизм внимания внутри окна полностью аналогичен 
обычному MHSA (QKV, softmax, weighted sum, concat, линейная 
проекция). 
Интуитивно: 
Каждое окно учится извлекать локальные взаимосвязи, как свёртка, 
но с большей гибкостью (т.к. внимание не фиксировано и может 
фокусироваться на произвольных позициях внутри окна). 
3. Shifted Window Attention: зачем сдвигать окна, как это влияет на 
глобальный контекст 
Shifted Window Attention (SWA) решает проблему того, что обычное 
Window Attention работает независимо внутри непересекающихся 
окон и не позволяет обмениваться информацией между ними. 
Как решается: 
• На каждом втором блоке окна сдвигаются (обычно на половину 
размера окна, например, на 3 патча при окне 7×7). 
• После сдвига новые окна перекрывают границы предыдущих — 
это создаёт перекрёстные связи между соседними областями. 
• Для корректности вычислений (т.к. после сдвига часть патчей 
может «выйти за границы»), используется attention mask, 
который блокирует взаимодействие патчей из разных окон при 
паддинге. 
Интуитивно: 
• Первый блок видит только локальные зависимости (локальные 
окна). 
• Второй (со сдвигом) связывает эти окна между собой, 
распространяя информацию глобальнее. 
Результат: 
• Получается эффект, аналогичный stride = 1 у свёрток: локальные 
receptive fields частично перекрываются, обеспечивая 
глобальный контекст при линейной сложности. 
4. Patch Merging и иерархическая структура признаков 
Patch Merging — это аналог операции downsampling / pooling в CNN, 
позволяющая Swin Transformer строить иерархические 
представления признаков, подобно пирамиде фичей в ResNet или 
FPN. 
Как работает: 
• После нескольких слоёв внимания внутри окна 
пространственное разрешение уменьшается. 
• В Patch Merging соседние патчи (обычно 2×2) объединяются 
(concat) по признаковому измерению и затем проходят 
линейную проекцию, уменьшая размерность (например, 
4×embed_dim → 2×embed_dim). 
• Это уменьшает разрешение фичей (H, W) в 2 раза, но 
увеличивает глубину каналов, создавая иерархию уровней 
признаков. 
Зачем нужно: 
• ViT использует одинаковое разрешение на всех уровнях (flat), что 
не подходит для dense-задач (детекция, сегментация). 
• Swin формирует multi-scale представление: нижние слои — 
локальные мелкие детали, верхние — глобальные 
семантические признаки. 
• Это позволяет использовать Swin как универсальный backbone 
для любых задач CV (подобно CNN). 
Интуитивно: 
Patch Merging = pooling + channel expansion в трансформерном виде. 
5. Разница между Swin и ViT по архитектуре и вычислительной 
сложности 
• Архитектура: 
ViT — плоский трансформер, одинаковое разрешение на всех 
слоях, внимание между всеми патчами. 
Swin — иерархический, с Patch Merging, как CNN: низкие уровни 
— локальные детали, верхние — семантика. 
• Внимание: 
ViT — глобальное self-attention (O(N²) по числу патчей). 
Swin — локальное внимание в окнах (O(M²)), плюс Shifted 
Windows для межокнового обмена. 
• Масштабируемость: 
Swin линейно масштабируется по размеру изображения → 
подходит для high-res задач (детекция, сегментация), 
ViT — плохо масштабируется при росте разрешения. 
• Вычислительная сложность: 
ViT: O((HW)²) 
Swin: O(HW) → на практике в 3–5× быстрее при равной точности. 
• Применение: 
ViT чаще используется для классификации, 
Swin стал универсальным backbone для CV-задач (детекция, 
сегментация, depth estimation и др.). 
6. Особенности Residual connections и LayerNorm в Swin 
• В Swin используются Post-Norm residual blocks: сначала 
операция (Self-Attention или MLP), потом добавляется skip 
connection. 
→ это упрощает оптимизацию при глубокой архитектуре и 
ускоряет сходимость. 
• В некоторых улучшенных версиях (например, Swin v2) переходят 
на Pre-Norm для большей стабильности при обучении на 
больших batch size. 
• LayerNorm применяется дважды в каждом Swin-блоке — 
перед W-MSA и перед MLP (внутри residual ветвей), что 
стабилизирует распределение активаций. 
• В отличие от ViT, где LN применяется к глобальному набору 
токенов, здесь LN работает локально по окнам, что делает 
обучение устойчивым даже при очень больших изображениях. 
• Residual connection помогает сохранить локальную информацию 
между блоками и предотвращает деградацию сигнала при 
последовательных Window/Shifted Window слоях.
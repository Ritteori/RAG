7. Forward-pass через Swin блок 
Forward-pass в одном Swin-блоке: 
LayerNorm → Window Partition: 
Входные фичи нормализуются, затем делятся на окна 
фиксированного размера (например, 7×7). 
Если это Shifted-блок — предварительно сдвигаем окна (torch.roll). 
W-MHSA или SW-MHSA: 
Для каждого окна вычисляется Multi-Head Self-Attention (локально). 
Если окна были сдвинуты — применяется attention mask, чтобы не 
пересекать границы. 
Reverse Shift + Window Reverse: 
После внимания окна собираются обратно в глобальную карту 
признаков, а при shift восстанавливается исходная позиция патчей. 
Residual connection: 
Результат складывается с исходным входом блока (skip). 
LayerNorm + MLP: 
Снова нормализация, двухслойный feed-forward MLP (обычно с GELU), 
потом ещё один skip connection. 
Итог: 
Выход имеет ту же форму, что и вход (bs, H, W, C), но с обновлёнными 
признаками, которые теперь учитывают локальный и межокновой 
контекст. 
8. Использование Swin для dense prediction задач (детекция, 
сегментация) 
Swin Transformer адаптируется под задачи детекции и сегментации 
благодаря тому, что его архитектура уже иерархична: 
• при переходе между стадиями (patch merging) уменьшается 
spatial resolution и увеличивается число каналов — как в CNN; 
• это позволяет извлекать признаки на разных уровнях (мелкие → 
крупные объекты), что удобно для Feature Pyramid Network 
(FPN); 
• поэтому Swin используется как backbone в моделях вроде Mask 
R-CNN, UPerNet и др.; 
• на выходе формируются 3–4 карты признаков разных масштабов 
(C2, C3, C4, C5), которые легко подключаются в детектор или 
сегментатор. 
Именно это делает Swin универсальным backbone для detection и 
segmentation. 
9. Регуляризация и обучение (DropPath, warmup, аугментации) 
❖ DropPath (Stochastic Depth) 
➢ Это аналог dropout, но для residual-путей. 
➢ В каждом блоке случайно "отключаются" целые 
остаточные соединения (residual branch) с некоторой 
вероятностью. 
➢ Это предотвращает переобучение и действует как 
ансамблирование — при каждом проходе сеть использует 
немного разный набор путей. 
➢ Особенно важно в глубоких ViT-подобных моделях (Swin 
может быть до 200+ слоёв). 
o Вероятность обычно увеличивается по мере глубины 
(например, от 0.0 в начале до 0.2–0.3 в конце). 
• Learning rate warmup 
o На старте обучения лосс и градиенты могут "взрываться", 
потому что параметры не сбалансированы. 
o Поэтому первые несколько эпох (5–10) lr линейно 
повышается от 0 до целевого значения. 
o Это стабилизирует градиенты и помогает сети начать 
обучение без коллапса. 
• Аугментации (Data Augmentation) 
o Swin обучался с мощным набором аугментаций (как и ViT): 
▪ RandAugment (случайные комбинации цветовых и 
геометрических преобразований), 
▪ Mixup (смешивание изображений и меток), 
▪ CutMix (вырезание фрагментов из других 
изображений), 
▪ Random Erasing (затирание случайных участков). 
o Это помогает обучить более инвариантные и устойчивые 
представления. 
• Оптимизация и лосс 
o Обычно используется AdamW (Adam + weight decay), 
o Weight decay ≈ 0.05–0.1, 
o Label smoothing ≈ 0.1 — чтобы не переуверяться в 
предсказаниях. 
10. 
Вариации Swin (Tiny, Small, Base, Large): различия и 
области применения 
Версия 
Tiny 
(Swin-T) 
Small 
Depth 
(слои) 
2–2–6–2 
Heads Hidden 
Dim 
3–6 
(Swin-S) 2–2–18–2 3–6 
Base 
96 
96 
(Swin-B) 2–2–18–2 3–12 128 
Large 
(Swin-L) 
2–2–18–2 3–16 
192 
Особенности / область применения 
Малые модели для мобильных 
устройств, low-res изображения 
(224×224), быстро обучаются. 
Баланс точности и скорости, подходит 
для обычных CV-задач, ImageNet-1k. 
Стандартный выбор для большинства 
задач, классификация, детекция, 
сегментация. 
Высокая точность на больших 
датасетах, требуется мощный GPU, 
используется в детекции, сегментации 
и fine-tuning на крупных датасетах. 
5. DETR 
1. Мотивация DETR: зачем нужен transformer для detection 
DETR (DEtection TRansformer) появился как попытка упростить 
архитектуру object detection, заменив сложный каскад (RPN, anchor
генерация, NMS, post-processing) на чисто end-to-end схему. 
Что решает DETR: 
• Anchor-free и NMS-free: 
Больше не нужно подбирать якоря, пороги IoU и вручную 
убирать дублирующиеся боксы. Модель сама учится 
"распределять" предсказания. 
• Глобальный контекст: 
Трансформер видит всё изображение целиком (через self
attention), а не ограниченные receptive fields, как CNN. 
• Простота архитектуры: 
Вместо множества отдельных компонентов (backbone, RPN, ROI 
Head) — один энкодер-декодер трансформер с loss-функцией 
Hungarian matching. 
• Скалируемость и универсальность: 
DETR легко адаптировать под сегментацию, captioning и другие 
vision-задачи без переизобретения архитектуры. 
Минус: 
медленная сходимость (обучение дольше, чем у Faster R-CNN), что 
позже решали в Deformable DETR и других версиях. 
2. Архитектура DETR: backbone + transformer encoder-decoder 
1
️
⃣ Backbone 
• DETR использует CNN-бэкбон (например, ResNet-50 или 
ResNet-101) для извлечения feature map из изображения. 
→ Это эквивалентно тому, как CNN делает низкоуровневое 
кодирование признаков. 
2
️
⃣ Flatten + Positional Encoding 
• Feature map после CNN имеет форму [C, H, W]. 
→ Его “распрямляют” в последовательность из H×W токенов 
(аналогично патчам в ViT). 
• Добавляются позиционные эмбеддинги, чтобы модель знала, 
где расположен каждый патч — ведь self-attention сам по себе 
не чувствителен к порядку. 
3
️
⃣ Transformer Encoder 
• Это классический энкодер, состоящий из слоёв self-attention + 
FFN. 
• Он обрабатывает все патчи одновременно и выстраивает 
глобальные взаимосвязи между объектами на изображении 
(например, части одной машины, контекст между предметами). 
4
️
⃣ Transformer Decoder 
• Главная инновация DETR. 
• На вход декодера подаются object queries — обучаемые векторы 
(например, 100 штук), каждый из которых “ищет” свой объект. 
• Декодер использует cross-attention между этими query и 
выходами энкодера (feature map), чтобы каждый query научился 
фокусироваться на конкретном объекте сцены. 
5
️
⃣ Output Head 
• Каждый query после декодера идёт в предсказатель (FFN), 
который выдаёт: 
o координаты бокса (через sigmoid, чтобы ограничить 0–1), 
o вероятность класса (включая “нет объекта”). 
6
️
⃣ Matching и Loss 
• В DETR используется Hungarian matching для сопоставления 
предсказанных query с истинными объектами (один к одному). 
• Лосс состоит из: 
o L1 для боксов, 
o GIoU loss для перекрытия, 
o Cross-entropy для классов. 
⚙
️ Итого — Pipeline DETR: 
Image 
↓ 
CNN Backbone (ResNet) 
↓ 
Flatten + Positional Encoding 
↓ 
Transformer Encoder 
↓ 
Transformer Decoder (object queries) 
↓ 
FFN Head → [BBox + Class] 
↓ 
Hungarian Matching + Loss 
3. Object Queries: что это и зачем нужны 
Классические детекторы (YOLO, Faster R-CNN) используют anchor
based подход: 
• Они заранее задают множество якорных прямоугольников 
(anchors) разных размеров и соотношений сторон. 
• Модель предсказывает смещения и классы для каждого anchor
а. 
• Потом через NMS (non-maximum suppression) выбираются 
финальные боксы. 
DETR (DEtection TRansformer) полностью отказывается от этой схемы. 
Вместо фиксированных якорей он использует object queries — это 
обучаемые вектора, которые проходят через декодер 
трансформера. 
Каждый query учится “фокусироваться” на каком-то объекте в 
изображении. 
Модель напрямую предсказывает ограничивающие рамки и классы 
для этих queries — без NMS и без anchor-ов. 
Сопоставление предсказанных боксов с истинными выполняется с 
помощью Hungarian matching (оптимальное назначение). 
Итоговое отличие: 
DETR заменяет фиксированные anchor-ы и NMS на end-to-end 
обучение через attention + Hungarian matching. 
Благодаря этому детекция становится “чистой” задачей 
трансформера, без эвристик и ручного подбора якорей. 
4. Transformer Encoder: входные признаки из backbone 
Что подаёт backbone: 
backbone (например, ResNet) выдаёт tensor признаков 
feat: (bs, C, H, W) 
— обычно это карта признаков с уменьшенным spatial (например, 
H=W=H_in/32). 
Подготовка к энкодеру: 
• reshape / flatten → переводим в последовательность токенов: 
x = feat.permute(0,2,3,1).reshape(bs, H*W, C)   # (bs, N, 
C) , N = H*W 
• добавляем positional embeddings: pos_embed: (1, N, C) и 
делаем 
x = x + pos_embed 
(positional — learnable или sin/cos; необходимы, т.к. self-attention 
инвариантен к перестановкам). 
Note: в DETR CLS-токен не используется — энкодер служит только 
для формирования локально-глобальных признаков, а не для 
агрегации всего изображения. 
Структура энкодера: 
• Энкодер — стек L блоков, каждый блок: 
x = x + MSA(LayerNorm(x)) 
x = x + MLP(LayerNorm(x)) 
где MSA = Multi-Head Self-Attention, MLP = двухслойный FFN (обычно 
dim→4·dim→dim), + residuals + LayerNorm. 
• После всех блоков получаем: 
memory = x  # (bs, N, C) 
— это «карта признаков в виде последовательности», обогащённая 
глобальным контекстом. 
Роль энкодера в DETR: 
• Собрать и передать глобально согласованные представления 
каждого spatial-позиции. 
• Эти представления служат источником информации для 
decoder-queries: декодер через cross-attention «читает» memory 
и решает, какие объекты описывать. 
Ключевые инженерные замечания: 
• positional embedding критичны (иначе внимание не знает о 
расположении). 
• Использовать Pre-LayerNorm + residual для стабильности при 
глубине. 
• Часто применяют conv-projection на выходе backbone, чтобы 
подогнать C под hidden_dim энкодера. 
5. Transformer Decoder: как работает с object queries 
Transformer Decoder в DETR принимает: 
• object queries — обучаемые вектора (по количеству объектов, 
которые мы хотим детектировать); 
• encoder output — признаки изображения (после self-attention). 
Далее в каждом блоке декодера: 
1. Проходит self-attention между самими queries (чтобы они не 
конфликтовали друг с другом). 
2. Проходит cross-attention, где 
a. Q = object queries, (bs, num_queries, hidden_dim) 
b. K, V = encoder features. (2,bs, num_patches, hidden_dim) 
Так формируется связь между каждым запросом и частью 
изображения. 
3. После — MLP предсказывает для каждого query: 
a. координаты бокса, 
b. класс объекта (или фон). 
В итоге каждый query становится «умным якорем», который сам 
находит объект, а не ищет среди заранее заданных anchor boxes (bs, 
num_queries, hidden_dim). 
17. 
Плюсы и минусы DETR на практике 
✅ Плюсы DETR 
1. End-to-End обучение 
— Без RPN, anchor’ов и NMS. 
— Модель учится напрямую сопоставлять объекты и 
предсказания. 
— Минимум ручных эвристик и постобработки. 
2. Простота пайплайна 
— Одна архитектура решает всё: detection, segmentation, 
keypoints. 
— Код проще и чище, легче отлаживать. 
3. Универсальность 
— Легко адаптируется под сегментацию (Panoptic / Instance 
DETR). 
— Работает и в видео, и в 3D (Video-DETR, 3D-DETR). 
— Можно подключать к любому backbone (ResNet, Swin, 
ConvNeXt, ViT). 
4. Интерпретируемость 
— Каждый query соответствует конкретному объекту → легко 
анализировать, кто что “увидел”. 
5. Современный потенциал 
— На базе DETR строят топовые модели (например, Deformable 
DETR, DN-DETR, DINO), 
которые догнали и превзошли классические CNN по качеству. 
❌ Минусы DETR 
1. Медленная сходимость 
— Оригинальный DETR требовал 500+ эпох, чтобы достичь 
качества Faster R-CNN. 
— Причина: сложность обучения matching и attention
механизма. 
2. Проблемы с малыми объектами 
— Attention по крупным патчам плохо улавливает мелкие детали. 
— Нет иерархии признаков (как в FPN у CNN). 
3. Высокие требования к вычислениям 
— Attention = O(N²) по числу патчей → плохо масштабируется на 
большие изображения. 
— Много GPU-памяти, медленный inference. 
4. Чувствительность к гиперпараметрам 
— Warmup, lr decay, matching weights сильно влияют на 
качество. 
— Ошибка в лоссе → деградация качества. 
5. Сложность дебага 
— Query-механизм не так интуитивен, как anchors: 
трудно понять, почему конкретный объект “пропал” или был 
“переобучен”. 
�
� Итог 
DETR — мощная и «чистая» идея, убравшая все костыли классических 
детекторов, 
но требующая больше вычислительных ресурсов, данных и времени 
обучения. 
На практике почти всегда используют усовершенствованные версии 
(Deformable DETR, DINO), 
а не оригинальный DETR. 
18. 
Использование в сегментации: DETR + MaskFormer 
�
� 1. Идея 
DETR изначально решает детекцию объектов через предсказание 
боксов и классов. 
Чтобы перейти к сегментации, нужно не просто знать где объект, но 
и какие пиксели ему принадлежат. 
→ Поэтому поверх архитектуры DETR добавляется масочный 
предсказатель (mask head), который генерирует карту 
принадлежности пикселей. 
�
� 2. Архитектура MaskFormer 
MaskFormer — это универсальная сегментационная модель, 
основанная на DETR, 
которая заменяет жёсткое «класс на пиксель» на предсказание 
набора масок и их классов. 
Основная идея: 
1. DETR даёт object queries и feature maps из encoder. 
2. Decoder выдаёт для каждого query: 
a. вектор признаков (query embedding) 
b. предсказание класса (class logits) 
3. Каждый query используется для построения маски через 
attention map: 
a. mask = sigmoid( linear(F) * queryᵀ ) 
b. где F — feature map из backbone. 
На выходе: 
• (bs, N_queries, H, W) — набор масок 
• (bs, N_queries, num_classes) — классы для каждой маски 
�
� 3. MaskDETR и Panoptic-версии 
MaskDETR — это более «детекционный» вариант: 
• использует Deformable DETR как основу; 
• предсказывает боксы и маски одновременно (instance 
segmentation); 
• улучшен loss и attention, быстрее сходит. 
Panoptic DETR / Mask2Former: 
• объединяют instance и semantic segmentation в одном 
фреймворке; 
• используют общие mask embeddings для всех задач. 
�
� 4. Преимущества подхода DETR для сегментации 
Единый механизм для разных типов задач: instance / semantic / 
panoptic. 
Нет постпроцессинга (без NMS, без post-merge). 
Маски получаются напрямую из attention-карт — объяснимость 
высокая. 
Простая интеграция backbone (Swin, ConvNeXt, ViT). 
�
� 5. Минусы 
Всё те же проблемы, что и у DETR — долгая сходимость, 
чувствительность к малым объектам. 
Маски иногда «пересекаются», особенно на сложных сценах. 
Вычислительно дорог при больших разрешениях. 
�
� 6. Выход модели 
• Mask logits: (bs, N_queries, H, W) 
• Class logits: (bs, N_queries, num_classes) 
• (Опционально) Bounding boxes: (bs, N_queries, 4) 
�
� Итог 
MaskFormer и MaskDETR превратили DETR из просто детектора в 
универсальный сегментатор, 
который учится маскам как наборам объектов, а не просто 
пиксельной классификации. 
Это сделало его одной из ключевых архитектур для современных 
задач сегментации. 
19. 
Применение pretraining и transfer learning 
�
� 1. Почему DETR требует pretraining 
DETR обучается end-to-end без якорей (anchor-free) и без NMS, 
а значит, модель должна сама выучить: 
• как выделять объекты, 
• как правильно их локализовать, 
• как интерпретировать queries. 
Без предобученного backbone (ResNet, Swin и т.д.) 
модель сходится крайне медленно (до 300 эпох на COCO) 
и часто не выучивает локальные признаки с нуля. 
�
� 2. Backbone pretraining 
Обычно backbone (например, ResNet-50, Swin-T или ConvNeXt) 
предобучается на ImageNet-1k / 21k. 
Зачем: 
• Обеспечивает уже готовое восприятие базовых текстур, форм и 
краёв. 
• DETR может сосредоточиться на обучении внимания и matching
а, 
а не на изучении низкоуровневых признаков. 
Технически: 
• Backbone берётся с весами pretrained=True. 
• Только Transformer encoder–decoder и head обучаются с нуля. 
• Иногда backbone замораживается на первых эпохах (до 
стабилизации attention). 
�
� 3. Transfer learning между задачами 
После предобучения DETR на крупном наборе (например, COCO), 
его можно дообучать на других задачах: 
Целевая задача 
Object detection 
Instance 
segmentation 
Panoptic 
segmentation 
Подход 
fine-tune decoder и heads 
добавить mask head 
Пример модели 
DETR → Deformable 
DETR 
MaskDETR, 
Mask2Former 
объединить semantic + instance Panoptic DETR 
Domain adaptation адаптация через fine-tuning 
backbone 
medical DETR, aerial 
DETR 
Главная идея: сохраняем архитектуру attention + queries, 
меняем только выходные головы под новую задачу. 
�
� 4. Multi-scale и feature pretraining 
Некоторые версии (Deformable DETR, DINO-DETR) 
используют pretraining на multi-scale features, 
чтобы ускорить сходимость и улучшить детекцию малых объектов. 
• Encoder работает с фичами разных уровней (C3, C4, C5). 
• Query attention учится работать с несколькими разрешениями 
сразу. 
• Такой pretraining ускоряет обучение в 3–5 раз. 
�
� 5. Использование больших моделей (ViT / Swin) 
DETR можно строить на любом backbone, включая Vision Transformer. 
Если взять, например, Swin-B с ImageNet-22k, 
то fine-tuning даёт SOTA-результаты на COCO и ADE20K. 
Эффект тот же, что и у LLM: 
Чем больше и богаче pretraining, тем быстрее и точнее fine-tuning. 
�
� Итог 
DETR критически зависит от предобученного backbone, 
и лучше всего переносится между задачами, 
где структура объектов и визуальные признаки похожи. 
Fine-tuning DETR на новых данных — стандартный способ 
адаптировать его под любую задачу detection / segmentation 
без изменения архитектуры. 
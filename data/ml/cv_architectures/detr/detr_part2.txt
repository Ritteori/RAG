6. Multi-Head Self-Attention и Cross-Attention в DETR 
Multi-Head Self-Attention (MSA) в DETR 
• Где используется: 
В энкодере — чтобы каждый патч (feature vector) "видел" 
остальные и формировал глобальный контекст. 
В декодере — чтобы object queries могли взаимодействовать 
между собой (избегая дублирования одних и тех же объектов). 
• Формы тензоров: 
Q,K,V∈R(B,N,D)  
где 
B — batch size, 
N — число токенов (патчей или queries), 
D — размерность скрытого пространства. 
• Назначение: 
MSA моделирует взаимосвязи внутри одного множества 
токенов — патчей или queries, улучшая их внутренний контекст. 
Cross-Attention (CA) 
• Где используется: 
Только в декодере. 
Здесь queries (Q) — обучаемые object queries (например, 100 
штук), 
а K, V — выход энкодера (feature map изображения). 
• Формы: 
Q∈R(B,Nq ,D)                        
K,V∈R(B,Nimg ,D)  
где Nq  — число object queries, 
Nimg  — число патчей (H×W после backbone). 
• Интерпретация: 
Каждый object query "спрашивает" у изображения: 
«Где на feature map есть что-то, похожее на мой объект?» 
Cross-attention вычисляет soft сопоставление между каждым query и 
всеми пикселями фичей изображения. 
Результат — признаки, специфичные для каждого потенциального 
объекта. 
⚙
️ Multi-Head аспект: 
Механизм разбивает внимание на несколько "голов", каждая из 
которых может фокусироваться на разных признаках (форма, цвет, 
контур, фон). 
Это позволяет одному query анализировать объект под разными 
углами. 
7. Positional Encoding: зачем и как используется 
Positional Encoding (позиционное кодирование) — добавляется, 
чтобы модель понимала расположение патчей в изображении, ведь 
трансформер не имеет информации о порядке элементов (патчи — 
это просто набор векторов). 
Без позиционной информации self-attention видит все токены как 
“множество”, где порядок не имеет значения — это разрушает 
пространственную структуру картинки. 
�
� В DETR: 
1. В Encoder: 
a. Каждому патчу (вектору из backbone, размер [B, N, D]) 
добавляется позиционный эмбеддинг того же размера [N, 
D]. 
b. Это помогает attention “понимать”, где каждый патч 
находится в пространстве (высота, ширина). 
Формула: 
zi=xi+piz_i = x_i + p_izi =xi +pi   
где xix_ixi  — вектор признаков патча, pip_ipi  — позиционный вектор. 
2. В Decoder: 
a. Позиционные эмбеддинги энкодера добавляются к K и V 
при cross-attention, чтобы queries знали, где в 
изображении искать объект. 
b. Query тоже могут иметь собственные learnable позиции, 
отражающие их “тип” объекта. 
�
� Итог: 
Positional encoding восстанавливает пространственное восприятие, 
без которого attention просто не знает, где “левая собака”, а где 
“правая часть экрана”. 
8. Предсказание боксов и классов: линейные слои после decoder 
После последнего блока Transformer Decoder мы имеем выход: 
Z∈R(B×Nq ×D) 
где: 
• B — batch size, 
• Nq  — количество object queries (например, 100), 
• D — размерность скрытого пространства (hidden dim, обычно 
256). 
�
� Две независимые “головы” (prediction heads): 
1. Классификация объектов 
a. Линейный слой: Linear(D, num_classes + 1) 
b. Выход: 
Pcls ∈RB×Nq ×(C+1)  
где +1 — класс “no object”. 
c. Используется softmax или logit-вход в cross-entropy. 
2. Регрессия координат бокса 
a. Линейный слой: Linear(D, 4) 
b. Выход: 
Pbbox ∈R (B×Nq ×4 ) 
координаты в формате (xcenter ,ycenter ,w,h). 
c. Обычно пропускается через sigmoid → все значения в 
[0,1], нормализованы относительно размера изображения. 
�
� Почему именно линейные: 
• Выход decoder уже содержит глобальную контекстную 
информацию, поэтому локальные свёртки не нужны. 
• Линейный слой просто проектирует скрытое представление в 
пространство классов и координат. 
�
� Что дальше: 
• На этапе обучения — применяется Hungarian matching между 
предсказаниями и GT-боксами, чтобы однозначно сопоставить 
query ↔ объект. 
• На инференсе — отбрасываются предсказания с высоким “no 
object” score. 
Итог: 
После decoder DETR выдаёт по каждому query скрытый вектор, 
который линейно проецируется в класс и координаты. Эти проекции 
полностью заменяют anchor-based head — один из ключевых 
признаков “anchor-free” подхода DETR. 
9. Hungarian Matching: принцип и зачем нужен 
• Входные данные 
o pred_logits — (bs, N_q, num_classes) 
o pred_boxes — (bs, N_q, 4) 
o target_labels — (num_targets,) 
o target_boxes — (num_targets, 4) 
• Вычисление cost matrix 
o Для каждого query i и объекта j считается стоимость 
сопоставления: 
• Применение алгоритма Hungarian 
o На основе cost matrix решается задача bipartite matching, 
которая минимизирует суммарную стоимость. 
o Результат: уникальное соответствие query ↔ объект. 
o Если N_q > N_objects, оставшиеся query назначаются 
как “no object”. 
• Использование сопоставления 
o После matching мы точно знаем, какой query соответствует 
какому GT объекту. 
o На основании этого строится loss: 
▪ CrossEntropy loss для классов. 
▪ L1 + GIoU для координат. 
o Loss суммируется по batch и по query. 
• Почему это заменяет NMS 
o В классических детекторах одна и та же область может 
генерировать несколько предсказаний → нужна NMS, 
чтобы убрать дубликаты. 
o В DETR каждый query уникально сопоставляется с объектом 
→ дубликаты исключены по определению, NMS не нужен. 
• Важно для реализации 
o Порядок GT объектов важен: labels[i] и boxes[i] 
должны соответствовать. 
o Все координаты нормализованы (0–1) относительно 
изображения. 
o Matching выполняется на каждом изображении в batch 
отдельно. 
• Выходные данные 
o matched_indices = [(query_idx_0, target_idx_0), (query_idx_1, 
target_idx_1), ...]
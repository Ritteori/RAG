10. 
Loss Function: комбинированный loss (class + bbox) 
�
� DETR Loss Function — структура 
1. Основная идея 
a. DETR использует один loss на каждый query, который 
сопоставлен с GT объектом через Hungarian Matching. 
b. Loss комбинированный: класс + bbox, иногда добавляют 
GIoU для точности бокса. 
2. Class Loss 
a. Для класса используем CrossEntropy: 
i. Включаем num_classes + 1 (последний класс = “no 
object”). 
ii. Формула для каждого query i: 
CE(pred_logits[i], target_label[i]) 
b. Для query, которые не сопоставлены с объектом — target = 
“no object”. 
3. BBox Loss 
a. L1 Loss по координатам: 
L1(pred_boxes[i], target_boxes[i]) 
b. GIoU Loss (Generalized IoU): 
i. Учитывает не только точность координат, но и 
пересечение/объём. 
ii. Особенно полезно, когда predicted box и GT немного 
смещены, но всё равно совпадают. 
4. Объединение 
a. Полный loss на один query: 
Loss_i = λ_cls * CE + λ_bbox * L1 + λ_giou * (1 - GIoU) 
b. λ — веса, обычно: 
i. λ_cls ≈ 1 
ii. λ_bbox ≈ 5 
iii. λ_giou ≈ 2 
5. Суммирование по batch и query 
a. Суммируем loss только по matched query, остальные с “no 
object” учитываются только в class loss. 
b. Потом усредняем по batch size. 
�
� Важные моменты 
• No object query влияет только на class loss. 
• Все query участвуют в loss, поэтому Hungarian Matching 
критически важен для правильного сопоставления. 
• В отличие от классических детекторов с NMS, здесь loss 
учитывает каждый query один раз. 
11. 
End-to-end обучение: отличие от RPN + ROI 
�
� DETR: End-to-End обучение 
1. Один этап 
a. DETR сразу берёт изображение → backbone → encoder-decoder → 
предсказания боксов и классов. 
b. Нет промежуточных стадий, всё обучается совместно. 
2. No anchor / no proposal 
a. В классических детекторах (Faster R-CNN, Mask R-CNN) есть RPN 
(Region Proposal Network), который генерирует тысячи candidate 
regions (якори). 
b. DETR использует object queries, которые уже “предназначены” для 
объектов, и нет необходимости в генерации якорей. 
3. Hungarian Matching вместо NMS 
a. Классические методы требуют Non-Max Suppression (NMS), чтобы 
убрать дублирующиеся предсказания. 
b. DETR использует Hungarian Matching: один query ↔ один GT объект → 
нет дублирующихся предсказаний. 
4. Прямая оптимизация loss 
a. Все параметры — backbone, encoder-decoder, линейные слои — 
обучаются одним loss. 
b. В RPN + ROI сначала обучается RPN, потом RoI Head → много 
отдельных шагов, иногда с freeze/unfreeze. 
�
� Итог 
Особенность 
DETR 
RPN + RoI (Faster R-CNN) 
Этапы обучения 
Anchors 
NMS 
Loss 
End-to-End (1 шаг) 
Нет 
Нет (Hungarian) 
Один общий 
Простота пайплайна Выше 
12. 
RPN + RoI (2+ шага) 
Да 
Да 
Разделённый (RPN loss + ROI 
loss) 
Ниже (больше инженерных 
шагов) 
Особенности градиентов и Backpropagation 
�
� 1. Единая градиентная цепочка 
• В DETR всё обучается end-to-end: 
o Backbone → Transformer Encoder → Transformer Decoder → 
Linear heads (классы + bbox) 
• Градиенты проходят сквозь все блоки сразу. 
• В отличие от RPN+RoI, где часто приходится отдельно обучать 
RPN и потом RoI Head, здесь один loss сразу корректирует все 
параметры. 
�
� 2. Self-Attention и Cross-Attention 
• Self-Attention (в encoder и decoder) и Cross-Attention создают 
многомерные связи между патчами и object queries. 
• Градиенты пропорционально распространяются через все 
патчи и все heads. 
• Благодаря residual connections и LayerNorm обучение 
стабильно даже при глубокой архитектуре. 
�
� 3. Hungarian Matching и влияние на градиенты 
• HM определяет сопоставление query ↔ GT объект. 
• Только matched query получают полноценный loss → только они 
генерируют градиенты. 
• Unmatched query участвуют только через class loss (“no object”), 
их bbox градиенты равны нулю. 
• Это предотвращает шум в градиентах от лишних query. 
�
� 4. Особенности MLP head и bbox loss 
• Линейные слои для bbox (обычно 4 значения) используют L1 / 
GIoU → градиенты идут напрямую в decoder. 
• MLP для классов (CE) также пропускает градиенты через decoder 
и encoder. 
�
� 5. Потенциальные проблемы 
• Градиентные коллапсы при больших learning rate → решается 
warmup. 
• Медленная сходимость → добавляют DropPath / Stochastic 
Depth и сильные аугментации. 
• Градиенты к “неактивным” query нулевые → помогает модели 
сосредоточиться на настоящих объектах. 
13. 
Регуляризация и стабилизация (dropout, warmup) 
�
� 1. Dropout / DropPath (Stochastic Depth) 
• Dropout отключает случайные нейроны в MLP и линейных слоях 
→ предотвращает переобучение. 
• DropPath / Stochastic Depth: вместо отдельных нейронов 
отключаются целые residual-ветви. 
o Особенно полезно для глубоких трансформеров 
(Encoder+Decoder по 6+ слоев). 
o Глубже слои — выше вероятность отключения ветви 
(линейно или экспоненциально). 
• Эффект: модель фактически обучается как ансамбль 
«укороченных» версий себя → повышает устойчивость и снижает 
переобучение. 
�
� 2. Learning Rate Warmup 
• На старте обучения градиенты могут «взрываться», если веса 
случайно инициализированы. 
• Warmup: LR постепенно увеличивается с 0 до целевого в первые 
5–10 эпох (линейно или экспоненциально). 
• Эффект: стабилизирует градиенты и помогает трансформеру 
корректно начать обучение, особенно с AdamW. 
�
� 3. Data Augmentation 
• Для DETR и ViT используют мощные аугментации: 
o RandAugment — случайные цветовые и геометрические 
трансформации. 
o Mixup / CutMix — смешение изображений и масок, 
улучшает обобщение. 
o Random Erasing — «стирание» случайных участков, чтобы 
модель не переучивалась на локальные признаки. 
• Эффект: улучшает invariance к масштабу, поворотам, цветам и 
частичным перекрытиям объектов. 
�
� 4. Weight Decay и Label Smoothing 
• Weight Decay (обычно 0.05–0.1) → регуляризация параметров, 
предотвращает переобучение. 
• Label Smoothing (например, 0.1) → не дает модели «слишком 
уверенно» предсказывать классы → улучшает стабильность 
градиентов и обобщение. 
�
� 5. Итог 
• DropPath + Warmup + Аугментации + Weight Decay + Label 
Smoothing → вместе обеспечивают: 
o Стабильность обучения глубоких encoder/decoder 
трансформеров. 
o Устойчивость к переобучению. 
o Улучшенное обобщение на новые изображения. 
14. 
Ограничения DETR: slow convergence, малые объекты 
�
� 1. Slow Convergence (медленное обучение) 
• DETR обучается медленнее классических детекторов (Faster R-CNN, 
RetinaNet) на ImageNet/COCO: 
o Требуется ~500 эпох на COCO vs ~50–100 у RPN+ROI подходов. 
• Причины: 
o Hungarian Matching для сопоставления query ↔ ground-truth каждую 
итерацию → требует больше итераций, чтобы «разучить» правильные 
correspondences. 
o Отсутствие индуктивных ограничений, присущих CNN (локальная 
инвариантность, приоритет соседних пикселей). 
• Решения / улучшения: 
o Deformable DETR: attention только к небольшому набору ключевых 
точек → ускоряет конвергенцию в 10×. 
o DN-DETR: добавление noisy queries для ускорения обучения. 
�
� 2. Малые объекты 
• DETR плохо справляется с маленькими объектами: 
o Классические CNN детекторы используют FPN / feature pyramid, где 
малые объекты масштабируются в более крупные карты признаков. 
o В vanilla DETR flat attention → каждая позиция в feature map 
одинаково «видна», малые объекты теряются. 
• Решения: 
o Deformable DETR: attention выбирает только несколько ключевых 
точек, что позволяет фиксировать малые объекты. 
o Использование backbone с FPN → multi-scale features помогают 
захватить мелкие объекты. 
�
� 3. Итог 
Ограничение 
Медленная 
конвергенция 
Малые 
объекты 
Причина 
Hungarian matching + flat 
attention 
Flat attention, отсутствие 
пирамидальных фич 
Способ решения 
Deformable DETR, DN-DETR, 
улучшенные оптимизаторы 
FPN backbone, Deformable 
attention, multi-scale features 
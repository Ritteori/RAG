Раздел 8: Поведенческие вопросы (Soft Skills) 
92. Расскажи о своем самом интересном ML проекте. (Будь готов 
рассказать все: задача, данные, какие модели пробовал, какие 
метрики, что получил, какие были проблемы и как их решал). 
Situation (Ситуация/Контекст): 
• "Я хотел глубоко понять, как работают современные LLM, и решил не просто 
использовать библиотеки, а с нуля реализовать генеративную языковую модель 
(аналог GPT) для генерации текста на русском/английском." 
Task (Задача): 
• "Задача — создать с нуля работоспособную модель трансформера с декодерной 
архитектурой, реализовать эффективный механизм внимания с кэшированием 
(KV-cache) для генерации, написать токенизатор и полный цикл обучения, а 
затем обучить её на доступном корпусе текстов (например, на Wikipedia или 
книгах)." 
Action (Действия — самая важная часть): 
• Архитектура: "Я реализовал декодерный трансформер с механизмом 
самовнимания, слоевой нормировкой (LayerNorm) и позиционным 
кодированием. Особое внимание уделил маскированию будущих токенов 
(look-ahead mask) для обучения и кэшированию ключей и значений (KV
cache) для ускорения генерации." 
• Данные и токенизация: "Сначала я писал токенизатор BPE (Byte Pair 
Encoding) вручную. Столкнулся с проблемой паддинга, которая размывала 
смысл. Решил её, изменив стратегию: вместо паддинга предложений я стал 
формировать непрерывный поток текста и разбивать его на блоки 
фиксированной длины, что почти полностью исключило паддинг и улучшило 
обучение." 
• Обучение и отладка: "Главной проблемой стали затухающие градиенты. 
Несмотря на нормировку и residual connections, градиенты "тухли" через 
несколько слоёв. Я провел детальную отладку: проверял инициализацию весов 
(использовал, например, инициализацию Kaiming), контролировал дисперсию 
активаций, добавлял скейлинг в механизме внимания. В итоге проблема была 
комплексной, и её решение потребовало тонкой настройки всех компонентов." 
• Инструменты: "Всё писал на PyTorch, логировал метрики в TensorBoard, 
использовал Weights & Biases для отслеживания экспериментов." 
Result (Результат): 
• Технический: "В итоге я получил рабочую модель на 20 млн параметров, 
способную генерировать осмысленные последовательности текста. Это 1000 
строк чистого, понятного кода, который я могу объяснить от и до. Модель 
успешно обучалась на датасете." 
• Измеряемый результат: "Я достиг перплексии (perplexity) на валидации ~25 
(цифру нужно вспомнить/взять реалистичную для вашего датасета), что 
подтвердило, что модель выучила языковые закономерности." 
• Выводы и компетенции: "Проект дал мне глубокое, а не поверхностное 
понимание архитектуры трансформеров, механизма внимания, проблем 
обучения глубоких сетей (vanishing gradients) и препроцессинга текста. Я 
научился отлаживать сложные тренировочные циклы и понял важность 
каждой детали, от инициализации до токенизации." 
93. Как ты работаешь с "грязными" данными? 
Работа с грязными данными — это системный процесс, который я разбиваю на этапы: 
1. Разведочный анализ (EDA) и Аудит качества: 
• Структура: Проверяю схему данных, типы столбцов (dtypes). 
• Пропуски: Определяю долю пропусков в каждом признаке (df.isnull().sum()). 
Анализирую паттерн пропусков: случайный (MCAR) или системный (например, 
не заполнена цена у товаров определённой категории). 
• Выбросы: Визуализирую распределения (боксплоты, гистограммы), считаю 
статистики (z-score, IQR) для поиска аномальных значений. 
• Некорректные значения: Ищу противоречия: отрицательный возраст, дата 
покупки из будущего, категории "Unknown" или "NULL" в строковых полях. 
• Инконсистентность: Приведение форматов (даты, единицы измерения), 
обработка опечаток в категориальных данных. 
2. Разработка стратегии очистки (на основе EDA): 
• Пропуски: 
o Удаление: Если пропусков мало (<5%) и они случайны — дропаю 
строки/колонки. 
o Импутация: Если пропусков много — импутирую в зависимости от типа 
данных: медианой/модой для категориальных, средним/медианой для 
числовых, прогнозирующими моделями (KNN) для сложных случаев. 
o Создание флага: Добавляю бинарный признак is_missing, если пропуск 
сам по себе информативен. 
• Выбросы: 
o Не удаляю автоматически. Сначала анализирую природу: это ошибка 
ввода (записан лишний ноль) или реальное, но редкое явление (крупная 
сделка)? Ошибки — исправляю или удаляю. Редкие события — часто 
оставляю, так как они могут быть важны для модели, или применяю 
робустные методы (логарифмирование, winsorization). 
• Некорректные и дублирующиеся значения: Использую бизнес-логику для 
корректировки или удаления. Ищу и удаляю точные и неполные дубликаты. 
3. Валидация и документирование: 
• После очистки повторяю ключевые шаги EDA, чтобы убедиться, что 
распределения не искажены критически. 
• Фиксирую все действия в коде (скрипт препроцессинга или Pipeline в sklearn), 
чтобы процесс был воспроизводимым. 
• Для продакшена сохраняю артефакты очистки (например, обученный 
импьютер, список категорий для one-hot) для применения к новым данным. 
Философия: Цель — не сделать данные "идеально чистыми", а сделать их 
пригодными для решения задачи, минимизировав шум и сохранив полезный сигнал. 
Каждое решение должно быть обосновано анализом и целью проекта. 
94. Что ты делаешь, если не можешь улучшить качество модели 
дальше? 
Я останавливаюсь и применяю системный анализ для диагностики "потолка", а не 
начинаю случайные эксперименты. 
1. Диагностика проблемы (Bias-Variance Decomposition): 
• Высокий bias (недообучение): Ошибки на трейн-сете высоки. Модель 
слишком проста. Что делать: Увеличивать сложность модели (глубину, 
ширину), добавлять новые фичи (feature engineering), уменьшать 
регуляризацию, увеличивать время обучения. 
• Высокий variance (переобучение): Большой разрыв между ошибкой на трейне 
и валидации. Что делать: Собирать больше данных, применять 
аугментацию, усилить регуляризацию (Dropout, L2), упростить модель, 
использовать раннюю остановку (early stopping). 
2. Приоритетные действия (по возрастанию сложности): 
• Тщательный анализ ошибок: Разобрать примеры, где модель ошибается чаще 
всего. Это часто даёт идеи для нового feature engineering или сбора 
специфичных данных. 
• Улучшение данных — самый эффективный путь: 
o Аугментация: Генерация синтетических данных (например, для 
изображений, текста). 
o Сбор дополнительных данных, особенно для проблемных классов. 
o Очистка и исправление разметки (noisy labels). 
• Оптимизация модели и обучения: 
o Гиперпараметрический поиск (Optuna, Hyperopt): Систематический, 
а не ручной подбор. 
o Продвинутые оптимизаторы и расписания LR (например, 
OneCycleLR). 
o Использование предобученных моделей (Transfer Learning). 
• Архитектурные изменения: 
o Эксперименты с разными архитектурами на основе анализа ошибок 
(например, перейти от CNN к Transformer для долгих 
последовательностей). 
o Ансамблирование — как последний шаг. Простые методы (блендинг, 
стекинг) могут дать прирост, но нужно оценивать оплату complexity vs 
performance. 
3. Переоценка постановки задачи: 
• Всегда возвращаюсь к бизнес-цели. Может, текущая метрика неоптимальна? 
Стоит ли оптимизировать Precision вместо Recall? 
• Проверяю пайплайн на утечку данных (data leakage) — это частая причина 
искусственного "потолка". 
Философия: Сначала диагностируй (bias/variance, анализ ошибок), потом действуй 
целенаправленно: сначала данные и фичи, потом гиперпараметры, потом 
архитектура, и только в конце — ансамбли. Если и это не помогает, возможно, 
задача не решается имещимися данными, и нужно пересматривать её постановку. 
95. Где ты находишь датасеты для своих проектов? 
Я использую комбинацию источников в зависимости от этапа проекта: 
1. Публичные репозитории для прототипирования и исследований: 
a. Kaggle, UCI ML Repo: Для классических задач и соревнований. 
b. Hugging Face Datasets, TensorFlow Datasets: Для современных задач 
(NLP, Audio), особенно с готовым splits и предобработкой. 
2. Специализированные платформы для данных "реального мира": 
a. Google Dataset Search, AWS Open Data Registry: Для больших и 
нетривиальных датасетов (спутниковые снимки, геномика). 
3. Самый ценный источник — создание своего датасета (Handcrafted): 
a. Парсинг и краулинг (с соблюдением robots.txt и законодательства) для 
сбора актуальных данных. 
b. Генерация синтетических данных (например, через библиотеки Faker 
или с использованием аугментации для CV/NLP). 
c. Разметка существующих внутренних данных — это часто самый 
релевантный источник для продакшн-задач. 
4. Для продакшен-проектов — внутренние данные компании: 
a. Работа с логами, базами данных, Data Lake (через SQL, Spark). 
b. Создание механизмов сбора и разметки данных как части продукта 
(active learning, feedback loops от пользователей). 
Ключевой принцип: Публичные датасеты — для быстрого старта и проверки гипотез. 
Но для реального продукта качественный, релевантный датасет часто приходится 
создавать или тщательно вычищать из внутренних источников. 
96. Какие ресурсы (книги, курсы, блоги) ты используешь для обучения? 
Я поддерживаю знания на нескольких уровнях, комбинируя разные форматы: 
1. Фундамент и теория (глубокое понимание): 
a. Классические книги: "Hands-On Machine Learning" Géron (для входа), 
"Pattern Recognition and Machine Learning" Bishop (для математики), 
"Deep Learning" Goodfellow (для нейросетей). 
b. Курсы: Стэнфордские CS229 (ML) и CS231n (CV) — как золотой 
стандарт для понимания основ. 
2. Практика и современные тренды (актуальные навыки): 
a. Документация и туториалы: Первоисточники — официальные docs 
PyTorch/TensorFlow, руководства Hugging Face, LangChain. 
b. Структурированные курсы: Специализированные курсы от ведущих 
компаний (например, Yandex Data School, Deep Learning.AI). 
c. Исследовательские статьи: Чтение ключевых работ на arXiv.org, слежу 
за конференциями (NeurIPS, ICML, CVPR). 
3. Оперативное обучение и решение задач: 
a. LLM (ChatGPT/DeepSeek): Использую как интеллектуальный 
ассистент для объяснения концепций, генерации шаблонного кода и 
отладки. Но всегда перепроверяю информацию по авторитетным 
источникам. 
b. Технические блоги и сообщества: Блоги Google AI, OpenAI, FAIR, а 
также Stack Overflow, Russian Stack Overflow (RuSO), Telegram-чаты 
по ML для обсуждения практических проблем. 
Мой принцип: LLM и курсы — отличные точки входа и помощники в работе. Но 
для формирования независимого экспертного мнения и глубины необходима работа 
с первоисточниками: книгами, статьями и документацией.